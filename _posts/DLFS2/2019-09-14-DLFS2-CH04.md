---
layout: post
title: Chapter 04. word2vec 속도 개선
comments: true
categories: [Deep Learning from Scratch 2]
tags: [Deep Learning, Machine Learning, NLP]
author: lsjhome


---

# Chapter 4 word2vec 속도 개선

앞의 구현은 말뭉치에 포함된 어휘 수가 많아지면 계산량도 커진다는 문제점이 있다. 실제로 어휘 수가 어느 정도를 넘어서면 앞 장의 CBOW 모델은 계산 시간이 너무 오래 걸린다.

이번 장의 목표는 word2vec의 속도 개선이다. 구체적으로는 앞 장의 단순한 wod2vec에 두 가지 개선을 추가한다. 첫 번째 개선으로 Embedding 이라는 새로운 계층을 도입한다. 그리고 두 번째 개선으로 네거티브 샘플링이라는 새로운 손실 함수를 도입한다. 이 두가지 개선으로 '진짜' word2vec을 완성할 수 있다. 진짜 word2vec이 완성되면 PTB 데이터셋을 가지고 학습을 수행한다. 그리고 그 결과로 얻은 단어의 분산 표현의 장점을 실제로 평가해본다.

## 4.1 word2vec 개선 1

![fig 4-1](/Users/jin/projects/lsjhome.github.io/assets/img/dlfs2/deep_learning_2_images/fig 4-1.png)

앞 장의 CBOW 모델은 단어 2개를 맥락으로 사용해, 이를 바탕으로 하나의 단어를 추측한다. 이때 입력 측 가중치 ($$W_{in}$$)와의 행렬 곱으로 은닉층이 계산되고, 다시 출력 측 가중치 ($$W_{out}$$)와의 행렬 곱으로 각 단어의 점수를 구한다. 그리고 이 점수에 소프트맥스 함수를 적용해 각 단어의 출현 확률을 얻고, 이 확률을 정답 레이블과 비교하여 손실을 구한다.

어휘 수가 작을 때는 문제가 되지 않는다. 하지만 거대한 말뭉치를 다루면 문제가 발생한다. 아래의 그림을 살펴보자

![fig 4-2](/Users/jin/projects/lsjhome.github.io/assets/img/dlfs2/deep_learning_2_images/fig 4-2.png)

입력층과 출력층에는 각 100만개의 뉴런이 존재한다. 이 수많은 뉴런 때문에 중간 계산에 많은 시간이 소요된다. 병목 구간을 살펴보면

- 입력층의 원핫 표현과 가중치 행렬 $$W_{in}$$의 곱 계산(4.1에서 해결)
- 은닉층과 가중치 행렬 $$W_{out}$$의 곱 및 Softmax 계층의 계산(4.2에서 해결)

첫 번째는 입력층의 원핫 표현과 관련된 문제이다. 단어를 원핫 표현으로 다루기 때문에 어휘 수가 많아지면 원핫 표현의 벡터 크기도 커지는 것이다. 예컨대 어휘가 100만개라면 그 원핫 표현 하나만 해도 원소 수가 100만 개인 벡터가 된다. 게다가 원핫 벡터와 가중치 행렬 $$W_{in}$$을 곱해야 하는데, 이것만으로 계산 자원을 상당히 사용하게 된다. 이 문제는 Embedding 계층을 도입하는 것으로 해결한다.

두 번째 문제는 은닉층 이후의 계산이다. 우선 은닉층과 가중치 행렬 $$W_{out}$$ 의 곱만 해도 계산량이 상당하다. 그리고 Softmax 계층에서도 다루는 어휘가 많아짐에 따라 계산량이 증가하는 문제가 있다. 이 문제는 4.2절에서 네거티브 샘플링이라는 새로운 손실 함수를 도입함으로서 해결한다. 그러면 두 병목을 해소할 수 있도록 각각의 개선을 적용해 보자.

### 4.1.1 Embedding 계층

어휘 수가 100개라고 생각해 보자. 이때 은늑칭 뉴런이 100개라면, MatMul 계층의 행렬 곱은 아래와 같다.

![fig 4-3](/Users/jin/projects/lsjhome.github.io/assets/img/dlfs2/deep_learning_2_images/fig 4-3.png)

위 그림처럼 만약 100만개의 어휘를 담은 말뭉치가 있다면,  단어의 원핫 표현도 100만 차원이 된다. 그리고 이런 거대한 벡터와 가중치 행렬을 곱해야 한다. 그러나 위 그림에서 결과적으로 수행하는 것은 단지 행렬의 특정 행을 추출하는 것 뿐이다. 따라서 원핫 표현으로의 변환과 MatMul 계층의 행렬 곱 계산은 사실 필요가 없는 것이다.

가주잋 매개변수로부터 '단어 ID에 해당하는 행(벡터)'를 추출하는 계층을 만들어 보자. 그 계층을 Embedding 계층이라고 부르겠다. Embedding 이란 단어 임베딩(word embedding)에서 유래했다. 즉, Embedding 계층에 단어 임베딩(분산 표현)을 저장하는 것이다.

> 자연어 처리 분야에서 단어의 밀집벡터 표현을 **단어 임베딩** 혹은 단어의 **분산 표현** 이라 한다. 참고로 통계 기반으로 얻은 단어 벡터는 영어로 distributional representation 이라 하고, 신경망을 사용한 추론 기반 기법으로 얻은 단어 벡터는 distributed representation 이라고 한다.

### 4.1.2 Embedding 계층 구현

행렬에서 특정 행을 추출하기란 아주 쉽다. 예컨데 가중치 W가 2차원 넘파이 행렬일 때, 이 가중치로부터 특정 행을 추출하려면 그저 W[2]나 W[5]같은 원하는 행을 명시하면 끝이다.

```

```

