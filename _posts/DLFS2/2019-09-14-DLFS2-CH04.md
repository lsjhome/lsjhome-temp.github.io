---
layout: post
title: Chapter 04. word2vec 속도 개선
comments: true
categories: [Deep Learning from Scratch 2]
tags: [Deep Learning, Machine Learning, NLP]
author: lsjhome


---

# Chapter 4 word2vec 속도 개선

앞의 구현은 말뭉치에 포함된 어휘 수가 많아지면 계산량도 커진다는 문제점이 있다. 실제로 어휘 수가 어느 정도를 넘어서면 앞 장의 CBOW 모델은 계산 시간이 너무 오래 걸린다.

이번 장의 목표는 word2vec의 속도 개선이다. 구체적으로는 앞 장의 단순한 wod2vec에 두 가지 개선을 추가한다. 첫 번째 개선으로 Embedding 이라는 새로운 계층을 도입한다. 그리고 두 번째 개선으로 네거티브 샘플링이라는 새로운 손실 함수를 도입한다. 이 두가지 개선으로 '진짜' word2vec을 완성할 수 있다. 진짜 word2vec이 완성되면 PTB 데이터셋을 가지고 학습을 수행한다. 그리고 그 결과로 얻은 단어의 분산 표현의 장점을 실제로 평가해본다.

## 4.1 word2vec 개선 1

![fig 4-1](/assets/img/dlfs2/deep_learning_2_images/fig 4-1.png)

앞 장의 CBOW 모델은 단어 2개를 맥락으로 사용해, 이를 바탕으로 하나의 단어를 추측한다. 이때 입력 측 가중치 ($$W_{in}$$)와의 행렬 곱으로 은닉층이 계산되고, 다시 출력 측 가중치 ($$W_{out}$$)와의 행렬 곱으로 각 단어의 점수를 구한다. 그리고 이 점수에 소프트맥스 함수를 적용해 각 단어의 출현 확률을 얻고, 이 확률을 정답 레이블과 비교하여 손실을 구한다.

어휘 수가 작을 때는 문제가 되지 않는다. 하지만 거대한 말뭉치를 다루면 문제가 발생한다. 아래의 그림을 살펴보자

![fig 4-2](/assets/img/dlfs2/deep_learning_2_images/fig 4-2.png)

입력층과 출력층에는 각 100만개의 뉴런이 존재한다. 이 수많은 뉴런 때문에 중간 계산에 많은 시간이 소요된다. 병목 구간을 살펴보면

- 입력층의 원핫 표현과 가중치 행렬 $$W_{in}$$의 곱 계산(4.1에서 해결)
- 은닉층과 가중치 행렬 $$W_{out}$$의 곱 및 Softmax 계층의 계산(4.2에서 해결)

첫 번째는 입력층의 원핫 표현과 관련된 문제이다. 단어를 원핫 표현으로 다루기 때문에 어휘 수가 많아지면 원핫 표현의 벡터 크기도 커지는 것이다. 예컨대 어휘가 100만개라면 그 원핫 표현 하나만 해도 원소 수가 100만 개인 벡터가 된다. 게다가 원핫 벡터와 가중치 행렬 $$W_{in}$$을 곱해야 하는데, 이것만으로 계산 자원을 상당히 사용하게 된다. 이 문제는 Embedding 계층을 도입하는 것으로 해결한다.

두 번째 문제는 은닉층 이후의 계산이다. 우선 은닉층과 가중치 행렬 $$W_{out}$$ 의 곱만 해도 계산량이 상당하다. 그리고 Softmax 계층에서도 다루는 어휘가 많아짐에 따라 계산량이 증가하는 문제가 있다. 이 문제는 4.2절에서 네거티브 샘플링이라는 새로운 손실 함수를 도입함으로서 해결한다. 그러면 두 병목을 해소할 수 있도록 각각의 개선을 적용해 보자.

### 4.1.1 Embedding 계층

어휘 수가 100개라고 생각해 보자. 이때 은늑칭 뉴런이 100개라면, MatMul 계층의 행렬 곱은 아래와 같다.

![fig 4-3](/assets/img/dlfs2/deep_learning_2_images/fig 4-3.png)

위 그림처럼 만약 100만개의 어휘를 담은 말뭉치가 있다면,  단어의 원핫 표현도 100만 차원이 된다. 그리고 이런 거대한 벡터와 가중치 행렬을 곱해야 한다. 그러나 위 그림에서 결과적으로 수행하는 것은 단지 행렬의 특정 행을 추출하는 것 뿐이다. 따라서 원핫 표현으로의 변환과 MatMul 계층의 행렬 곱 계산은 사실 필요가 없는 것이다.

가주잋 매개변수로부터 '단어 ID에 해당하는 행(벡터)'를 추출하는 계층을 만들어 보자. 그 계층을 Embedding 계층이라고 부르겠다. Embedding 이란 단어 임베딩(word embedding)에서 유래했다. 즉, Embedding 계층에 단어 임베딩(분산 표현)을 저장하는 것이다.

> 자연어 처리 분야에서 단어의 밀집벡터 표현을 **단어 임베딩** 혹은 단어의 **분산 표현** 이라 한다. 참고로 통계 기반으로 얻은 단어 벡터는 영어로 distributional representation 이라 하고, 신경망을 사용한 추론 기반 기법으로 얻은 단어 벡터는 distributed representation 이라고 한다.

### 4.1.2 Embedding 계층 구현

행렬에서 특정 행을 추출하기란 아주 쉽다. 예컨데 가중치 W가 2차원 넘파이 행렬일 때, 이 가중치로부터 특정 행을 추출하려면 그저 W[2]나 W[5]같은 원하는 행을 명시하면 끝이다. 가중치  W로부터 여러 행을 한꺼번에 추출하는 일도 간단하게 할 수 있다.

```python
class Embedding:
    def __init__(self, W):
        self.params = [W]
        self.grads = [np.zeros_like(W)]
        self.idx = None

    def forward(self, idx):
        W, = self.params
        self.idx = idx
        out = W[idx]
        return out

    def backward(self, dout):
        dW, = self.grads
        dW[...] = 0
        dW[self.idx] = dout # 실은 나쁜 예
        return None
```

역전파를 생각해 보자. Embedding 계층의 순전파는 가중치 W의 특정 행을 열로 추출할 뿐이었다. 단순히 가중치의 특정 행 뉴런만을 (아무것도 손대지 않고) 다음 층으로 흘려보낸 것이다. 따라서 역전파에서는 앞 층(출력 측 층)으로부터 전해진 기울기를 다음 층(입력 측 층)으로 그대로 흘려주면 된다.

다만, 앞 층으로부터 정해진 기울기를 가중치 기울기 dW의 특정 행(idx번째 행)에 설정한다. 그림으로는 다음과 같다.

![fig 4-4](/assets/img/dlfs2/deep_learning_2_images/fig 4-4.png)

이 코드는 가중치 기울기  dW를 꺼낸 다음, dW[...] = 0 문장에서 dW의 원소를 0으로 덮어 쓴다. (dW자체를 0으로 설정하는게 아니라, dW의 형상을 유지한 채, 그 원소들을 0으로 덮어쓴다.) 그리고 앞 층에서 전해진 기울기 dout을 idx번째 행에 할당한다.

```python
a = np.array([1, 2, 3])
a[...] = 0
>>> a
array([0, 0, 0])
```

하지만 이러한 구현은 문제가 있다. idx가 중복될때 발생한다. 아래 그림을 보자

![fig 4-5](/assets/img/dlfs2/deep_learning_2_images/fig 4-5.png)

이렇게 되면 dW의 0번째 행에 2개의 값이 할당되어, 먼저 쓰여진 값을 덮어쓴다.

이 중복 문제를 해결하려면 '할당'이 아닌 '더하기'를 해야 한다. 즉, dh의 각 행의 값을 dW의 해당 행에 더해준다. 역전파를 올바르게 구현해 보자.

```python
def backward(self, dout):
    dW, = self.grads
    dW[...] = 0
    
    for i, word_id in enumerate(self.idx):
        dW[word_id] += dout[i]
    
    # 혹은
    # np.add.at(dW, self.idx, dout)
    
    return None
```

for문을 사용해 해당 인덱스에 기울기를 더해 준다. 이제 중복 인덱스가 있더라도 올바르게 처리된다. 참고로, for 문 대신 넘파이의 np.add.at() 을 사용해도 가능하다. np.add.at(A, idx, B)를 idx번째 행에 더해준다.

이제 word2vec(CBOW 모델)의 구현은 입력 측 MatMul 계층을 Embedding 계층으로 전환할 수 있다. 그 효과로 메모리 사용량을 줄이고 쓸데없는 계산도 생략할 수 있게 되었다.

## 4.2 word2vec 개선 2

남은 병목은 은닉층 이후의 처리(행렬 곱과 Softmax 계층의 계산)다. 병목을 해소하는게 이번 절의 목표이다. 바로 **네거티브 샘플링** 이라는 기법을 사용할 수 있다. Softmax 대신 네거티브 샘플링을 이용하면 어휘가 아무리 많아져도 계산량은 낮은 수준에서 일정하게 억제할 수 있다.

### 4.2.1 은닉층 이후 계산의 문제점

은닉층 이후 계싼의 문제점을 알아보기 위해, 앞 절과 마찬가지로 어휘가 100만 개, 은닉층 뉴런이 10개일때 word2vec(CBOW)모델의 예를 들어 생각해 보자.

![fig 4-6](/assets/img/dlfs2/deep_learning_2_images/fig 4-6.png)

입력층과 출력층에는 뉴런이 각 100만 개씩 존재한다. 앞 절에서는 Embedding 계층을 도입하여 입력층 계산에서 낭비를 줄였다. 남은 문제는 은닉층 이후의 처리이다. 은닉층 이후에서 계산이 오래 걸리는 곳은 다음의 두 부분이다.

- 은닉층의 뉴런과 가중치 행렬 ($$W_{out}$$)의 곱
- Softmax 계층의 계산

은닉층의 벡터 크기가 100이고, 가중치 행렬의 크기가 100 X 100만이다. 이렇게 큰 행렬의 곱을 계산하려면 시간이 오래 걸린다. (메모리도 많이 필요하다.) 또한 역전파 때도 같은 계산을 수행하므로 이 행렬 곱을 '가볍게' 만들어야 한다.

소프트맥스에서도 같은 문제가 발생한다. 어휘가 많아지면 Softmax의 계산량도 증가한다. 이 사실은 Softmax 식을 생각해 보면 명확하다.

![e 4-1](/assets/img/dlfs2/deep_learning_2_images/e 4-1.png)

어휘의 수가 100만개 이므로, 분모의 값을 얻으려면 exp 계산을 100만번 수행해야 한다. 이 계산도 어휘 수에 비례해서 증가하므로, Softmax 계산을 대신할 '가벼운' 계산이 절실하다.

### 4.2.2 다중 분류에서 이진 분류로

네거티브 샘플링 기법에 대해서 본론부터 말하면, 이 기법의 핵심은 '이진 분류'에 있다. 정확하게 말하면 '다중 분류(혹은 다중 클래스 분류)'를 '이진 분류'로 근사하는 것이 네거티브 샘플링을 이해하는 중요한 포인트이다.

지금까지는 맥락이 주어졌을 때 정답이 되는 단어를 높은 확률로 추측하도록 만드는 일을 했다. 예컨대 맥락으로 "you"와 "goodbye"를 주면 정답인 "say"의 확률이 높아지도록 신경망을 학습시켰다. 그리고 학습이 잘 이뤄지면 그 신경망은 올바른 추측을 수행하게 된다. 즉, 신경망은 '맥락이 "you"와 "goodbye"일때, 타깃 단어는 무엇일까?'라는 질문에 올바른 답을 내어줄 수 있다.

이제부터 우리가 생각해야할 것은 '다중 분류'문제를 '이진 분류' 방식으로 해결하는 것이다. 그러기 위해서는 "Yes/No"로 대답할 수 있는 질문을 생각해 내야 한다. 예컨대 "맥락이 'you'와 'goodbye' 일때, 타깃 단어는 'say' 입니가?" 라는 질문에 대답하는 신경망을 생각해 내야 한다. 이렇게 하면 출력층에서는 뉴런 하나만 준비하면 된다. 출력층이 이 뉴런이 "say"의 점수를 출력하는 것이다.

![fig 4-7](/assets/img/dlfs2/deep_learning_2_images/fig 4-7.png)

위 그림에서 보듯 출력층의 뉴런은 하나뿐이다. 따라서 은닉층과 출력 층의 가중치 행렬의 내적은 "say"에 해당하는 역(단어 벡터)만을 추출하고, 그 추출된 벡터와 은닉층 뉴런과의 내적을 계산하면 끝이다. 

![fig 4-8](/assets/img/dlfs2/deep_learning_2_images/fig 4-8.png)

위 그림처럼 출려 측의 가중치 $$W_{out}$$ 에서는 각 단어 ID의 단어 벡터가 각각의 열로 저장되어 있다. 이 예에서는 "say"에 해당하는 단어 벡터를 추출한다. 그리고 그 벡터와 은닉층 뉴런과의 내적을 구한다. 이렇게 구한 값이 최종 점수이다.

> 이전까지는 출력층에서는 모든 단어를 대상으로 계산을 수행했다. 하지만 이제는 "say"라는 단어 하나에 주목하여 그 점수만을 계산하는 것이 차이다. 그리고 시그모이드 함수를 이용해 그 점수를 확률로 변환시킨다.

### 4.2.3 시그모이드 함수와 교차 엔트로피 오차

이진 분류 문제를 신경망으로 풀려면 점수에 시그모이드 함수를 적용해 확률로 변환하고, 손실을 구할 때는 손실 함수로 '교차 엔트로피 오차'를 사용한다. 이 둘은 이진 분류 신경망에서 가장 흔하게 사용하는 조합이다.

> 다중 분류의 경우, 출력층에서는 '소프트맥스 함수'를, 손실 함수로는 '교차 엔트로피 오차'를 이용한다. 이진 분류의 경우, 출력층에서는 '시그모이드 함수'를, 손실 함수로는 '교차 엔트로피 오차'를 이용한다.

시그모이드 함수를 복습해보자. 시그모이드 함수는 [식 4.2]와 같이 쓴다.

![e 4-2](/assets/img/dlfs2/deep_learning_2_images/e 4-2.png)

계산 그래프로는 다음과 같다.

![fig 4-9](/assets/img/dlfs2/deep_learning_2_images/fig 4-9.png)

시그모이드 함수를 적용해 확률 y를 얻으면, 이 확률 y로부터 손실을 구한다. 시그모이드 함수에 사용되는 손실 함수는 다중 분류 때처럼 '교차 엔트로피 오차'이다. 교차 엔트로피 오차는 다음과 같이 쓸 수 있다.

![e 4-3](/assets/img/dlfs2/deep_learning_2_images/e 4-3.png)

여기에서 y는 시그모이드 함수의 출력이고, t는 정답 레이블이다. 이 정답 레이블의 값은 0 혹은 1이다. t가 1이면 정답이 "yes"이고, 0이면 "no"이다. 따라서 t가 1이면 -logy가 출력되고, 반대로 0이면 -log(1-y)가 출력된다.

sigmoid 계층과 Cross Entropy Error 계층의 계산 그래프를 살펴보자.

![fig 4-10](/assets/img/dlfs2/deep_learning_2_images/fig 4-10.png)

위 그림에서 주목할 점은 역전파의 y-t 값이다. 여기에서 y는 신경망이 출력한 확률이고, t는 정답 레이블이다. 그리고 y-t는 정확히 두 값의 차이이다. 예컨대 정답 레이블이 1이라면, y가 1에 가까워질수록 오차가 줄어든다는 뜻이다. 반대로 y가 1로부터 멀어지면 오차가 커진다. 그리고 그 오차가 앞 계층으로 흘러가므로, 오차가 크면 '크게' 학습하고, 오차가 작으면 '작게' 학습하게 된다.

> '시그모이드 함수'와 '교차 엔트로피 오차'를 조합하면 역전파의 값이 y-t라는 '깔끔한 결과'를 도출한다. 마찬가지로 '소프트맥스 함수'와 '교차 엔트로피 오차'와의 조합, 또는 '항등 함수'와 '2제곱 오차'의 조합에서도 역전파 시에는 y-t값이 전파된다.

### 4.2.4 다중 분류에서 이진 분류로 (구현)

우리는 지금까지 다중 분류 문제를 다뤘다. 다중 분류에서는 출력층에 어휘 수만큼의 뉴런을 준비하고 이 뉴런들이 출력한 값을 Softmax 계층에 통과시켰다. 이때 이용되는 신경망을 '계층'과 '연산' 중심으로 그리면 아래 그림처럼 된다.

![fig 4-11](/assets/img/dlfs2/deep_learning_2_images/fig 4-11.png)

입력층에서는 각각에 해당하는 단어 ID의 분산 표현을 추출하기 위해 Embedding 계층을 사용했다.

> 앞 절에서 Embedding 계층을 구현했었다. 이 계층은 대상 단어 ID의 분산 표현(단어 벡터)를 추출한다. 이전에는  Embedding 계층 자리에 MatMul 계층을 사용했다.

그럼 [그림 4-11]의 신경망을 이진 분류 신경망으로 변환해 보자.

![fig 4-12](/assets/img/dlfs2/deep_learning_2_images/fig 4-12.png)

여기에서는 은닉층 뉴런 h와, 출력 측의 가중치 $$W_{out}$$에서 단어 "say"에 해당하는 단어 벡터와의 내적을 계산한다. 그리고 그 출력을 Sigmoid with Loss 계층에 입력해 최종 손실을 얻는다.

> 위 그림의 Sigmoid with Loss 계층에 정답 레이블로 "1"이 입력되어 있다. 이는 현재 문제의 답이 "Yes"임을 의미한다. 답이 "No"라면 Sigmoid with Loss에 정답 레이블로 "0"을 입력해야 한다.

후반부를 더 단순하게 만들어 보자. 이를 위해 Embeeding Dot 계층을 도입해 보자. 이 계층은 그림 4-12의 Embedding 계층과 dot 연산(내적)의 처리를 합친 계층이다. 이 계층을 사용하면 위 그림의 후반부를 아래 그림처럼 그릴 수 있다.

![fig 4-13](/assets/img/dlfs2/deep_learning_2_images/fig 4-13.png)

은닉층 뉴런 **h**는 Embedding Dot 계층을 거쳐 Sigmoid with Loss 계층을 통과한다. 보다시피  Embedding Dot 계층을 사용하면서 은닉층 이후의 처리가 간단해졌다.

그럼 Embedding Dot 계층의 구현을 간단히 살펴보자.

```python
class EmbeddingDot:
    def __init__(self, W):
        self.embed = Embedding(W)
        self.params = self.embed.params
        self.grads = self.embed.grads
        self.cache = None

    def forward(self, h, idx):
        target_W = self.embed.forward(idx)
        out = np.sum(target_W * h, axis=1)

        self.cache = (h, target_W)
        return out

    def backward(self, dout):
        h, target_W = self.cache
        dout = dout.reshape(dout.shape[0], 1)

        dtarget_W = dout * h
        self.embed.backward(dtarget_W)
        dh = dout * target_W
        return dh
```

embed는 Embedding 계층을, cache는 순전파 시의 계산 결과를 잠시 유지하기 위한 변수로 사용한다.

순전파를 담당하는 forward(h, idx) 메서드는 인수로 은닉층 뉴런(h)과 단어 ID의 넘파이 배열(idx)을 받는다. 여기에서 idx는 단어 id의 '배열'인데, 배열로 받는 이유는 데이터를 한꺼번에 처리하는 '미니배치 처리'를 가정했기 때문이다.

forward() 메서드에서는 우선 Embeddint 계층의 forward(idx)를 호출한 다음 내적을 계산한다. 내적 계산은 np.sum(self.target\_w * h, axis=1)이라는 한 줄로 이루어진다. 예를 살펴보자

![fig 4-14](/assets/img/dlfs2/deep_learning_2_images/fig 4-14.png)

idx가 [0, 3, 1]인데, 이는 3개의 데이터를 미니배치로 한번에 처리하는 예임을 뜻한다. idx가 [0, 3, 1]이므로 target\_W는  W의 0번, 3번, 1번째 행을 추출한 결과이다. 이후 target\_W * h는 각 원소의 곱을 계산한다. 그리고 이 결과를 행마다 전부 더해 최종 결과 out을 얻을 수 있다.

### 4.2.5 네거티브 샘플링

'다중 분류'에서 '이진 분류'로 변환할 수 있었다. 하지만 이것이 다가 아니다. 지금까지는 긍정적인 예(정답)에 대해서만 학습했기 때문이다. 다시 말해 부정적인 예(오답)을 입력하면 어떤 결과가 나올지 확실하지 않다.

좋은 가중치가 준비되어 있고, 정답 타깃을 가리키고 있는 경우에는 아래 그림처럼 될 수 있다.

![fig 4-15](/assets/img/dlfs2/deep_learning_2_images/fig 4-15.png)

현재의 신경망에서는 긍정적 예인 "say"에 대해서만 학습하게 된다. 그러나 부정적인 예에 대해서는 아무런 지식도 획득하지 못했다. 긍정적 예("say")에 대해서는  Sigmoid 계층의 출력을 0에 가깝게 만들어야 하고, 부정적 예("say" 이외의 단어)에 대해서는 Sigmoid 계층의 출력을 0에 가깝게 만들어야 한다.

![fig 4-16](/assets/img/dlfs2/deep_learning_2_images/fig 4-16.png)

예컨대 맥락이 "you"와 "goodbye"일 때, 타깃이 "hello"일 확률(틀린 단어일 경우의 확률)은 낮은 값이어야 바람직하다. 타깃이 "hello"일 확률은 0.021(2.1%)이다. 그리고 이런 결과를 만들어주는 가중치가 필요하다.

> 다중 분류 문제를 이진 분류로 다루려면 '정답(긍정적 예)'와 '오답(부정적 예)' 각각에 대해 바르게(이진) 분류할 수 있어야 한다. 따라서 긍정적 예와 부정적 예 모두를 대상으로 문제를 생각해야 한다.

모든 부정적 예를 대상으로 하여 이진 분류를 학습시키면, 어휘 수가 늘어나면 감당할 수 없다. 그래서 근사적인 해법으로, 부정적 예를 몇개(5개라든지, 10개라든지) 선택한다. 즉, 적은 수의 부정적 예를 샘플링에서 사용한다. 이것이 바로 '네거티브 샘플링'기법이 의미하는 바이다.

정리하면, 네거티브 샘플링 기법은 긍정적 예를 타깃으로 한 경우의 손실을 구한다. 그와 동시에 부정적 예를 몇 개 샘플링(선별)하여, 그 부정적 예에 대해서도 마찬가지로 손실을 구한다. 그리고 각각의 데이터(긍정적 예와 샘플링된 부정적 예)의 손실을 더한 값을 최종 손실로 한다.

계산 그래프는 다음과 같다.

![fig 4-17](/assets/img/dlfs2/deep_learning_2_images/fig 4-17.png)

주의할 부분은 긍정적 예와 부정적 예를 다루는 방식이다. 긍정적 예("say")에 대해서는 지금까지처럼 Sigmoid with Loss 계층에 정답 레이블로 "1"을 입력하지만, 부정적 예("hello", "I")에 대해서는 Sigmoid with Loss 계층에 정답 레이블로 "0"을 입력한다.

### 4.2.6 네거티브 샘플링의 샘플링 기법

부정적 예를 어떻게 샘플링하느냐에 관한 문제가 있다. 단순히 무작위로 샘플링하는 것보다는 말뭉치의 통계 데이터를 기초로 샘플링하는 것이 좋다. 구체적으로 말하면, 말뭉치에서 자주 등장하는 단어를 많이 추출하고, 드물게 등장하는 단어를 적게 추출하는 것이다. 말뭉치에서 단어 빈도를 기준으로 샘플링하면, 먼저 말뭉치에서 각 단어의 출현 횟수를 구해 '확률분포'로 나타낸다. 그런 다음 그 확률분포대로 단어를 샘플링하면 된다.

![fig 4-18](/assets/img/dlfs2/deep_learning_2_images/fig 4-18.png)

말뭉치에서 단어별 출현 횟수를 바탕으로 확률분포를 구한 다음, 그 확률분포에 따라서 샘플링을 수행하면 된다. 확률분포대로 샘플링하므로 말뭉치에서 자주 등장하는 단어는 선택될 가능성이 높다. 같은 이유로, '희소한 단어'는 선택되기 어렵다.

> 네거티브 샘플링에서 부정적 예를 가능한 많이 다루는 것이 좋지만 계산량 문제 때문에 적은 수(5개나 10개 등)으로 한정해야 한다. 우연히도 '희소한 단어'만 선택되었다면 어떻게 될까? 물론 결과는 나빠진다. 실전 문제에서도 희소한 단어는 거의 출현하지 않기 때문이다. 즉, 드문 단어를 잘 처리하는 것은 중요도가 낮다. 그보다는 흔한 단어를 잘 처리하는 것이 좋은 결과로 이어질 것이다.

확률분포에 따라 샘플링하는 예를 파이선 코드로 설명해 보자. 이 용도에는 넘파이의 np.random.choice() 메서드를 사용할 수 있다.

word2vec의 네거티브 샘플링에는 앞의 확률분포에서 한 가지를 수정하라고 권고하고 있다. 바로 아래 식처럼 기본 확률분포에 0.75를 제곱하는 것이다.

![e 4-4](/assets/img/dlfs2/deep_learning_2_images/e 4-4.png)

P($$w_{i}$$)은 i번째 단어의 확률을 뜻한다. 위 식은 단순히 원래 확률분포의 각 요소를 '0.75 제곱'할 뿐이다. 다만 수정 후에도 확률의 총합은 1이 되어야 하므로, 분모로는 '수정 후 확률분포의 총합'이 필요하다.

이렇게 수정하는 이유는? 이는 출현 확률이 낮은 단어를 '버리지 않기' 위해서다. '0.75'제곱을 함으로써, 낮은 단어의 확률을 살짝 높일 수 있다. 예를 보자.

```python
p = [0.7, 0.29, 0.01]
new_p = np.power(p, 0.75)
new_p /= np.sum(new_p)
>>> print (new_p)
[0.64196878 0.33150408 0.02652714]
```

0.01이 0.0265로 높아졌다.

이 책에서는 이러한 처리를 UnigramSampler 라는 이름으로 제공한다.

> 유니그램이란 하나의 (연속된) 단어라는 뜻이다. 바이그램은 2개의 연속된 단어를, 트라이그램은 3개의 연속된 단어를 뜻한다. 따라서 UnigramSampler 클래스의 이름에는 한 단어를 대상으로 확률분포를 만든다는 의미가 녹여져 있다. 만약 이를 '바이그램' 버전으로 만든다면 ('you', 'say'), ('you', 'goodbye') ... 같이 두 단어로 구성된 대상에 대한 확률분포를 만들게 된다.

UnigramSampler 클래스는 3개의 인수를 초기화시 받는다. ID목록인 corpus, 확률분포에 제곱할 값인 power, 부정적 예 샘플링을 수행하는 횟수인 sample\_size. 또한 UnigramSampler 클래스는 get\_negative\_sample(target) 메서드를 제공한다. 이 메서드는 target 인수로 지정한 단어를 긍정적 예로 해석하고, 그 외의 단어 ID를 샘플링한다.

```python
corpus = np.array([0, 1, 2, 3, 4, 1, 2, 3])
power = 0.75
sample_size = 2

sampler = UnigramSampler(corpus, power, sample_size)
target = np.array([1, 3, 0])
negative_sample = sampler.get_negative_sample(target)
>>> print (negative_sample)
# [[3 0]
#  [1 2]
#  [3 2]]
```

긍정적 예로 [1, 3, 0] 이라는 3개의 데이터를 미니배치로 다루었다. 각각의 데이터에 대해서 부정적 예를 2개씩 샘플링한다. 이 예에서는 첫번째 데이터에 대한 부정적 예는 [0 3], 두번째는 [1 2], 3번째는 [2 3]이 뽑혔음을 알 수 있다. 

### 4.2.7 네거티브 샘플링 구현

마지막으로 네거티브 샘플링을 구현해 보자. NegativeSamplingLoss라는 클래스로 구현해 보자. 우선은 초기화 메서드이다.

```python
class NegativeSamplingLoss:
    def __init__(self, W, corpus, power=0.75, sample_size=5):
        self.sample_size = sample_size
        self.sampler = UnigramSampler(corpus, power, sample_size)
        self.loss_layers = [SigmoidWithLoss() for _ in range(sample_size + 1)]
        self.embed_dot_layers = [EmbeddingDot(W) for _ in range(sample_size) + 1]

        self.params, self.grads = [], []
        for layer in self.embed_dot_layers:
            self.params += layer.params
            self.grads += layer.grads
```

초기화 메서드의 인수로 출력 측 가중치를 나타내는 W, 말뭉치(단어 ID의 리스트)를 뜻하는  corpus, 확률분포에 제곱할 값인 power, 그리고 부정적 예의 샘플링 횟수인  sample\_size를 사용한다.

여기에서는 앞 절에서 설명한 UnigramSAmpler 클래스를 생성하여 인스턴스 변수인 sampler로 저장한다. 부정적 예의 샘플링 횟수는 인스턴스 변수인  sample\_size에 저장한다.

인스턴스 변수인 loss\_layers와 embed\_dot\_layers에는 원하는 계층을 리스트로 보관한다. 이때 두 리스트에는 sample\_size + 1개의 계층을 생성하는데, 부정적 예를 다루는 계층이 sample\_size 개만큼이고, 여기에 긍정적 예를 다루는 계층이 하나 더 필요하기 때문이다. 정확히는 0번째 계층, 즉 loss\_layers[0]과 embed\_dot\_layers[0]이 긍정적 예를 다루는 계층이다. 그런 다음 이 계층에서 사용하는 매개변수와 기울기를 각각 배열로 저장한다.

이어서 순전파의 구현을 함께 보자.

```python
class NegativeSamplingLoss:

    def __init__(self, W, corpus, power=0.75, sample_size=5):
        self.sample_size = sample_size
        self.sampler = UnigramSampler(corpus, power, sample_size)
        self.loss_layers = [SigmoidWithLoss() for _ in range(sample_size + 1)]
        self.embed_dot_layers = [EmbeddingDot(W) for _ in range(sample_size) + 1]

        self.params, self.grads = [], []
        for layer in self.embed_dot_layers:
            self.params += layer.params
            self.grads += layer.grads

            
    def forward(self, h, target):
        batch_size = target.shape[0]
        negative_sample = self.sampler.get_negative_sample(target)

        # 긍정적 예 순전파
        score = self.embed_dot_layers[0].forward(h, target)
        correct_label = np.ones(batch_size, dtype=np.int32)
        loss = self.loss_layers[0].forward(score, correct_label)

        # 부정적 예 순전파
        negative_label = np.zeros(batch_size, dtype=np.int32)
        for i in range(self.sample_size):
            negative_target = negative_sample[:, 1]
            score = self.embed_dot_layers[1 + i].forward(h, negative_target)
            loss += self.loss_layers[1 + i].forward(score, negative_label)
        
        return loss
      
    def backward(self, dout=1):
        dh = 0
        for l0, l1 in zip(self.loss_layers, self.embed_dot_layers):
            dscore = l0.backward(dout)
            dh += l1.backward(dscore)
            
        return dh
```

forward(h, target) 메서드가 받는 인수는 은닉층 뉴런 h와 긍정적 예의 타깃을 뜻하는  target이다. 이 메서드는 우선  self.sampler를 이용해 부정적 예를 샘플링하는 negative\_sample에 저장한다.

그런 다음 긍정적 예와 부정적 예 각각의 데이터에 대해서 순전파를 수행해 그 손실들을 더한다. 구체적으로는 Embedding Dot 계층의 forward 점수를 구하고, 이어서 이 점수와 레이블을 Sigmoid with Loss 계층으로 흘려 손실을 구한다. 여기에서 긍정적 예의 정답 레이블(correct\_label)은 "1"이고, 부정적 예의 정답 레이블(negative\_label)은 "0"임에 주의하자.

역전파는 순전파 때의 역순으로 각 계츠의 backward()를 호출하면 된다. 은닉층의 뉴런은 순전파 시에 여러 개로 복사되었다. 이는 '1.3.4 계산 그래프'에서 설명한 Repeat 노드에 해당한다. 따라서 역전파 때는 여러 개의 기울기 값을 더해준다. 

## 4.3 개선판  word2vec 학습

PTB 데이터셋을 사용해 학습하고 더 실용적인 단어의 분산 표현을 알아보자.

### 4.3.1 CBOW 모델 구현

단순한 SimpleCBOW 클래스를 개선해 보자. 개선점은  Embedding 계층과 Negative Sampling Loss 계층을 적용하는 것이다. 나아가 맥락의 윈도우 크기를 임의로 조절할 수 있도록 확장한다.

개선된 CBOW 클래스의 모습은 다음과 같다. 우선 초기화 매서드를 살펴보자.

```python
# coding: utf-8
import sys
sys.path.append('..')
from common.np import *  # import numpy as np
from common.layers import Embedding
from ch04.negative_sampling_layer import NegativeSamplingLoss


class CBOW:
    def __init__(self, vocab_size, hidden_size, window_size, corpus):
        V, H = vocab_size, hidden_size

        # 가중치 초기화
        W_in = 0.01 * np.random.randn(V, H).astype('f')
        W_out = 0.01 * np.random.randn(V, H).astype('f')

        # 계층 생성
        self.in_layers = []
        for i in range(2 * window_size):
            layer = Embedding(W_in)  # Embedding 계층 사용
            self.in_layers.append(layer)
        self.ns_loss = NegativeSamplingLoss(W_out, corpus, power=0.75, sample_size=5)

        # 모든 가중치와 기울기를 배열에 모은다.
        layers = self.in_layers + [self.ns_loss]
        self.params, self.grads = [], []
        for layer in layers:
            self.params += layer.params
            self.grads += layer.grads

        # 인스턴스 변수에 단어의 분산 표현을 저장한다.
        self.word_vecs = W_in

    def forward(self, contexts, target):
        h = 0
        for i, layer in enumerate(self.in_layers):
            h += layer.forward(contexts[:, i])
        h *= 1 / len(self.in_layers)
        loss = self.ns_loss.forward(h, target)
        return loss

    def backward(self, dout=1):
        dout = self.ns_loss.backward(dout)
        dout *= 1 / len(self.in_layers)
        for layer in self.in_layers:
            layer.backward(dout)
        return None
```

이 초기화 메서드는 4개의 인수를 받는다. vocab\_size는 어휘 수, hidden\_size는 은닉층의 뉴런 수, corpus는 단어 ID 목록이다 그리고 맥락의 크기(주변 단어 중 몇 개나 맥락으로 포함시킬지)를 window\_size로 지정한다. 예컨데 window\_size가 2이면 타깃 단어의 좌우 2개씩, 총 4개 단어가 맥락이 된다. 

가중치 초기화가 끝나면, 이어서 계층을 생성한다. 여기서는 Embedding 계층을 2 * window\_size 개 작성하여 인스턴스 변수인 in\_layers에 배열로 보관한다. 그런 다음 Negative Sampling Loss 계층을 생성한다. 

계층을 다 생성했으면, 이 신경망에서 사용하는 모든 매개변수와 기울기를 인스턴스 변수인 params과 grads에 모은다. 또한 나중에 단어의 분산 표현에 접근할 수 있도록 인스턴스 변수인 word\_vecs에 가중치 W\_in을 할당한다. 

forward에서는 맥락과 타깃을 메서드 인수로 받는다.

![fig 4-19](/assets/img/dlfs2/deep_learning_2_images/fig 4-19.png)

### 4.3.2 CBOW 모델 학습 코드

```python
# coding: utf-8
import sys
sys.path.append('..')
import numpy as np
from common import config
# GPU에서 실행하려면 아래 주석을 해제하세요(CuPy 필요).
# ===============================================
# config.GPU = True
# ===============================================
import pickle
from common.trainer import Trainer
from common.optimizer import Adam
from cbow import CBOW
from skip_gram import SkipGram
from common.util import create_contexts_target, to_cpu, to_gpu
from dataset import ptb


# 하이퍼파라미터 설정
window_size = 5
hidden_size = 100
batch_size = 100
max_epoch = 10

# 데이터 읽기
corpus, word_to_id, id_to_word = ptb.load_data('train')
vocab_size = len(word_to_id)

contexts, target = create_contexts_target(corpus, window_size)
if config.GPU:
    contexts, target = to_gpu(contexts), to_gpu(target)

# 모델 등 생성
model = CBOW(vocab_size, hidden_size, window_size, corpus)
# model = SkipGram(vocab_size, hidden_size, window_size, corpus)
optimizer = Adam()
trainer = Trainer(model, optimizer)

# 학습 시작
trainer.fit(contexts, target, max_epoch, batch_size)
trainer.plot()

# 나중에 사용할 수 있도록 필요한 데이터 저장
word_vecs = model.word_vecs
if config.GPU:
    word_vecs = to_cpu(word_vecs)
params = {}
params['word_vecs'] = word_vecs.astype(np.float16)
params['word_to_id'] = word_to_id
params['id_to_word'] = id_to_word
pkl_file = 'cbow_params.pkl'  # or 'skipgram_params.pkl'
with open(pkl_file, 'wb') as f:
    pickle.dump(params, f, -1)

```

CBOW 모델은 윈도우 크기를 5로, 은닉층의 뉴런 수를 100개로 설정했다. 사용하는 말뭉치에 따라 다르지만, 윈도우 크기는 2~10개, 은닉층의 뉴런 수는 50~500개 정도면 좋은 결과를 얻을 수 있다. 

이번에 다루는 PTB는 지금까지의 말뭉치보다 월등히 커서 학습 시간이 상당히 오래 걸린다. (반나절) GPU를 사용하기 위해서는 #config.GPU = True 주석을 해제하면 된다. 물론 이를 위해서는 엔비디아 GPU와 쿠파이가 필요하다.

학습이 끝나면 입력 가중치를 꺼내(여기서는 입력 층 가중치면), 나중에 이용할 수 있도록 파일에 보관한다. (단어와 단어 ID변환을 위해 사전도 함께 보관한다.) 파일로 저장할 때는 '피클(pickle)' 기능을 이용한다. 피클은 파이썬 코드의 객체를 파일로 저장(또는 파일에서 읽기)하는데 이용할 수 있다.

### 4.3.3 CBOW 모델 평가

앞 절에서 학습한 단어의 분산 표현을 평가해 보자. 2장에서 구현한 most\_similar() 메서드를 이용하여, 단어 몇 개에 대해 거리가 가장 가까운 단어들을 뽑아보기로 하겠다.

```python
# coding: utf-8
import sys
sys.path.append('..')
from common.util import most_similar, analogy
import pickle


pkl_file = 'cbow_params.pkl'
# pkl_file = 'skipgram_params.pkl'

with open(pkl_file, 'rb') as f:
    params = pickle.load(f)
    word_vecs = params['word_vecs']
    word_to_id = params['word_to_id']
    id_to_word = params['id_to_word']

# 가장 비슷한(most similar) 단어 뽑기
querys = ['you', 'year', 'car', 'toyota']
for query in querys:
    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)

[query] you
 we: 0.6103515625
 someone: 0.59130859375
 i: 0.55419921875
 something: 0.48974609375
 anyone: 0.47314453125

[query] year
 month: 0.71875
 week: 0.65234375
 spring: 0.62744140625
 summer: 0.6259765625
 decade: 0.603515625

[query] car
 luxury: 0.497314453125
 arabia: 0.47802734375
 auto: 0.47119140625
 disk-drive: 0.450927734375
 travel: 0.4091796875

[query] toyota
 ford: 0.55078125
 instrumentation: 0.509765625
 mazda: 0.49365234375
 bethlehem: 0.47509765625
 nissan: 0.474853515625
```

word2vec으로 얻은 단어의 분산 표현은 비슷한 단어를 가까이 모을 뿐 아니라, 더 복잡한 패턴을 파악하는것으로 알려져 있다. 대표적인 예가 "king - man + woman = queen" 으로 유명한 유추 문제(비유 문제)이다. 더 정확하게 말하면, word2vec의 단어의 분산 표현을 사용하면 유추 문제를 벡터의 덧셈과 뺄셈으로 풀 수 있다.

실제로 유추 문제를 풀려면 아래 그림처럼 단어 벡터 공간에서 "man -> woman" 벡터와 "king -> ?" 벡터가 가능한 한 가까워지는 단어를 찾아야 한다.

![fig 4-20](/assets/img/dlfs2/deep_learning_2_images/fig 4-20.png)

단어 "man"의 분산 표현(단어 벡터)를 "vec('man')"이라고 표현해 보자. 그러면 vec('woman') - vec('man') = vec('?') - vec('king')이 된다. 즉 vec('king') + vec('woman') - vec('main') = vec('?')이 된다.

이 함수를 사용하면 지금과 같은 유추 문제를 analogy라는 한 줄로 처리할 수 있다. 

```python
analogy('man', 'king', 'woman',  word_to_id, id_to_word, word_vecs)
```

```python
# 유추(analogy) 작업
print('-'*50)
analogy('king', 'man', 'queen',  word_to_id, id_to_word, word_vecs)
analogy('take', 'took', 'go',  word_to_id, id_to_word, word_vecs)
analogy('car', 'cars', 'child',  word_to_id, id_to_word, word_vecs)
analogy('good', 'better', 'bad',  word_to_id, id_to_word, word_vecs)
--------------------------------------------------

[analogy] king:man = queen:?
 woman: 5.16015625
 veto: 4.9296875
 ounce: 4.69140625
 earthquake: 4.6328125
 successor: 4.609375

[analogy] take:took = go:?
 went: 4.55078125
 points: 4.25
 began: 4.09375
 comes: 3.98046875
 oct.: 3.90625

[analogy] car:cars = child:?
 children: 5.21875
 average: 4.7265625
 yield: 4.20703125
 cattle: 4.1875
 priced: 4.1796875

[analogy] good:better = bad:?
 more: 6.6484375
 less: 6.0625
 rather: 5.21875
 slower: 4.734375
 greater: 4.671875
```

## 4.4 word2vec 남은 주제

### 4.4.1 word2vec을 사용한 애플리케이션의 예

word2vec으로 얻은 단어의 분산 표현은 비슷한 단어를 찾는 용도로 이용할 수 있다. 그러나 이것이 끝이 아니다. 자연어 처리 분야에서 단어의 분산 표현이 중요한 이유는 **전이 학습(transfer learning)** 에 있다. 전이 학습은 한 분야에서 배운 지식을 다른 분야에서도 적용하는 기법이다. 

자연어 문제를 풀 때, word2vec의 단어 분산 표현을 처음부터 학습하는 일은 거의 없다. 그 대신 먼저 클 말뭉치로 학습을 끝낸 후, 그 분산 표현을 각자의 작업에 이용한다. 예컨대 텍스트 분류, 문서 클러스터링, 품사 태그 달기, 감정 분석 등 자연어 처리 작업이라면 가장 먼저 단어를 벡터로 변환하는 작업을 해야 하는데, 이때 학습을 미리 끝낸 단어의 분산 표현을 이용할 수 있다. 그리고 이 학습된 분산 표현이 방급 언급한 자연어 처리 작업 대부분에 훌륭한 결과를 가져다 준다.

단어의 분산 표현은 단어를 고정 길이 벡터로 변환해주는 장점도 있다. 게다가 문장(단어의 흐름)도 단어 분산 표현을 사용하여 고정 길이 벡터로 변환할 수 있다. 

가장 간단한 방법은 문장의 각 단어를 분산 표현으로 변환하고 그 합을 구하는 것이다. 이를 bag-of-words라 하여, 단어의 순서를 고려하지 않는 모델이다.

5장에서는 RNN을 사용하여 한층 더 세련된 방법으로 문장을 고정 길이 벡터로 변환할 수 있다.

단어나 문자을 고정 길이 벡터로 변호나할 수 있는 점은 매우 중요하다. 자연어를 벡터로 변환할 수 있다면, 일반적인 머신러닝 기법(신경망이나 SVM 등)을 적용할 수 있기 때문이다. 이를 그림으로 나타내면 아래와 같다.

![fig 4-21](/assets/img/dlfs2/deep_learning_2_images/fig 4-21.png)

자연어로 쓰여진 질문을 고정 길이 벡터로 변환할 수 있다면, 그 벡터를 다른 머신러닝 시스템의 입력으로 사용할 수 있다. 자연어를 벡터로 변환함으로써, 일반적인 머신러닝 시스템의 틀에서 원하는 답을 출력하는 것(그리고 학습하는 것)이 가능해 진다.

> 위 그림의 파이프라인에서는 단어의 분산 표현 학습과 머신러닝 시스템의 핛브은 서로 다른 데이터셋을 사용해 개별적으로 수행하는 것이 일반적이다. 예컨대 단어의 분산 표현은 위키백과와 같은 범용 말뭉치를 사용해 미리 학습해 둔다. 그리고 현재 직면한 문제에 관련하여 수집한 데이터를 가지고 머신러닝 시스템(SVM 등)을 학습시킨다. 다만, 직면한 문제의 학습 데이터가 아주 많다면 단어의 분산 표현과 머신러닝 시스템 학습 모두를 처음부터 수행하는 방안도 고려해 볼 수 있다.

사용자가 1억명 이상인 스마트폰 앱을 개발, 운영한다고 가정해 보자. 회사가 감당하기 어려울 정도의 메일이 사용자로부터 매일 쏟아져 들어올 것이다.

받은 메일(트윗)등을 자동으로 분류하는 시스템을 만든다고 생각해 보자. 가령 아래 그림처럼 사용자의 감정을 3단계로 나눌 수 있다면, 불만을 가진 사용자의 메일부터 순서대로 살펴볼 수 있다. 그러면 앱의 치명적인 문제를 조기에 발견하고 손을 쓸 수 있을지도 모른다.

<img src="/assets/img/dlfs2/deep_learning_2_images/fig 4-22.png" alt="fig 4-22" style="zoom:50%;" />

우선 메일을 수집하고, 모은 메일들에 수동으로 레이블을 붙인다. 예컨대 3단계의 감정을 나타내는 레이블(긍정적, 중립적, 부정적)을 붙인다. 레이블링 작업이 끝나면 학습된 word2vec을 이용해 메일을 벡터로 변환한다. 그런 다음 감정 분석을 수행하는 어떤 분류 시스템(SVM이나 신경망 등)에 벡터화된 메일과 감정 레이블을 입력하여 학습을 수행한다.

이 예처럼 자연어를 다루는 문제는 단어의 분산 표현이라는 방법으로 벡터화할 수 있다. 그 덕분에 일반적인 머신러닝 기법으로 해결할 수 있다. 게다가 word2vec 전이 학습의 혜택을 누릴 수 있다.

### 4.4.2 단어 벡터 평가 방법

word2vec을 통해 단어의 분산 표현을 얻을 수 있었다. 그러면 그 분산 표현이 좋은지는 어떻게 평가할까? 

단어의 분산 표현은 앞 절에서 본 감정 분석 예처럼, 현실적으로는 특정한 애플리케이션에서 사용되는 것이 대부분이다. 

단어의 분산 표현을 만드는 시스템과 분류하는 시스템의 학습은 따로 수행할 수도 있다. 그 경우, 예컨대 단어의 분산 표현의 차원 수가 최종 정확도에 어떤 영향을 주는지를 조사하려면, 우선 단어의 분산 표현을 학습하고, 그 분산 표현을 사용하여 또 하나의 머신러닝 시스템을 학습시켜야 한다. 즉, 두 단계의 학습을 수행한 다음 평가해야 한다. 또한, 이 경우 두 시스템 각각에서의 최적의 하이퍼파라미터를 찾기 위한 튜닝도 필요하므로, 그만큼 시간이 오래 걸린다.

그래서 단어의 분산 표현의 우수성을 실제 애플리케이션과는 분리해 평가하는 것이 일반적이다. 이때 자주 사용되는 평가 척도가 단어의 '유사성' 이나 '유추 문제'를 활용한 평가이다.

단어의 유사성 평가에서는 사람이 작성한 단어 유사도를 검증 세트를 사용해 평가하는 것이 일반적이다. 가령 유사도를 0에서 10 사이로 점수화한다면, "cat"과 "animal"의 유사도는 8점, "cat"과 "car"의 유사도는 2점과 같이, 사람이 단어 사이의 유사한 정도를 규정한다. 긜고 사람이 부여한 점수와 word2vec에 의한 코사인 유사도 점수를 비교해 그 상관성을 보는 것이다.

유추 문제를 활용한 평가는 "king : queen = man : ?"와 같은 유추 문제를 출제하고, 그 정답률로 단어의 분산 표현의 우수성을 측정한다. 

아래는 word2vec 모델, 단어의 분산 표현의 차원 수, 말뭉치의 크기를 매개변수로 사용해 비교 실험을 수행한 결과이다. 의미(semantics)열은 단어의 의미를 유추하는 유추 문제의 정답률을 보여준다. 가령 "King : queen = actor : actress"와 같이 단어의 의미를 묻는 문제이다. 한편 구문(syntax)열은 단어의 행태 정보를 묻는 문제로, "bad : worst = good : best" 같은 문제를 뜻한다.

![fig 4-23](/assets/img/dlfs2/deep_learning_2_images/fig 4-23.png)

> - 모델에 따라 정확도가 다르다 (말뭉치에 따라 적합한 모델 선택)
> - 일반적으로 말뭉치가 클수록 결과가 좋다 (항상 데이터가 많은 게 좋음)
> - 단어 벡터 차원 수는 적당한 크기가 좋다 (너무 커도 정확도가 나빠짐)

유추 문제를 정확하게 풀 수 있는 단어의 분산 표현이라면, 자연어를 다루는 애플리케이션에서도 좋은 결과를 기대할 수 있을 것이다.

## 4.5 정리

이번 장에서는 word2vec 고속화를 주제로 앞 장의 CBOW 모델을 개선했다. 구체적으로는 Embedding 계층을 구현하고 네거티브 샘플링이라는 새로운 기법을 도입했다. 이렇게 한 배경에는 말뭉치의 어휘 수 증가에 비례해 계산량이 증가하는 문제가 있었다.

이번 장에서의 핵심은 '모두' 대신 '일부'를 처리하는 것이다. 인간 역시 모든 것을 알 수 없듯이 컴퓨터도 현재의 성능으로는 모든 데이터를 처리하는 것이 비현실적이다. 그보다는 꼭 필요한 일부에 집중하는 편이 얻는게 많다. 이번 장에서는 이 생각에 기초한 기법인 네거티브 샘플링을 자세히 살펴보았다. 네거티브 샘플링은 '모든' 단어가 아닌 '일부' 단어만을 대상으로 하는 것으로, 계산을 효율적으로 수행해준다.

word2vec은 자연어 처리 분야에 큰 영향을 주었다. 여기서 얻은 단어의 분산 표현은 다양한 자연어 처리 작업에 이용되고 있고, word2vec의 사상은 자연어뿐 아니라 다른 분야(음성, 이미지, 동영상 등)에도 응용되고 있다. 이번 장에서 word2vec을 재대로 이해했다면, 그 지식은 다양한 분야에서 큰 도움이 될 것이다.

이번 장에서 배운 내용

- Embedding 계층은 단어의 분산 표현을 담고 있으며, 순전파 시 지정한 단어 ID의 벡터를 추출한다.
- word2vec은 어휘 수의 증가에 비례하여 계산량도 증가하므로, 근사치로 계산하는 빠른 기법을 사용하면 좋다. 
- 네거티브 샘플링은 부정적 예를 몇 개 샘플링하는 기법으로, 이를 이용하면 다중 분류를 이진 분류처럼 취급할 수 있다.
- word2vec으로 얻은 단어의 분산 표현에는 단어의 의미가 녹아들어 있으며, 비슷한 맥락에서 사용되는 단어는 단어 벡터 공간에서 가까이 위치한다.
- word2vec의 단어의 분산 표현을 이용하면 유추 문제를 벡터의 덧셈과 뺄셈으로 풀 수 있게 된다.
- word2vec은 전이 학습 측면에서 특히 중요하며, 그 단어의 분산 표현은 다양한 자연어 처리 작업에 이용할 수 있다.

