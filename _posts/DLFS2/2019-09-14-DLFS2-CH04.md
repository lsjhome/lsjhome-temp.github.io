---
layout: post
title: Chapter 04. word2vec 속도 개선
comments: true
categories: [Deep Learning from Scratch 2]
tags: [Deep Learning, Machine Learning, NLP]
author: lsjhome


---

# Chapter 4 word2vec 속도 개선

앞의 구현은 말뭉치에 포함된 어휘 수가 많아지면 계산량도 커진다는 문제점이 있다. 실제로 어휘 수가 어느 정도를 넘어서면 앞 장의 CBOW 모델은 계산 시간이 너무 오래 걸린다.

이번 장의 목표는 word2vec의 속도 개선이다. 구체적으로는 앞 장의 단순한 wod2vec에 두 가지 개선을 추가한다. 첫 번째 개선으로 Embedding 이라는 새로운 계층을 도입한다. 그리고 두 번째 개선으로 네거티브 샘플링이라는 새로운 손실 함수를 도입한다. 이 두가지 개선으로 '진짜' word2vec을 완성할 수 있다. 진짜 word2vec이 완성되면 PTB 데이터셋을 가지고 학습을 수행한다. 그리고 그 결과로 얻은 단어의 분산 표현의 장점을 실제로 평가해본다.

## 4.1 word2vec 개선 1

![fig 4-1](/Users/jin/projects/lsjhome.github.io/assets/img/dlfs2/deep_learning_2_images/fig 4-1.png)

앞 장의 CBOW 모델은 단어 2개를 맥락으로 사용해, 이를 바탕으로 하나의 단어를 추측한다. 이때 입력 측 가중치 ($$W_{in}$$)와의 행렬 곱으로 은닉층이 계산되고, 다시 출력 측 가중치 ($$W_{out}$$)와의 행렬 곱으로 각 단어의 점수를 구한다. 그리고 이 점수에 소프트맥스 함수를 적용해 각 단어의 출현 확률을 얻고, 이 확률을 정답 레이블과 비교하여 손실을 구한다.

어휘 수가 작을 때는 문제가 되지 않는다. 하지만 거대한 말뭉치를 다루면 문제가 발생한다. 아래의 그림을 살펴보자

![fig 4-2](/Users/jin/projects/lsjhome.github.io/assets/img/dlfs2/deep_learning_2_images/fig 4-2.png)

입력층과 출력층에는 각 100만개의 뉴런이 존재한다. 이 수많은 뉴런 때문에 중간 계산에 많은 시간이 소요된다. 병목 구간을 살펴보면

- 입력층의 원핫 표현과 가중치 행렬 $$W_{in}$$의 곱 계산(4.1에서 해결)
- 은닉층과 가중치 행렬 $$W_{out}$$의 곱 및 Softmax 계층의 계산(4.2에서 해결)

첫 번째는 입력층의 원핫 표현과 관련된 문제이다. 단어를 원핫 표현으로 다루기 때문에 어휘 수가 많아지면 원핫 표현의 벡터 크기도 커지는 것이다. 예컨대 어휘가 100만개라면 그 원핫 표현 하나만 해도 원소 수가 100만 개인 벡터가 된다. 게다가 원핫 벡터와 가중치 행렬 $$W_{in}$$을 곱해야 하는데, 이것만으로 계산 자원을 상당히 사용하게 된다. 이 문제는 Embedding 계층을 도입하는 것으로 해결한다.

두 번째 문제는 은닉층 이후의 계산이다. 우선 은닉층과 가중치 행렬 $$W_{out}$$ 의 곱만 해도 계산량이 상당하다. 그리고 Softmax 계층에서도 다루는 어휘가 많아짐에 따라 계산량이 증가하는 문제가 있다. 이 문제는 4.2절에서 네거티브 샘플링이라는 새로운 손실 함수를 도입함으로서 해결한다. 그러면 두 병목을 해소할 수 있도록 각각의 개선을 적용해 보자.

### 4.1.1 Embedding 계층

어휘 수가 100개라고 생각해 보자. 이때 은늑칭 뉴런이 100개라면, MatMul 계층의 행렬 곱은 아래와 같다.

![fig 4-3](/Users/jin/projects/lsjhome.github.io/assets/img/dlfs2/deep_learning_2_images/fig 4-3.png)

위 그림처럼 만약 100만개의 어휘를 담은 말뭉치가 있다면,  단어의 원핫 표현도 100만 차원이 된다. 그리고 이런 거대한 벡터와 가중치 행렬을 곱해야 한다. 그러나 위 그림에서 결과적으로 수행하는 것은 단지 행렬의 특정 행을 추출하는 것 뿐이다. 따라서 원핫 표현으로의 변환과 MatMul 계층의 행렬 곱 계산은 사실 필요가 없는 것이다.

가주잋 매개변수로부터 '단어 ID에 해당하는 행(벡터)'를 추출하는 계층을 만들어 보자. 그 계층을 Embedding 계층이라고 부르겠다. Embedding 이란 단어 임베딩(word embedding)에서 유래했다. 즉, Embedding 계층에 단어 임베딩(분산 표현)을 저장하는 것이다.

> 자연어 처리 분야에서 단어의 밀집벡터 표현을 **단어 임베딩** 혹은 단어의 **분산 표현** 이라 한다. 참고로 통계 기반으로 얻은 단어 벡터는 영어로 distributional representation 이라 하고, 신경망을 사용한 추론 기반 기법으로 얻은 단어 벡터는 distributed representation 이라고 한다.

### 4.1.2 Embedding 계층 구현

행렬에서 특정 행을 추출하기란 아주 쉽다. 예컨데 가중치 W가 2차원 넘파이 행렬일 때, 이 가중치로부터 특정 행을 추출하려면 그저 W[2]나 W[5]같은 원하는 행을 명시하면 끝이다. 가중치  W로부터 여러 행을 한꺼번에 추출하는 일도 간단하게 할 수 있다.

```python
class Embedding:
    def __init__(self, W):
        self.params = [W]
        self.grads = [np.zeros_like(W)]
        self.idx = None

    def forward(self, idx):
        W, = self.params
        self.idx = idx
        out = W[idx]
        return out

    def backward(self, dout):
        dW, = self.grads
        dW[...] = 0
        dW[self.idx] = dout # 실은 나쁜 예
        return None
```

역전파를 생각해 보자. Embedding 계층의 순전파는 가중치 W의 특정 행을 열로 추출할 뿐이었다. 단순히 가중치의 특정 행 뉴런만을 (아무것도 손대지 않고) 다음 층으로 흘려보낸 것이다. 따라서 역전파에서는 앞 층(출력 측 층)으로부터 전해진 기울기를 다음 층(입력 측 층)으로 그대로 흘려주면 된다.

다만, 앞 층으로부터 정해진 기울기를 가중치 기울기 dW의 특정 행(idx번째 행)에 설정한다. 그림으로는 다음과 같다.

![fig 4-4](/Users/jin/projects/lsjhome.github.io/assets/img/dlfs2/deep_learning_2_images/fig 4-4.png)

이 코드는 가중치 기울기  dW를 꺼낸 다음, dW[...] = 0 문장에서 dW의 원소를 0으로 덮어 쓴다. (dW자체를 0으로 설정하는게 아니라, dW의 형상을 유지한 채, 그 원소들을 0으로 덮어쓴다.) 그리고 앞 층에서 전해진 기울기 dout을 idx번째 행에 할당한다.

```python
a = np.array([1, 2, 3])
a[...] = 0
>>> a
array([0, 0, 0])
```

하지만 이러한 구현은 문제가 있다. idx가 중복될때 발생한다. 아래 그림을 보자

![fig 4-5](/Users/jin/projects/lsjhome.github.io/assets/img/dlfs2/deep_learning_2_images/fig 4-5.png)

이렇게 되면 dW의 0번째 행에 2개의 값이 할당되어, 먼저 쓰여진 값을 덮어쓴다.

이 중복 문제를 해결하려면 '할당'이 아닌 '더하기'를 해야 한다. 즉, dh의 각 행의 값을 dW의 해당 행에 더해준다. 역전파를 올바르게 구현해 보자.

```python
def backward(self, dout):
    dW, = self.grads
    dW[...] = 0
    
    for i, word_id in enumerate(self.idx):
        dW[word_id] += dout[i]
    
    # 혹은
    # np.add.at(dW, self.idx, dout)
    
    return None
```

for문을 사용해 해당 인덱스에 기울기를 더해 준다. 이제 중복 인덱스가 있더라도 올바르게 처리된다. 참고로, for 문 대신 넘파이의 np.add.at() 을 사용해도 가능하다. np.add.at(A, idx, B)를 idx번째 행에 더해준다.

이제 word2vec(CBOW 모델)의 구현은 입력 측 MatMul 계층을 Embedding 계층으로 전환할 수 있다. 그 효과로 메모리 사용량을 줄이고 쓸데없는 계산도 생략할 수 있게 되었다.

## 4.2 word2vec 개선 2

남은 병목은 은닉층 이후의 처리(행렬 곱과 Softmax 계층의 계산)다. 병목을 해소하는게 이번 절의 목표이다. 바로 **네거티브 샘플링** 이라는 기법을 사용할 수 있다. Softmax 대신 네거티브 샘플링을 이용하면 어휘가 아무리 많아져도 계산량은 낮은 수준에서 일정하게 억제할 수 있다.

### 4.2.1 은닉층 이후 계산의 문제점

은닉층 이후 계싼의 문제점을 알아보기 위해, 앞 절과 마찬가지로 어휘가 100만 개, 은닉층 뉴런이 10개일때 word2vec(CBOW)모델의 예를 들어 생각해 보자.

![fig 4-6](/Users/jin/projects/lsjhome.github.io/assets/img/dlfs2/deep_learning_2_images/fig 4-6.png)

입력층과 출력층에는 뉴런이 각 100만 개씩 존재한다. 앞 절에서는 Embedding 계층을 도입하여 입력층 계산에서 낭비를 줄였다. 남은 문제는 은닉층 이후의 처리이다. 은닉층 이후에서 계산이 오래 걸리는 곳은 다음의 두 부분이다.

- 은닉층의 뉴런과 가중치 행렬 ($$W_{out}$$)의 곱
- Softmax 계층의 계산

은닉층의 벡터 크기가 100이고, 가중치 행렬의 크기가 100 X 100만이다. 이렇게 큰 행렬의 곱을 계산하려면 시간이 오래 걸린다. (메모리도 많이 필요하다.) 또한 역전파 때도 같은 계산을 수행하므로 이 행렬 곱을 '가볍게' 만들어야 한다.

소프트맥스에서도 같은 문제가 발생한다. 어휘가 많아지면 Softmax의 계산량도 증가한다. 이 사실은 Softmax 식을 생각해 보면 명확하다.

![e 4-1](/Users/jin/projects/lsjhome.github.io/assets/img/dlfs2/deep_learning_2_images/e 4-1.png)

어휘의 수가 100만개 이므로, 분모의 값을 얻으려면 exp 계산을 100만번 수행해야 한다. 이 계산도 어휘 수에 비례해서 증가하므로, Softmax 계산을 대신할 '가벼운' 계산이 절실하다.

### 4.2.2 다중 분류에서 이진 분류로

네거티브 샘플링 기법에 대해서 본론부터 말하면, 이 기법의 핵심은 '이진 분류'에 있다. 정확하게 말하면 '다중 분류(혹은 다중 클래스 분류)'를 '이진 분류'로 근사하는 것이 네거티브 샘플링을 이해하는 중요한 포인트이다.

지금까지는 맥락이 주어졌을 때 정답이 되는 단어를 높은 확률로 추측하도록 만드는 일을 했다. 예컨대 맥락으로 "you"와 "goodbye"를 주면 정답인 "say"의 확률이 높아지도록 신경망을 학습시켰다. 그리고 학습이 잘 이뤄지면 그 신경망은 올바른 추측을 수행하게 된다. 즉, 신경망은 '맥락이 "you"와 "goodbye"일때, 타깃 단어는 무엇일까?'라는 질문에 올바른 답을 내어줄 수 있다.

이제부터 우리가 생각해야할 것은 '다중 분류'문제를 '이진 분류' 방식으로 해결하는 것이다. 그러기 위해서는 "Yes/No"로 대답할 수 있는 질문을 생각해 내야 한다. 예컨대 "맥락이 'you'와 'goodbye' 일때, 타깃 단어는 'say' 입니가?" 라는 질문에 대답하는 신경망을 생각해 내야 한다. 이렇게 하면 출력층에서는 뉴런 하나만 준비하면 된다. 출력층이 이 뉴런이 "say"의 점수를 출력하는 것이다.

![fig 4-7](/Users/jin/projects/lsjhome.github.io/assets/img/dlfs2/deep_learning_2_images/fig 4-7.png)

위 그림에서 보듯 출력층의 뉴런은 하나뿐이다. 따라서 은닉층과 출력 층의 가중치 행렬의 내적은 "say"에 해당하는 역(단어 벡터)만을 추출하고, 그 추출된 벡터와 은닉층 뉴런과의 내적을 계산하면 끝이다. 

![fig 4-8](/Users/jin/projects/lsjhome.github.io/assets/img/dlfs2/deep_learning_2_images/fig 4-8.png)

위 그림처럼 출려 측의 가중치 $$W_{out}$$ 에서는 각 단어 ID의 단어 벡터가 각각의 열로 저장되어 있다. 이 예에서는 "say"에 해당하는 단어 벡터를 추출한다. 그리고 그 벡터와 은닉층 뉴런과의 내적을 구한다. 이렇게 구한 값이 최종 점수이다.

> 이전까지는 출력층에서는 모든 단어를 대상으로 계산을 수행했다. 하지만 이제는 "say"라는 단어 하나에 주목하여 그 점수만을 계산하는 것이 차이다. 그리고 시그모이드 함수를 이용해 그 점수를 확률로 변환시킨다.

### 4.2.3 시그모이드 함수와 교차 엔트로피 오차

이진 분류 문제를 신경망으로 풀려면 점수에 시그모이드 함수를 적용해 확률로 변환하고, 손실을 구할 때는 손실 함수로 '교차 엔트로피 오차'를 사용한다. 이 둘은 이진 분류 신경망에서 가장 흔하게 사용하는 조합이다.

> 다중 분류의 경우, 출력층에서는 '소프트맥스 함수'를, 손실 함수로는 '교차 엔트로피 오차'를 이용한다. 이진 분류의 경우, 출력층에서는 '시그모이드 함수'를, 손실 함수로는 '교차 엔트로피 오차'를 이용한다.

시그모이드 함수를 복습해보자. 시그모이드 함수는 [식 4.2]와 같이 쓴다.

![e 4-2](/Users/jin/projects/lsjhome.github.io/assets/img/dlfs2/deep_learning_2_images/e 4-2.png)

계산 그래프로는 다음과 같다.

![fig 4-9](/Users/jin/projects/lsjhome.github.io/assets/img/dlfs2/deep_learning_2_images/fig 4-9.png)

시그모이드 함수를 적용해 확률 y를 얻으면, 이 확률 y로부터 손실을 구한다. 시그모이드 함수에 사용되는 손실 함수는 다중 분류 때처럼 '교차 엔트로피 오차'이다. 교차 엔트로피 오차는 다음과 같이 쓸 수 있다.

![e 4-3](/Users/jin/projects/lsjhome.github.io/assets/img/dlfs2/deep_learning_2_images/e 4-3.png)

여기에서 y는 시그모이드 함수의 출력이고, t는 정답 레이블이다. 이 정답 레이블의 값은 0 혹은 1이다. t가 1이면 정답이 "yes"이고, 0이면 "no"이다. 따라서 t가 1이면 -logy가 출력되고, 반대로 0이면 -log(1-y)가 출력된다.

sigmoid 계층과 Cross Entropy Error 계층의 계산 그래프를 살펴보자.

![fig 4-10](/Users/jin/projects/lsjhome.github.io/assets/img/dlfs2/deep_learning_2_images/fig 4-10.png)

위 그림에서 주목할 점은 역전파의 y-t 값이다. 여기에서 y는 신경망이 출력한 확률이고, t는 정답 레이블이다. 그리고 y-t는 정확히 두 값의 차이이다. 예컨대 정답 레이블이 1이라면, y가 1에 가까워질수록 오차가 줄어든다는 뜻이다. 반대로 y가 1로부터 멀어지면 오차가 커진다. 그리고 그 오차가 앞 계층으로 흘러가므로, 오차가 크면 '크게' 학습하고, 오차가 작으면 '작게' 학습하게 된다.

> '시그모이드 함수'와 '교차 엔트로피 오차'를 조합하면 역전파의 값이 y-t라는 '깔끔한 결과'를 도출한다. 마찬가지로 '소프트맥스 함수'와 '교차 엔트로피 오차'와의 조합, 또는 '항등 함수'와 '2제곱 오차'의 조합에서도 역전파 시에는 y-t값이 전파된다.

### 4.2.4 다중 분류에서 이진 분류로 (구현)

우리는 지금까지 다중 분류 문제를 다뤘다. 다중 분류에서는 출력층에 어휘 수만큼의 뉴런을 준비하고 이 뉴런들이 출력한 값을 Softmax 계층에 통과시켰다. 이때 이용되는 신경망을 '계층'과 '연산' 중심으로 그리면 아래 그림처럼 된다.

![fig 4-11](/Users/jin/projects/lsjhome.github.io/assets/img/dlfs2/deep_learning_2_images/fig 4-11.png)

입력층에서는 각각에 해당하는 단어 ID의 분산 표현을 추출하기 위해 Embedding 계층을 사용했다.

> 앞 절에서 Embedding 계층을 구현했었다. 이 계층은 대상 단어 ID의 분산 표현(단어 벡터)를 추출한다. 이전에는  Embedding 계층 자리에 MatMul 계층을 사용했다.

그럼 [그림 4-11]의 신경망을 이진 분류 신경망으로 변환해 보자.

![fig 4-12](/Users/jin/projects/lsjhome.github.io/assets/img/dlfs2/deep_learning_2_images/fig 4-12.png)

여기에서는 은닉층 뉴런 h와, 출력 측의 가중치 $$W_{out}$$에서 단어 "say"에 해당하는 단어 벡터와의 내적을 계산한다. 그리고 그 출력을 Sigmoid with Loss 계층에 입력해 최종 손실을 얻는다.

> 위 그림의 Sigmoid with Loss 계층에 정답 레이블로 "1"이 입력되어 있다. 이는 현재 문제의 답이 "Yes"임을 의미한다. 답이 "No"라면 Sigmoid with Loss에 정답 레이블로 "0"을 입력해야 한다.

후반부를 더 단순하게 만들어 보자. 이를 위해 Embeeding Dot 계층을 도입해 보자. 이 계층은 그림 4-12의 Embedding 계층과 dot 연산(내적)의 처리를 합친 계층이다. 이 계층을 사용하면 위 그림의 후반부를 아래 그림처럼 그릴 수 있다.

![fig 4-13](/Users/jin/projects/lsjhome.github.io/assets/img/dlfs2/deep_learning_2_images/fig 4-13.png)

은닉층 뉴런 **h**는 Embedding Dot 계층을 거쳐 Sigmoid with Loss 계층을 통과한다. 보다시피  Embedding Dot 계층을 사용하면서 은닉층 이후의 처리가 간단해졌다.

그럼 Embedding Dot 계층의 구현을 간단히 살펴보자.

```python
class EmbeddingDot:
    def __init__(self, W):
        self.embed = Embedding(W)
        self.params = self.embed.params
        self.grads = self.embed.grads
        self.cache = None

    def forward(self, h, idx):
        target_W = self.embed.forward(idx)
        out = np.sum(target_W * h, axis=1)

        self.cache = (h, target_W)
        return out

    def backward(self, dout):
        h, target_W = self.cache
        dout = dout.reshape(dout.shape[0], 1)

        dtarget_W = dout * h
        self.embed.backward(dtarget_W)
        dh = dout * target_W
        return dh
```

embed는 Embedding 계층을, cache는 순전파 시의 계산 결과를 잠시 유지하기 위한 변수로 사용한다.

순전파를 담당하는 forward(h, idx) 메서드는 인수로 은닉층 뉴런(h)과 단어 ID의 넘파이 배열(idx)을 받는다. 여기에서 idx는 단어 id의 '배열'인데, 배열로 받는 이유는 데이터를 한꺼번에 처리하는 '미니배치 처리'를 가정했기 때문이다.

forward() 메서드에서는 우선 Embeddint 계층의 forward(idx)를 호출한 다음 내적을 계산한다. 내적 계산은 np.sum(self.target\_w * h, axis=1)이라는 한 줄로 이루어진다. 예를 살펴보자

![fig 4-14](/Users/jin/projects/lsjhome.github.io/assets/img/dlfs2/deep_learning_2_images/fig 4-14.png)

idx가 [0, 3, 1]인데, 이는 3개의 데이터를 미니배치로 한번에 처리하는 예임을 뜻한다. idx가 [0, 3, 1]이므로 target\_W는  W의 0번, 3번, 1번째 행을 추출한 결과이다. 이후 target\_W * h는 각 원소의 곱을 계산한다. 그리고 이 결과를 행마다 전부 더해 최종 결과 out을 얻을 수 있다.

### 4.2.5 네거티브 샘플링

'다중 분류'에서 '이진 분류'로 변환할 수 있었다. 하지만 이것이 다가 아니다. 지금까지는 긍정적인 예(정답)에 대해서만 학습했기 때문이다. 다시 말해 부정적인 예(오답)을 입력하면 어떤 결과가 나올지 확실하지 않다.

좋은 가중치가 준비되어 있고, 정답 타깃을 가리키고 있는 경우에는 아래 그림처럼 될 수 있다.

![fig 4-15](/Users/jin/projects/lsjhome.github.io/assets/img/dlfs2/deep_learning_2_images/fig 4-15.png)

현재의 신경망에서는 긍정적 예인 "say"에 대해서만 학습하게 된다. 그러나 부정적인 예에 대해서는 아무런 지식도 획득하지 못했다. 긍정적 예("say")에 대해서는  Sigmoid 계층의 출력을 0에 가깝게 만들어야 하고, 부정적 예("say" 이외의 단어)에 대해서는 Sigmoid 계층의 출력을 0에 가깝게 만들어야 한다.

![fig 4-16](/Users/jin/projects/lsjhome.github.io/assets/img/dlfs2/deep_learning_2_images/fig 4-16.png)

예컨대 맥락이 "you"와 "goodbye"일 때, 타깃이 "hello"일 확률(틀린 단어일 경우의 확률)은 낮은 값이어야 바람직하다. 타깃이 "hello"일 확률은 0.021(2.1%)이다. 그리고 이런 결과를 만들어주는 가중치가 필요하다.

> 다중 분류 문제를 이진 분류로 다루려면 '정답(긍정적 예)'와 '오답(부정적 예)' 각각에 대해 바르게(이진) 분류할 수 있어야 한다. 따라서 긍정적 예와 부정적 예 모두를 대상으로 문제를 생각해야 한다.

모든 부정적 예를 대상으로 하여 이진 분류를 학습시키면, 어휘 수가 늘어나면 감당할 수 없다. 그래서 근사적인 해법으로, 부정적 예를 몇개(5개라든지, 10개라든지) 선택한다. 즉, 적은 수의 부정적 예를 샘플링에서 사용한다. 이것이 바로 '네거티브 샘플링'기법이 의미하는 바이다.

정리하면, 네거티브 샘플링 기법은 긍정적 예를 타깃으로 한 경우의 손실을 구한다. 그와 동시에 부정적 예를 몇 개 샘플링(선별)하여, 그 부정적 예에 대해서도 마찬가지로 손실을 구한다. 그리고 각각의 데이터(긍정적 예와 샘플링된 부정적 예)의 손실을 더한 값을 최종 손실로 한다.

계산 그래프는 다음과 같다.

![fig 4-17](/Users/jin/projects/lsjhome.github.io/assets/img/dlfs2/deep_learning_2_images/fig 4-17.png)

주의할 부분은 긍정적 예와 부정적 예를 다루는 방식이다. 긍정적 예("say")에 대해서는 지금까지처럼 Sigmoid with Loss 계층에 정답 레이블로 "1"을 입력하지만, 부정적 예("hello", "I")에 대해서는 Sigmoid with Loss 계층에 정답 레이블로 "0"을 입력한다.

### 4.2.6 네거티브 샘플링의 샘플링 기법

부정적 예를 어떻게 샘플링하느냐에 관한 문제가 있다. 단순히 무작위로 샘플링하는 것보다는 말뭉치의 통계 데이터를 기초로 샘플링하는 것이 좋다. 구체적으로 말하면, 말뭉치에서 자주 등장하는 단어를 많이 추출하고, 드물게 등장하는 단어를 적게 추출하는 것이다. 말뭉치에서 단어 빈도를 기준으로 샘플링하면, 먼저 말뭉치에서 각 단어의 출현 횟수를 구해 '확률분포'로 나타낸다. 그런 다음 그 확률분포대로 단어를 샘플링하면 된다.

![fig 4-18](/Users/jin/projects/lsjhome.github.io/assets/img/dlfs2/deep_learning_2_images/fig 4-18.png)

말뭉치에서 단어별 출현 횟수를 바탕으로 확률분포를 구한 다음, 그 확률분포에 따라서 샘플링을 수행하면 된다. 확률분포대로 샘플링하므로 말뭉치에서 자주 등장하는 단어는 선택될 가능성이 높다. 같은 이유로, '희소한 단어'는 선택되기 어렵다.

> 네거티브 샘플링에서 부정적 예를 가능한 많이 다루는 것이 좋지만 계산량 문제 때문에 적은 수(5개나 10개 등)으로 한정해야 한다. 우연히도 '희소한 단어'만 선택되었다면 어떻게 될까? 물론 결과는 나빠진다. 실전 문제에서도 희소한 단어는 거의 출현하지 않기 때문이다. 즉, 드문 단어를 잘 처리하는 것은 중요도가 낮다. 그보다는 흔한 단어를 잘 처리하는 것이 좋은 결과로 이어질 것이다.

확률분포에 따라 샘플링하는 예를 파이선 코드로 설명해 보자. 이 용도에는 넘파이의 np.random.choice() 메서드를 사용할 수 있다.

word2vec의 네거티브 샘플링에는 앞의 확률분포에서 한 가지를 수정하라고 권고하고 있다. 바로 아래 식처럼 기본 확률분포에 0.75를 제곱하는 것이다.

![e 4-4](/Users/jin/projects/lsjhome.github.io/assets/img/dlfs2/deep_learning_2_images/e 4-4.png)

P($$w_{i}$$)은 i번째 단어의 확률을 뜻한다. 위 식은 단순히 원래 확률분포의 각 요소를 '0.75 제곱'할 뿐이다. 다만 수정 후에도 확률의 총합은 1이 되어야 하므로, 분모로는 '수정 후 확률분포의 총합'이 필요하다.

이렇게 수정하는 이유는? 이는 출현 확률이 낮은 단어를 '버리지 않기' 위해서다. '0.75'제곱을 함으로써, 낮은 단어의 확률을 살짝 높일 수 있다. 예를 보자.

```python
p = [0.7, 0.29, 0.01]
new_p = np.power(p, 0.75)
new_p /= np.sum(new_p)
>>> print (new_p)
[0.64196878 0.33150408 0.02652714]
```

0.01이 0.0265로 높아졌다.

이 책에서는 이러한 처리를 UnigramSampler 라는 이름으로 제공한다.

> 유니그램이란 하나의 (연속된) 단어라는 뜻이다. 바이그램은 2개의 연속된 단어를, 트라이그램은 3개의 연속된 단어를 뜻한다. 따라서 UnigramSampler 클래스의 이름에는 한 단어를 대상으로 확률분포를 만든다는 의미가 녹여져 있다. 만약 이를 '바이그램' 버전으로 만든다면 ('you', 'say'), ('you', 'goodbye') ... 같이 두 단어로 구성된 대상에 대한 확률분포를 만들게 된다.

UnigramSampler 클래스는 3개의 인수를 초기화시 받는다. ID목록인 corpus, 확률분포에 제곱할 값인 power, 부정적 예 샘플링을 수행하는 횟수인 sample\_size. 또한 UnigramSampler 클래스는 get\_negative\_sample(target) 메서드를 제공한다. 이 메서드는 target 인수로 지정한 단어를 긍정적 예로 해석하고, 그 외의 단어 ID를 샘플링한다.

```python
corpus = np.array([0, 1, 2, 3, 4, 1, 2, 3])
power = 0.75
sample_size = 2

sampler = UnigramSampler(corpus, power, sample_size)
target = np.array([1, 3, 0])
negative_sample = sampler.get_negative_sample(target)
>>> print (negative_sample)
# [[3 0]
#  [1 2]
#  [3 2]]
```

긍정적 예로 [1, 3, 0] 이라는 3개의 데이터를 미니배치로 다루었다. 각각의 데이터에 대해서 부정적 예를 2개씩 샘플링한다. 이 예에서는 첫번째 데이터에 대한 부정적 예는 [0 3], 두번째는 [1 2], 3번째는 [2 3]이 뽑혔음을 알 수 있다. 

### 4.2.7 네거티브 샘플링 구현

마지막으로 네거티브 샘플링을 구현해 보자. NegativeSamplingLoss라는 클래스로 구현해 보자. 우선은 초기화 메서드이다.

```python
class NegativeSamplingLoss:
    def __init__(self, W, corpus, power=0.75, sample_size=5):
        self.sample_size = sample_size
        self.sampler = UnigramSampler(corpus, power, sample_size)
        self.loss_layers = [SigmoidWithLoss() for _ in range(sample_size + 1)]
        self.embed_dot_layers = [EmbeddingDot(W) for _ in range(sample_size) + 1]

        self.params, self.grads = [], []
        for layer in self.embed_dot_layers:
            self.params += layer.params
            self.grads += layer.grads
```

초기화 메서드의 인수로 출력 측 가중치를 나타내는 W, 말뭉치(단어 ID의 리스트)를 뜻하는  corpus, 확률분포에 제곱할 값인 power, 그리고 부정적 예의 샘플링 횟수인  sample\_size를 사용한다.

여기에서는 앞 절에서 설명한 UnigramSAmpler 클래스를 생성하여 인스턴스 변수인 sampler로 저장한다. 부정적 예의 샘플링 횟수는 인스턴스 변수인  sample\_size에 저장한다.

인스턴스 변수인 loss\_layers와 embed\_dot\_layers에는 원하는 계층을 리스트로 보관한다. 이때 두 리스트에는 sample\_size + 1개의 계층을 생성하는데, 부정적 예를 다루는 계층이 sample\_size 개만큼이고, 여기에 긍정적 예를 다루는 계층이 하나 더 필요하기 때문이다. 정확히는 0번째 계층, 즉 loss\_layers[0]과 embed\_dot\_layers[0]이 긍정적 예를 다루는 계층이다. 그런 다음 이 계층에서 사용하는 매개변수와 기울기를 각각 배열로 저장한다.

이어서 순전파의 구현을 함께 보자.

```python
class NegativeSamplingLoss:

    def __init__(self, W, corpus, power=0.75, sample_size=5):
        self.sample_size = sample_size
        self.sampler = UnigramSampler(corpus, power, sample_size)
        self.loss_layers = [SigmoidWithLoss() for _ in range(sample_size + 1)]
        self.embed_dot_layers = [EmbeddingDot(W) for _ in range(sample_size) + 1]

        self.params, self.grads = [], []
        for layer in self.embed_dot_layers:
            self.params += layer.params
            self.grads += layer.grads

            
    def forward(self, h, target):
        batch_size = target.shape[0]
        negative_sample = self.sampler.get_negative_sample(target)

        # 긍정적 예 순전파
        score = self.embed_dot_layers[0].forward(h, target)
        correct_label = np.ones(batch_size, dtype=np.int32)
        loss = self.loss_layers[0].forward(score, correct_label)

        # 부정적 예 순전파
        negative_label = np.zeros(batch_size, dtype=np.int32)
        for i in range(self.sample_size):
            negative_target = negative_sample[:, 1]
            score = self.embed_dot_layers[1 + i].forward(h, negative_target)
            loss += self.loss_layers[1 + i].forward(score, negative_label)
        
        return loss
      
    def backward(self, dout=1):
        dh = 0
        for l0, l1 in zip(self.loss_layers, self.embed_dot_layers):
            dscore = l0.backward(dout)
            dh += l1.backward(dscore)
            
        return dh
```

forward(h, target) 메서드가 받는 인수는 은닉층 뉴런 h와 긍정적 예의 타깃을 뜻하는  target이다. 이 메서드는 우선  self.sampler를 이용해 부정적 예를 샘플링하는 negative\_sample에 저장한다.

그런 다음 긍정적 예와 부정적 예 각각의 데이터에 대해서 순전파를 수행해 그 손실들을 더한다. 구체적으로는 Embedding Dot 계층의 forward 점수를 구하고, 이어서 이 점수와 레이블을 Sigmoid with Loss 계층으로 흘려 손실을 구한다. 여기에서 긍정적 예의 정답 레이블(correct\_label)은 "1"이고, 부정적 예의 정답 레이블(negative\_label)은 "0"임에 주의하자.

역전파는 순전파 때의 역순으로 각 계츠의 backward()를 호출하면 된다. 은닉층의 뉴런은 순전파 시에 여러 개로 복사되었다. 이는 '1.3.4 계산 그래프'에서 설명한 Repeat 노드에 해당한다. 따라서 역전파 때는 여러 개의 기울기 값을 더해준다. 