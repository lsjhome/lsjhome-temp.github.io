---
layout: post
title: Chapter 02. 자연어와 단어의 분산 표현
comments: true
categories: [Deep Learning from Scratch 2]
tags: [Deep Learning, Machine Learning]
author: lsjhome



---

# Chapter 02 자연어와 단어의 분산 표현

## 2.1 자연어 처리란

**자연어 처리** 란 '자연어를 처리하는 분야' 이고, '우리의 말을 컴퓨터에게 이해시키기 위한 기술(분야)' 이다.

### 2.1.1 단어의 의미

'단어의 의미'를 잘 파악하는 표현 방법으로 다음과 같은 세 가지 기법을 살펴볼 수 있다.

- 시소러스를 활용한 기법 (이번 장)
- 통계 기반 기법 (이번 장)
- 추론 기반 기법(word2vec) (다음 장)

## 2.2 시소러스

시소러스란 (기본적으로는) 유의어 사전으로, '뜻이 같은 단어(동의어)'나 '뜻이 비슷한 단어(유의어)'가 한 그룹으로 분류되어 있다.

![fig 2-1](/Users/jin/projects/lsjhome.github.io/assets/img/dlfs2/deep_learning_2_images/fig 2-1.png)

또한 자연어 처리에 이용되는 시소러스에는 단어 사이의 '상위와 하위' 혹은 '전체와 부분' 등, 더 세세한 관계까지 정의해둔 경우가 있다.

![fig 2-2](/Users/jin/projects/lsjhome.github.io/assets/img/dlfs2/deep_learning_2_images/fig 2-2.png)

이처럼 모든 단어에 대한 유의어 집합을 만든 다음, 단어들의 관계를 그래프로 표현하여 단어 사이의 연결을 정의 할 수 있다. 그러면 이 '단어 네트워크'를 이용하여 컴퓨터에게 단어 사이의 관계를 가르칠 수 있다.

### 2.2.1 WordNet

자연어 처리 분야에서 가장 유명한 시소러스는 **WordNet**이다. 프린스턴 대학교에서 1985년부터 구축하기 시작하였다. 

### 2.2.2 시소러스의 문제점

WordNet과 같은 시소러스에는 수많은 단어에 대한 동의어와 계층 구조 등의 관계까 정의돼 있다. 그리고 이 지식을 이용하면 '단어의 의미'를 (간접적으로라도) 컴퓨터에 전달할 수 있다. 하지만 이런 방식은 아래의 단점을 지니고 있다.

- **시대 변화에 대응하기 어렵다.**
  "heavy"에는 심각하다 라는 뜻이 있지만 예전에는 그런 뜻이 없었다. 이런 변화에 대응하려면 시소러스를 사람이 수작업으로 끊임없이 갱신하여야 한다.
- **사람을 쓰는 비용은 크다.**
  시소러스를 만드는 데는 엄청난 인적 비용이 발생한다. 
- **단어의 미묘한 차이를 표현할 수 없다.**

이러한 문제를 피하기 위해 '통계 기반 기법'과 신경망을 사용한 '추론 기반 기법'을 알아볼 것이다. 이 두 기법에서는 대량의 텍스트 데이터로부터 '단어의 의미'를 자동으로 추출한다. 사람은 손수 단어를 연결짓는 노동에서 해방된다.

## 2.3 통계 기반 기법

**말뭉치**(corpus)란 대량의 텍스트 데이터이다. 텍스트 데이터가 아닌 자연어 처리 연구나 애플리케이션을 염두해 두고 수집된 텍스트 데이터를 일반적으로 '말뭉치' 라고 한다.

말뭉치란 텍스트 데이터에 지나지 않지만, 그 안에 담긴 문장들은 사람이 쓴 글이다. 다른 시각에서 생각해 보면, 말뭉치는 자연어에 대한 사람의 '지식'이 충분히 담겨 있다고 볼 수 있다. 통계 기반 기법의 목표는 이처럼 사람의 지식으로 가득한 말뭉치에서 자동으로, 그리고 효율적으로 그 핵심을 추출하는 것이다.

### 2.3.1 파이썬으로 말뭉치 전처리하기

자연어 처리에는 다양한 말뭉치가 사용된다. 유명한 것으로는 위키백과와 구글 뉴스등의 텍스트 데이터를 들 수 있다.

```python
text = 'You say goodbye and I say hello.'
text = text.lower()
text = text.replace('.', ' .')
>>> text
'you say goodbye and i say hello .'
```

```python
words = text.split(' ')
>>> words
['you', 'say', 'goodbye', 'and', 'i', 'say', 'hello', '.']
```

```python
word_to_id = {}
id_to_word = {}

for word in words:
    if word not in word_to_id:
        new_id = len(word_to_id)
        word_to_id[word] = new_id
        id_to_word[new_id] = word
        
>>> word_to_id
{'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5, '.': 6}

>>> id_to_word
{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}
```

'단어 목록'을 '단어 ID 목록'으로 변경해 보자. 다음 코드에서는 파이썬의 내포(comprehension) 표기를 사용하여 단어 목록에서 단어 ID목록으로 변환한 다음, 다시 넘파이 배열로 변환했다.

```python
import numpy as np
corpus = [word_to_id[w] for w in words]
corpus = np.array(corpus)
>>> corpus
array([0, 1, 2, 3, 4, 1, 5, 6])
```

```python
def preprocess(text):
    text = text.lower()
    text = text.replace('.', ' .')
    words = text.split(' ')

    word_to_id = {}
    id_to_word = {}
    for word in words:
        if word not in word_to_id:
            new_id = len(word_to_id)
            word_to_id[word] = new_id
            id_to_word[new_id] = word

    corpus = np.array([word_to_id[w] for w in words])

    return corpus, word_to_id, id_to_word
```

```python
text = 'You say goodbye and I say hello.'
corpus, word_to_id, id_to_word = preprocess(text)
```

### 2.3.2 단어의 분산 표현

'색'을 벡터로 표현하듯 '단어'도 벡터로 표현할 수 있을까? 이제부터 우리가 원하는 것은 '단어의 의미'를 정확하게 파악할 수 있는 벡터 표현이다. 이를 자연어 처리 분야에서는 단어의 **분산 표현**(distributional representation) 이라고 한다.

### 2.3.3 분포 가설

자연어 처리의 역사에서 단어를 벡터로 표현하는 연구는 수없이 이루어져 왔다. 중요한 기법의 거의 모두가 하나의 아이디어에 뿌리를 두고 있다. '단어의 의미는 주변 단어에 의해 형성된다'라는 것이다. 이를 **분포 가설**(distributioanl hypothesis)라고 한다. 단어를 벡터로 표현하는 최근 연구도 대부분 이 가설에 뿌리를 두고 있다.

분포 가설이 말하고자 하는 바는 명백하다. 단어 자체에는 의미가 없고. 그 단어가 사용되는 맥락(context)가 그 의미를 형성한다는 것이다. '맥락' 이라 하면 주변에 놓인 단어를 가리킨다.

![fig 2-3](/Users/jin/projects/lsjhome.github.io/assets/img/dlfs2/deep_learning_2_images/fig 2-3.png)

맥락의 크기를 '윈도우 크기'(window size)라고 한다. 윈도우 크기가 1이면 좌우 한 단어씩이, 윈도우 크기가 2이면 좌우 두 단어씩이 맥락에 포함된다.

### 2.3.4 동시발행 행렬

주변 단어를 '세어 보는' 방법이 있다. 어떤 단어에 주목했을 때, 그 주변에 어떤 단어가 몇 번이나 등장하는 지를 세어 집계하는 방법이다. 이를 통계 기반(statistical based) 기법이라고 한다.

먼 전처리를 시작하자. 단어의 수가 총 7개임을 알 수 있따. 단어의 맥락에 해당하는 단어의 빈도를 세어 보자. 윈도우 크기는 1로 하고 단어 ID가 인 "you"부터 시작해 보자.

![fig 2-4](/Users/jin/projects/lsjhome.github.io/assets/img/dlfs2/deep_learning_2_images/fig 2-4.png)

![fig 2-5](/Users/jin/projects/lsjhome.github.io/assets/img/dlfs2/deep_learning_2_images/fig 2-5.png)

ID가 1인 "say"에 대해서도 같은 작업을 수행해 본다.

![fig 2-6](/Users/jin/projects/lsjhome.github.io/assets/img/dlfs2/deep_learning_2_images/fig 2-6.png)

이 결과로부터 "say"라는 단어는 벡터 [1, 0, 1, 0, 1, 1, 0] 으로 표현할 수 있다. 이상의 작업을 모든 단어에 대해서 수행하면 아래와 같이 된다.

![fig 2-7](/Users/jin/projects/lsjhome.github.io/assets/img/dlfs2/deep_learning_2_images/fig 2-7.png)

참고로 이 표가 행렬의 형태를 띤다는 뜻에서 **동시발생 행렬**이라고 한다.

그래프를 그대로 입력해 보면

```python
C = np.array([
    [0, 1, 0, 0, 0, 0, 0],
    [1, 0, 1, 0, 1, 1, 0],
    [0, 1, 0, 1, 0, 0, 0],
    [0, 0, 1, 0, 1, 0, 0],
    [0, 1, 0, 1, 0, 0, 0],
    [0, 1, 0, 0, 0, 0, 1],
    [0, 0, 0, 0, 0, 1, 0],
])

>>> print (C[0]) # ID가 0인 단어의 벡터 표현
[0 1 0 0 0 0 0]
>>> print (C[4]) # ID가 4인 단어의 벡터 표현
[0 1 0 1 0 0 0]
>>> print (C[word_to_id['goodbye']]) # "goodbye"의 벡터 표현
[0 1 0 1 0 0 0]
```

```python
def create_co_matrix(corpus, vocab_size, window_size=1):
    '''동시발생 행렬 생성

    :param corpus: 말뭉치(단어 ID 목록)
    :param vocab_size: 어휘 수
    :param window_size: 윈도우 크기(윈도우 크기가 1이면 타깃 단어 좌우 한 단어씩이 맥락에 포함)
    :return: 동시발생 행렬
    '''
    corpus_size = len(corpus)
    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)

    for idx, word_id in enumerate(corpus):
        for i in range(1, window_size + 1):
            left_idx = idx - i
            right_idx = idx + i

            if left_idx >= 0:
                left_word_id = corpus[left_idx]
                co_matrix[word_id, left_word_id] += 1

            if right_idx < corpus_size:
                right_word_id = corpus[right_idx]
                co_matrix[word_id, right_word_id] += 1

    return co_matrix
```

먼저 co\_matrix를 0으로 채워진 2차원 배열로 초기화한다. 그 다음은 말뭉치의 모든 단어 각각에 대하여 윈도우에 포함된 주변 단어를 세어나간다. 이때 말뭉치의 왼쪽 끝과 오른쪽 끝 경계를 벗어나지 않는지도 확인한다. 

### 2.3.5 벡터 간 유사도

벡터의 유사도를 나타낼 때는 **코사인 유사도**를 자주 이용한다. 

![e 2-1](/Users/jin/projects/lsjhome.github.io/assets/img/dlfs2/deep_learning_2_images/e 2-1.png)

분에서는 벡터의 내적이, 분모에서는 각 베겉의 노름(norm)이 등장한다. 노름은 벡터의 크기를 나타낸 것으로, 여기에서는 'L2 노름'을 계산한다.(L2 노름은 벡터의 각 원소를 제곱해 더한 후 다시 제곱근을 구해 계산한다.) 이[식 2.1]의 핵심은 벡터를 정규화하고 내적을 구하는 것이다.

> 코사인 유사도를 직관적으로 풀어보면 '두 벡터가 가리키는 방향이 얼마나 비슷한가' 이다. 두 벡터의 방향이 완전히 같다면 코사인 유사도가 1이 되며, 완전히 반대라면 -1이 된다.

```python
def cos_similarity(x, y, eps=1e-8):
    '''코사인 유사도 산출

    :param x: 벡터
    :param y: 벡터
    :param eps: '0으로 나누기'를 방지하기 위한 작은 값
    :return:
    '''
    nx = x / (np.sqrt(np.sum(x ** 2)) + eps)
    ny = y / (np.sqrt(np.sum(y ** 2)) + eps)
    return np.dot(nx, ny)
```

> 1e-8을 통해 '0'으로 나누기 오류가 나는 사태를 막아 준다.

이 함수를 사용하면 단어 벡터의 유사도를 다음과 같이 구할 수 있다. 다음은 "you"와 "i(=I)"의 유사도를 구하는 코드이다.

```python
import sys
sys.path.append('..')
from common.util import preprocess, create_co_matrix, cos_similarity


text = 'You say goodbye and I say hello.'
corpus, word_to_id, id_to_word = preprocess(text)
vocab_size = len(word_to_id)
C = create_co_matrix(corpus, vocab_size)

c0 = C[word_to_id['you']]  # "you"의 단어 벡터
c1 = C[word_to_id['i']]    # "i"의 단어 벡터
>>> print(cos_similarity(c0, c1))

0.7071067691154799
```

실행 결과 "you"와 "i"의 코사인 유사도는 0.70 .. 으로 나왔다. 코사인 유사도 값은 -1에서 1 사이이므로, 이 값은 비교적 높다(유사성이 크다)고 말할 수 있다.

### 2.3.6 유사 단어의 랭킹 표시

어떤 단어가 주어지면, 그 검색어와 비슷한 단어를 유사도 순으로 출력하는 함수를 만들어보자. 함수 이름은 most\_similar()로 하고, 다음 인수들을 입력받도록 구현해 보자.

```python
def most_similar(query, word_to_id, id_to_word, word_matrix, top=5):
    '''유사 단어 검색

    :param query: 쿼리(텍스트)
    :param word_to_id: 단어에서 단어 ID로 변환하는 딕셔너리
    :param id_to_word: 단어 ID에서 단어로 변환하는 딕셔너리
    :param word_matrix: 단어 벡터를 정리한 행렬. 각 행에 해당 단어 벡터가 저장되어 있다고 가정한다.
    :param top: 상위 몇 개까지 출력할 지 지정
    '''
    # 1. 검색어를 꺼낸다.
    if query not in word_to_id:
        print('%s(을)를 찾을 수 없습니다.' % query)
        return

    print('\n[query] ' + query)
    query_id = word_to_id[query]
    query_vec = word_matrix[query_id]

    # 2. 코사인 유사도 계산
    vocab_size = len(id_to_word)

    similarity = np.zeros(vocab_size)
    for i in range(vocab_size):
        similarity[i] = cos_similarity(word_matrix[i], query_vec)

    # 3. 코사인 유사도를 기준으로 내림차순으로 출력
    count = 0
    for i in (-1 * similarity).argsort():
        if id_to_word[i] == query:
            continue
        print(' %s: %s' % (id_to_word[i], similarity[i]))

        count += 1
        if count >= top:
            return
```

1. 검색어의 단어 벡터를 꺼낸다.
2. 검색어의 단어 벡터와 다른 모든 단어 벡터와의 코사인 유사도를 각각 구한다.
3. 계산한 코사인 유사도 결과를 기준으로 값이 높은 순서대로 출력한다.

3에서는 similarity 배열에 담긴 원소의 인덱스를 내림차순으로 정렬한 후 상위 원소들을 출력한다. 

```python
# coding: utf-8
import sys
sys.path.append('..')
from common.util import preprocess, create_co_matrix, most_similar


text = 'You say goodbye and I say hello.'
corpus, word_to_id, id_to_word = preprocess(text)
vocab_size = len(word_to_id)
C = create_co_matrix(corpus, vocab_size)

>>> most_similar('you', word_to_id, id_to_word, C, top=5)
[query] you
 goodbye: 0.7071067691154799
 i: 0.7071067691154799
 hello: 0.7071067691154799
 say: 0.0
 and: 0.0

```

말뭉치가 너무 작아서 좋은 결과가 나오지 않았다.

## 2.4 통계 기반 기법 개선하기

앞 절에서는 단어의 동시발생 행렬을 만들었다. 이를 이용해 단어를 벡터로 표현하는 데는 성공했지만 개선할 점이 있다. 개선을 완료한 다음 더 실용적인 말뭉치를 사용하여 '진짜' 단어의 분산 표현을 손에 넣어보자.

### 2.4.1 상호정보량

'발생 횟수'라는 것은 사실 그리 좋은 표현이 아니다. 고빈도 단어(많이 출현하는 단어)로 눈을 돌려보면 그 이유를 알 수 있다.

"the"와 "car"의 동시발생을 생각해 보자. "... the car ..."라는 문구가 자주 보이겠지만, "car" "drive"는 확실히 관련이 더 깊다. 등장 횟수만을 본다면 "car"는 "drive"보다는 "the"와의 관련성이 훨씬 강하다.

이 문제를 해결하기 위해 **점별 상호정보량**(Pointwise Mutual Information) (PMI) 이라는 척도를 사용한다. 

PMI는 확률 변수 x와  y에 대해서 다음 식으로 정의된다.

![e 2-2](/Users/jin/projects/lsjhome.github.io/assets/img/dlfs2/deep_learning_2_images/e 2-2.png)

P(x)는 x가 일어날 확률, P(y)는 y가 일어날 확률, P(x, y)는  x와 y가 동시에 잎어날 확률을 뜻한다. 이 PMI값이 높을수록 관련성이 높다는 의미이다.

이 식을 앞의 자연어 예에 적용하면 P(x)는 단어 x가 말뭉치에 등장할 확률을 가리킨다. 예컨데 10,000개의 단어로 이뤄진 말뭉치에서 "the"가 100번 등장한다면 P("the") = 100 / 10000 = 0.01이 된다. 또한 P(x, y)는 단어 x와  y가 동시발생할 확률이므로, 마찬가지로 "the"와 "car"가 10번 동시발생했다면 P("the", "car") = 10 / 10000 = 0.001이 된다.

그럼 동시발생 행렬을 사용하여 위 식을 다시 써 보자. C는 동시발생 행렬, C(x, y)는 단어 x와 y가 동시 발생하는 횟수, C(x)와 C(y)는 각각 단어 x와 y의 등장 횟수이다. 이때 말뭉치에 포함된 단어 수를 N이라 하면, 식[2.2]는 다음과 같이 변한다.

![e 2-3](/Users/jin/projects/lsjhome.github.io/assets/img/dlfs2/deep_learning_2_images/e 2-3.png)

[식 2.3]에 따라 동시발생 행렬로부터 PMI를 구할 수 있다. 그러면 말뭉치의 단어 수(N)을 10,000이라 하고, "the"와 "car"와 "drive"가 각각 1000번, 20번, 10번 등장했다고 해보자. 그리고 "the"와 "car"의 동시발생 수는 10회, "car"와 "drive"의 동시발생 수는 5회사로 가정하자.

동시발생 횟수 관점에서는 "car"는 "drive"보다 "the"와 관련이 깊다고 나온다. 하지만  PMI관점은 어떨까?

![e 2-4](/Users/jin/projects/lsjhome.github.io/assets/img/dlfs2/deep_learning_2_images/e 2-4.png)

![e 2-5](/Users/jin/projects/lsjhome.github.io/assets/img/dlfs2/deep_learning_2_images/e 2-5.png)

PMI를 이용하면 "car"는 "the"보다 "drive"와의 관련성이 강해진다. 단독으로 출현하는 횟수가 고려되었기 때문이다. 하지만 두 단어의 동시발생 횟수가 0이 되면 $$log_{2}0 = - \infin$$ 가 된다. 이 문제를 피하기 위해서 실제로 구현할 때는 **양의 상호정보량**(Positive PMI, PPMI)을 사용한다.

이 식에 따라 PMI가 음수일 때는 0으로 취급한다. 이제 단어 사이의 관련성을 0이상의 실수로 나타낼 수 있다. 그러면 동시발생 행렬을 PPMI 행렬로 변환하는 함수를 구현해보자.

```python
def ppmi(C, verbose=False, eps = 1e-8):
    '''PPMI(점별 상호정보량) 생성

    :param C: 동시발생 행렬
    :param verbose: 진행 상황을 출력할지 여부
    :return:
    '''
    M = np.zeros_like(C, dtype=np.float32)
    N = np.sum(C)
    S = np.sum(C, axis=0)
    total = C.shape[0] * C.shape[1]
    cnt = 0

    for i in range(C.shape[0]):
        for j in range(C.shape[1]):
            pmi = np.log2(C[i, j] * N / (S[j]*S[i]) + eps)
            M[i, j] = max(0, pmi)

            if verbose:
                cnt += 1
                if cnt % (total//100) == 0:
                    print('%.1f%% 완료' % (100*cnt/total))
    return M
```

동시발생 행렬을 PPMI 행렬로 변환해 보자.

```python
# coding: utf-8
import sys
sys.path.append('..')
import numpy as np
from common.util import preprocess, create_co_matrix, cos_similarity, ppmi


text = 'You say goodbye and I say hello.'
corpus, word_to_id, id_to_word = preprocess(text)
vocab_size = len(word_to_id)
C = create_co_matrix(corpus, vocab_size)
W = ppmi(C)

np.set_printoptions(precision=3)  # 유효 자릿수를 세 자리로 표시
print('동시발생 행렬')
print(C)
print('-'*50)
print('PPMI')
>>> print(W)

동시발생 행렬
[[0 1 0 0 0 0 0]
 [1 0 1 0 1 1 0]
 [0 1 0 1 0 0 0]
 [0 0 1 0 1 0 0]
 [0 1 0 1 0 0 0]
 [0 1 0 0 0 0 1]
 [0 0 0 0 0 1 0]]
--------------------------------------------------
PPMI
[[0.    1.807 0.    0.    0.    0.    0.   ]
 [1.807 0.    0.807 0.    0.807 0.807 0.   ]
 [0.    0.807 0.    1.807 0.    0.    0.   ]
 [0.    0.    1.807 0.    1.807 0.    0.   ]
 [0.    0.807 0.    1.807 0.    0.    0.   ]
 [0.    0.807 0.    0.    0.    0.    2.807]
 [0.    0.    0.    0.    0.    2.807 0.   ]]
```

하지만 PPMI행렬에도 문제가 있다. 말뭉치의 어휘 소가 증가함에 따라 각 단어 벡터의 차원 수도 증가한다. 말뭉치의 어휘 수가 10만 개라면 그 벡터의 차원 수도 똑같이 10만이 된다. 10만 차원의 벡터를 다루는 것은 그리 현실적이지 않다.

또한 이 행렬의 내용을 들여다보면 원소 대부분이 0인것을 알 수 있다. 벡터의 원소 대부분이 중요하지 않나는 것이다. 다르게 표현하면 각 원소의 '중요도'가 낮다는 뜻이다. 더구나 이런 벡터는 노이즈에 약하고 견고하지 못하다는 약점이 있다. 이 문제에 대처하고자 자주 사용되는 기법이 바로 벡터의 차원 감소이다. 

### 2.4.2 차원 감소

단순히 차원을 '중요한 정보'는 최대한 유지하면서 줄이는 것이 핵심이다. 데이터의 분포를 고려해 중요한 '축'을 찾는 일을 수행한다.

![fig 2-8](/Users/jin/projects/lsjhome.github.io/assets/img/dlfs2/deep_learning_2_images/fig 2-8.png)

위 그림에서는 1차원 값만으로도 데이터의 본질적인 차이를 구별할 수 있어야 한다. 이와 같은 작업은 다차원 데이터에 대해서도 수행할 수 있다.

> 원소 대부분이 0인 행렬 또는 벡터를 '희소행렬(sparse matrix)' 또는 '희소벡터' 라고 한다. 차원 감소의 핵심은 희소벡터에서 중요한 축을 찾아내어 더 적은 차원으로 다시 표현하는 것인데, 차원 감소의 결과로 원래의 희소벡터는 원소 대부분이 0이 아닌 값으로 구성된 '밀집벡터'로 변환된다. 이 조밀한 벡터야말로 우리가 원하는 단어의 분산 표현이다.

**특잇값분해**(Singular Value Decomposition, SVD)를 이용한다. SVD는 임의의 행렬을 세 행렬의 곱으로 분해하며, 수식으로는 다음과 같다.

![e 2-7](/Users/jin/projects/lsjhome.github.io/assets/img/dlfs2/deep_learning_2_images/e 2-7.png)

SVD는 임의의 행렬 **X**를 **U**, **S**, **V** 라는 세 행렬의 곱으로 분해한다. 여기서 **U**와 **V**는 직교행렬(orthogonal matrix)이고, 그 열벡터는 서로 직교한다. 또한 **S**는 대각행렬(diagonal matrix)이다. 이 수식을 시각적으로 표현한 것이 아래의 그림이다.

![fig 2-9](/Users/jin/projects/lsjhome.github.io/assets/img/dlfs2/deep_learning_2_images/fig 2-9.png)

**U**는 직교행렬이고, 직교행렬은 어떠한 공간의 축(기저)을 형성한다. **U**행렬을 '단어 공간'으로 취급할 수 있다. **S**는 대각 행렬로, 그 대각성분에는 '특잇값'이 큰 순서로 나열되어 있다. 특잇값이란 '해당 축'의 중요도라고 간주할 수 있다. 그래서 아래 그림과 같이 중요도가 낮은 원소를 깍아내는 방법을 생각할 수 있다.

![fig 2-10](/Users/jin/projects/lsjhome.github.io/assets/img/dlfs2/deep_learning_2_images/fig 2-10.png)

행렬  **S** 에서 특잇값이 작다면 중요도가 낮다는 뜻이므로, 행렬 **U**에 여분의 열벡터를 깍아내어 원래의 행렬을 근사할 수 있다. 이를 '단어의 PPMI 행렬'에 적용해 보면, 행렬  X의 각 행에는 해당 단어  ID의 단어 벡터가 저장되어 있으며, 그 단어 벡터가 행렬  **U**라는 차원 감소된 벡터로 표현된다.

### 2.4.3 SVD에 의한 차원 감소

SVD를 파이썬 코드로 살펴보자. SVD는 넘파이의 linalg 모듈이 제공하는 svd 메서드로 실행할 수 있다. 

```python
# coding: utf-8
import sys
sys.path.append('..')
import numpy as np
import matplotlib.pyplot as plt
from common.util import preprocess, create_co_matrix, ppmi


text = 'You say goodbye and I say hello.'
corpus, word_to_id, id_to_word = preprocess(text)
vocab_size = len(id_to_word)
C = create_co_matrix(corpus, vocab_size, window_size=1)
W = ppmi(C)

# SVD
U, S, V = np.linalg.svd(W)

np.set_printoptions(precision=3)  # 유효 자릿수를 세 자리로 표시
>>> print(C[0])
>>> print(W[0])
>>> print(U[0])

[0 1 0 0 0 0 0]
[0.    1.807 0.    0.    0.    0.    0.   ]
[ 3.409e-01  0.000e+00 -1.205e-01 -3.886e-16 -9.323e-01 -1.110e-16
 -2.426e-17]
```

이 결과에서 보듯 원래는 희소벡터인 W[0]가  SVD에 의해서 밀집벡터 U[0]으로 변했다. 그리고 이 밀집벡터의 차원을 감소시키려면, 2차원 벡터로 줄이려면 다음의 처음 두 원소를 꺼내면 된다.

```python
>>> print (U[0, :2])
[0.341 0.   ]
```

각 단어를 2차원 벡터로 표현한 후 그래프로 그려보자.

```python
# 플롯
for word, word_id in word_to_id.items():
    plt.annotate(word, (U[word_id, 0], U[word_id, 1]))
plt.scatter(U[:,0], U[:,1], alpha=0.5)
plt.show()
```

![fig 2-11](/Users/jin/projects/lsjhome.github.io/assets/img/dlfs2/deep_learning_2_images/fig 2-11.png)

사용한 말뭉치가 아주 작아서 이 결과를 그대로 받아들이기에는 조금 석연치 않다. PTB 데이터셋이라는 더 큰 말뭉치를 사용하여 똑같은 실험을 수행해 보자.

> 행렬의 크기가  N이면  SVD 계산은 $$O(N^{3})$$이 걸린다. 계싼량이  N의 3 제곱이 비례해 늘어난다. 이는 현실적으로 감당하기 어려운 수준이므로  Truncated SVD 같은 더 빠른 기법을 이용한다. Truncated SVD는 특잇값이 작은 것은 버리는 방식으로 성능 향상을 꾀한다. 사이킷런 라이브러리의 Truncated SVD를 이용한다. 

### 2.4.4 PTB 데이터셋

**펜 트리뱅크**(PTB) 말뭉치는 주어진 기법의 품질을 측정하는 벤치마크로 자주 이용된다. 이 책에서도 PTB 말뭉치를 이용하여 다양한 실험을 수행한다.

희한 단어가 \<unk>라는 특수 문자로 치환되어 있고 구체적인 숫자가 "N"으로 대체되어 있다. PTB 말뭉치에서는 한 문장이 하나의 줄로 저장되어 있다. 이 책에서는 각 문장을 연결한 '하나의 큰 시계열 데이터'로 취급한다. 이때 각 문장 끝에 \<eos> 라는 특수 문자를 삽입한다.

