---
layout: post
title: Chapter 07. RNN을 사용한 문장 생성
comments: true
categories: [Deep Learning from Scratch 2]
tags: [Deep Learning, Machine Learning, NLP, RNN, LSTM]
author: lsjhome

---

# Chapter 07 RNN을 사용한 문장 생성

seq2seq라는 새로운 구조의 신경망을 다뤄 보자. seq2seq란 sequence to sequence를 뜻하는 말로, 한 시계열 데이터를 다른 시계열 데이터로 변환하는 것을 뜻한다.

## 7.1 언어 모델을 사용한 문장 생성

앞 장에서는 LSTM 계층을 이용해서 언어 모델을 구현했는데, 그 모델의 신경망 구성은 아래 그림처럼 생겼었다. 

![fig 7-1](/Users/jin/projects/lsjhome.github.io/assets/img/dlfs2/deep_learning_2_images/fig 7-1.png)

친숙한 "you say goodbye and I say hello."라는 말뭉치로 학습한 언어 모델을 예로 생각해 보자. 이 학습된 언어 모델에 "I"라는 단어를 입력으로 주면 어떻게 될까? 이 언어 모델은 아래와 같은 확률분포를 출력한다.

![fig 7-2](/Users/jin/projects/lsjhome.github.io/assets/img/dlfs2/deep_learning_2_images/fig 7-2.png)

언어 모델은 지금까지 주어진 단어들에서 다음에 출현하는 단어의 확률분포를 출력한다. "say"라는 단어가 확률적으로 선택되었다고 하자. 

![fig 7-3](/Users/jin/projects/lsjhome.github.io/assets/img/dlfs2/deep_learning_2_images/fig 7-3.png)

> 결정적(deterministic)이란, 알고리즘의 결과가 하나로 정해지는 것, 결과가 예측 가능한 것을 뜻한다. 예컨대 앞의 예에서 확률이 가장 높은 단어를 선택하도록 하면, 그것은 '결정적'인 알고리즘이다. 한편, 확률적인 알고리즘에서는 결과가 확률에 따라 정해진다. 따라서, 선택되는 단어는 실행할 때마다 달려진다.(혹은 달라질 수 있다.)

두번째 단어 또한 방금 생성한 단어인 "say"를 언어 모델에 입력하면 다음 단어의 확률분포를 얻을 수 있다. 

![fig 7-4](/Users/jin/projects/lsjhome.github.io/assets/img/dlfs2/deep_learning_2_images/fig 7-4.png)

여기서 주목할 점은, 이렇게 생성한 문장은 훈련 데이터에는 존재하지 않는, 말 그대로 새로 생성된 문장이라는 것이다. 왜냐하면 언어 모델은 훈련 데이터를 암기한 것이 아니라, 훈련 데이터에서 사용된 단어의 정렬 패턴을 학습한 것이기 때문이다. 만약 언어 모델이 맒우치로부터 단어의 출현 패턴을 올바르게 학습할 수 있다면, 그 모델이 새로 생성하는 문장은 우리 인간에게도 자연스럽고 의미가 통하는 문장일 것으로 기대할 수 있다.

### 7.1.2 문장 생성 구현

문장을 생성하는 코드를 구현해 보자. Rnnlm 클래스를 상속해서 RnnlmGen 클래스를 만들고, 이 클래스에 문장 생성 메서드를 추가한다.

```python
# coding: utf-8
import sys
sys.path.append('..')
import numpy as np
from common.functions import softmax
from ch06.rnnlm import Rnnlm
from ch06.better_rnnlm import BetterRnnlm


class RnnlmGen(Rnnlm):
    def generate(self, start_id, skip_ids=None, sample_size=100):
        word_ids = [start_id]

        x = start_id
        while len(word_ids) < sample_size:
            x = np.array(x).reshape(1, 1)
            score = self.predict(x)
            p = softmax(score.flatten())

            sampled = np.random.choice(len(p), size=1, p=p)
            if (skip_ids is None) or (sampled not in skip_ids):
                x = sampled
                word_ids.append(int(x))

        return word_ids
```

문장 생성을 담당하는 generate(start\_id, skip\_ids, sample\_size) 메서드를 살펴보자. start\_id는 최초로 주는 단어의 ID, sample\_size는 샘플링 하는 단어의 수를 뜻한다. 그리고 skip\_ids는 단어 id의 리스트인데 (예컨대 [12, 20]), 이 리스트에 속하는 단어 ID는 샘플링되지 않도록 해준다. 이 인수는 PTB 데이터셋에 있는 \<unk>나 N등, 전처리된 단어를 샘플링하지 않게 하는 용도로 사용한다.

generate() 메서드는 가장 먼저 model.predict(x)를 호출해 각 단어의 점수를 출력한다. (점수는 정규화되기 전의 값이다.) 그리고 p=softmax(score) 코드에서는 이 점수들을 소프트맥스 함수를 이용해 정규화한다. 이것으로 목표로 하는 확룰분포 p를 얻을 수 있다. 그런 다음 확률분포 p로부터 다음 단어를 샘플링한다.

> model의 predict() 메서드는 미니배치 처리를 하므로 입력 x는 2차원 배열이어야 한다. 그래서 단어 ID를 하나만 입력하더라도 미니배치 크기를 1로 간주해 1 X 1 넘파이 배열로 reshape 한다.

문장 생성을 위한 코드는 다음과 같다.

```python
# coding: utf-8
import sys
sys.path.append('..')
from rnnlm_gen import RnnlmGen
from dataset import ptb


corpus, word_to_id, id_to_word = ptb.load_data('train')
vocab_size = len(word_to_id)
corpus_size = len(corpus)

model = RnnlmGen()
model.load_params('../ch06/Rnnlm.pkl')

# start 문자와 skip 문자 설정
start_word = 'you'
start_id = word_to_id[start_word]
skip_words = ['N', '<unk>', '$']
skip_ids = [word_to_id[w] for w in skip_words]
# 문장 생성
word_ids = model.generate(start_id, skip_ids)
txt = ' '.join([id_to_word[i] for i in word_ids])
txt = txt.replace(' <eos>', '.\n')
>>> print(txt)

# you will continue to collect and homes.
# if some stocks may be to deemed aware of solutions to its supply and management between the proposition of thinking said regard to the funds of the structure d.c..
# to reduce rate postponed he says achievement pass promising market achievement could turn according to a safer stereo fed.
# however averaged more at least about the feet of bank targets.
# have been aware of commissioned says white football officials of arafat in our title in the mine is the last thing removing the trade fund heavy concessions employment the executives
```

문법적으로 이상하고 의미도 통하지 않지만, 그럴듯한 문장도 섞여 있다. '완벽한 문장'은 섞여있지 않지만, 더 자연스러운 문장이 필요하다. 더 나은 언어 모델을 쓰면 가능하다. 

## 7.2 seq2seq

시계열 데이터를 또 다른 시계열 데이터로 변환하는 문제도 생각해 볼 수 있다. 가령 기계 번역이나 음성 인식을 예로 들 수 있다. 그 외에도 챗봇처럼 대화하는 애플리케이션이나 컴파일러 처럼 소스 코드를 기계여로 변환하는 작업도 그 예가 된다.

2개의 RNN 을 이용하는 **seq2seq**(sequence to sequence)라는 방법을 살펴보자.

### 7.2.1 seq2seq의 원리

seq2seq를 Encoder-Decoder 모델이라고도 한다. 이름이 말해주듯, 여기에는 2개의 Encoder와 Decoder가 등장한다. 문자 그대로 Encoder는 입력 데이터를 인코딩(부호화)하고, Decoder는 인코딩된 데이터를 디코딩(복호화) 한다.

가령 "나는 고양이로소이다"라는 문장을 "I am a cat"으로 번역하면 아래와 같다.

![fig 7-5](/Users/jin/projects/lsjhome.github.io/assets/img/dlfs2/deep_learning_2_images/fig 7-5.png)

위 그림에서 먼저 Encoder가 "나는 고양이로소이다" 라는 출발어 문장을 인코딩한다. 이어서 그 인코딩한 정보를 Decoder에 전달하고, Decoder가 도착어 문장을 생성한다. 이때 Encoder가 인코딩한 정보에는 번역에  필요한 정보가 조밀하게 응축되어 있다. Decoder는 이 조밀하게 응축된 정보를 바탕으로 도착어 문장을 생성한다.

이것이 seq2seq의 전체 그림이다. Encoder와 Decoder가 협력하여 시계열 데이터를 다른 시계열 데이터로 변환하는 것이다. 그리고 Encoder와 Decoder로는 RNN을 사용할 수 있다.

![fig 7-6](/Users/jin/projects/lsjhome.github.io/assets/img/dlfs2/deep_learning_2_images/fig 7-6.png)

Encoder는 RNN을 이용해 시계열 데이터를 **h** 라는 은닉 상태 벡터로 변환한다. 지금 예에서는 RNN으로써 LSTM을 이용했지만, 단순한 RNN이나 GRU 등도 이용할 수 있다. 그리고 여기에서는 우리말 문장을 단어 단위로 쪼개 입력한다고 가정한다.

Encoder가 출력하는 벡터 **h**는 LSTM 계층의 마지막 은닉 상태이다. 이 마지막 은닉 상태 **h**에 입력 문장(출발어)을 번역하는데 필요한 정보가 인코딩된다. 여기서 중요한 점은 LSTM의 은닉 상태 **h**는 고정 길이 벡터라는 사실이다. 그래서 인코딩한다 함은 결국 임의 길이의 문장을 고정 길이 벡터로 변환하는 작업이 된다.

![fig 7-7](/Users/jin/projects/lsjhome.github.io/assets/img/dlfs2/deep_learning_2_images/fig 7-7.png)

위 그림에서 보듯, Encoder는 문장을 고정 길이 벡터로 변환한다. 그렇다면 Decoder는 이 인코딩한 벡터를 어떻게 '요리'하여 도착어 문장을 생성하는 것일까?

![fig 7-8](/Users/jin/projects/lsjhome.github.io/assets/img/dlfs2/deep_learning_2_images/fig 7-8.png)

위 그림과 같이, Decoder는 앞 절의 신경망과 완전히 같은 구성이다. 다만, LSTM 계층이 벡터 **h**를 입력받는다는 점이 다르다. 참고로, 앞 절의 언어 모델에서는 LSTM 계층이 아무것도 받지 않았다. (굳이 따지자면, 은닉 상태로 '영벡터'를 받았다고 할 수 있다.) 이처럼 단 하나의 사솧나 차이가 평범한 모델을 번역도 해낼 수 있는 Decoder로 탈바꿈시킨다.

다음은 Decoder와 Encoder를 연결한 계층 구성을 살펴보자.

![fig 7-9](/Users/jin/projects/lsjhome.github.io/assets/img/dlfs2/deep_learning_2_images/fig 7-9.png)

seq2seq는 LSTM 두개 (Encoder의 LSTM과 Decoder의 LSTM)로 구성된다. 이때 LSTM 계층의 은닉 상태가 Encoder와 Decoder를 이어주는 '가교'가 된다. 순전파 때는 Encoder에 인코딩된 정보가 LSTM 계층의 은닉 상태를 통해 Decoder에 전해진다. 그리고 seq2seq의 역전파 때는 이 '가교'를 통해 기울기가 Decoer에서 Encoder로 전해진다.

### 7.2.2 시계열 데이터 변환용 장난감 문제

![fig 7-10](/Users/jin/projects/lsjhome.github.io/assets/img/dlfs2/deep_learning_2_images/fig 7-10.png)

### 7.2.3 가변 길이 시계열 데이터

덧셈 문장("57 + 5"나 "628 + 521" 등)이나 그 대답("62"나 "1149")의 문자 수가 문제마다 다르다는 것이다. 예컨대 "57+5"는 4문자이고, "628+521"은 총 7문자이다.

이처럼 '덧셈' 문제에서는 샘플마다 데이터의 시간 방향 크기가 다르다. '가변 길이 시계열 데이터'를 다룬다는 뜻이다. 따라서 신경망 학습 시 '미니배치 처리'를 하려면 무언가 추가 노력이 필요하게 된다.

> 미니배치로 학습할 때는 다수의 샘플을 한꺼번에 처리한다. 이때 (우리 구현에서는) 한 미니배치에 속한 샘플들의 데이터 형상이 모두 똑같아야 한다.

가변 길이 시계열 데이터를 미니배치로 학습하기 위한 가장 단순한 방법은 **패딩**을 사용하는 것이다. 패딩이란 원래의 데이터에 의미 없는 데이터를 채워 모든 데이터의 길이를 균일하게 맞추는 기법이다. 아래 그림은 이번 덧셈 문제에 적용한 모습이다. 모든 입력 데이터의 길이를 통일하고, 남는 공간에는 의미 없는 데이터(여기서는 '공백')를 채운걸 볼 수 있다.

![fig 7-11](/Users/jin/projects/lsjhome.github.io/assets/img/dlfs2/deep_learning_2_images/fig 7-11.png)

이번 문제에서는 0~999 사이의 숫자 2개만 더하기로 한다. 따라서 '+'까지 포함하면 입력의 최대 문자 수는 7이 된다. 자연스럽게, 덧셈 결과는 최대 4문자가 된다. (999+999=1998) 더불어 정답 데이터에도 패딩을 수행해 모든 샘플 데이터의 길이를 통일한다. 그리고 질문과 정답을 구분하기 위해 출력 앞에 구분자로 밑줄(_)을 붙이기로 한다. 그 결과 출력 데이터는 총 5문자로 통일한다. 참고로, 이 구분자는 Decoder에 문자열을 생성하라고 알리는 신호로 사용된다.

이처럼 패딩을 적용해 데이터 크기를 통일시키면 가변 길이 시계열 데이터도 처리할 수 있다. 그러나 원래는 존재하지 않았던 패딩용 문자까지 seq2seq가 처리하게 된다. 따라서 패딩을 적용해야 하지만 정확성이 중요하다면 seq2seq에 패딩 전용 처리를 추가해야 한다. 가령 Decoder에 입력된 데이터가 패딩이라면 손실의 결과에 반영하지 않도록 한다. (Softmax with Loss 계층에 '마스크' 기능을 추가해 해결할 수 있다.) 한편 Encoder에 입력된 데이터가 패딩이라면 LSTM 계층이 이전 시각의 입력을 그대로 출력하게 한다. 즉, LSTM 계층은 마치 처음부터 패딩이 존재하지 않았던 것처럼 인코딩할 수 있다.

### 7.2.4 덧셈 데이터셋

![fig 7-12](/Users/jin/projects/lsjhome.github.io/assets/img/dlfs2/deep_learning_2_images/fig 7-12.png)

```python
# coding: utf-8
import sys
sys.path.append('..')
from dataset import sequence


(x_train, t_train), (x_test, t_test) = \
    sequence.load_data('addition.txt', seed=1984)
char_to_id, id_to_char = sequence.get_vocab()

print(x_train.shape, t_train.shape)
print(x_test.shape, t_test.shape)
# (45000, 7) (45000, 5)
# (5000, 7) (5000, 5)

print(x_train[0])
print(t_train[0])
# [ 3  0  2  0  0 11  5]
# [ 6  0 11  7  5]

print(''.join([id_to_char[c] for c in x_train[0]]))
print(''.join([id_to_char[c] for c in t_train[0]]))
# 71+118
# _189
```

이처럼 sequence 모듈을 이용하면 seq2seq용 데이터를 간단히 읽어 들일 수 있다. 여기에서 x\_train과 t\_train에는 '문자 ID'가 저장되어 있다. 또한 문자 ID와 문자의 대응 관계는 char\_to\_id와 id\_to\_char를 이용해 상호 변환할 수 있다.

> 정석대로라면 데이터셋을 3개(훈련용, 검증용, 테스트용) 으로 나누어서 사용해야 한다. 훈련용 데이터로는 학습을 하고, 검증용 데이터로는 하이퍼파라미터를 튜닝한다. 마지막으로 테스트용 데이터로는 모델의 성능을 평가한다. 다만, 여기에서는 이야기를 단순하게 하고자, 훈련용과 테스트용으로만 분리하여 모델을 훈련시키고 평가한다.

## 7.3 seq2seq 구현

seq2seq는 2개의 RNN을 연결한 신경망이다. 먼저 두 RNN을 Encoder 클래스와 Decoder  클래스로 각각 구현한다. 그런 다음 두 클래스를 연결하는 Seq2seq 클래스를 구현하는 흐름으로 진행해 보자.