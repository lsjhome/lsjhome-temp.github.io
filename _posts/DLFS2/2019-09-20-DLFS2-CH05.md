---
layout: post
title: Chapter 05. RNN
comments: true
categories: [Deep Learning from Scratch 2]
tags: [Deep Learning, Machine Learning, NLP]
author: lsjhome



---

# Chapter 5 RNN

**피드포워드** 는 흐름이 단방향인 신경망을 말한다. 입력 신호가 다음 층(중간층)으로 전달되고, 그 신호를 받은 층은 그 다음 층으로 전달하고, 다시 다음 식으로... 식으로 한 방향으로만 신호가 전달된다.

피드포워드 신경망은 구성이 단순하여 구조를 이해하기 쉽고, 많은 문제에 응용할 수 있지만, 시계열 데이터를 잘 다루지 못한다.

## 5.1 확률과 언어 모델

### 5.1.1 word2vec을 확률 관점에서 바라보자

$w_{1}$, $w_{2}$ ,,, $$w_{T}$$ 라는 단어별로 표현되는 말뭉치를 생각해 보자. t번째 단어를 타깃으로, 그 전후 단어 (t-1번째와 t+1 번째)를 '맥락'으로 취급해 보자. 

![fig 5-1](/assets/img/dlfs2/deep_learning_2_images/fig 5-1.png)

$$W_{t-1}$$와 $$W_{t+1}$$이 주어졌을 때 타깃이 $$w_{t}$$ 이 될 확률을 수식으로 나타내 보자.

![e 5-1](/assets/img/dlfs2/deep_learning_2_images/e 5-1.png)

CBOW 모델은 위 식의 사후 확률을 모델링한다. 이것이 윈도우 크기가 1일때의 CBOW 모델이다.

여기서 맥락을 좌우 대칭으로 하는 대신, 왼쪽 윈도우만으로 한정해 보자.

![fig 5-2](/assets/img/dlfs2/deep_learning_2_images/fig 5-2.png)

왼쪽 두 단어만을 맥락으로 생각하면 아래와 같이 된다.

![e 5-2](/assets/img/dlfs2/deep_learning_2_images/e 5-2.png)

그런데 [식 5.2]의 표기를 사용하면, CBOW 모델이 다루는 손실 함수를 아래 식처럼 쓸 수 있다.

![e 5-3](/assets/img/dlfs2/deep_learning_2_images/e 5-3.png)

CBOW 모델의 학습으로 수행하는 일은 [식 5.3]의 손실 함수(정확히는 맒우치 전체의 손실 함수의 총합)을 최소화하는 가중치 매개변수를 찾는 일이다. 이러한 가중치 매개변수가 발견되면 CBOW 모델은 맥락으로부터 타깃을 더 정확하게 추측할 수 있게 된다.

이처럼 CBOW 모델을 학습시키는 본래 목적은 맥락으로부터 타깃을 정확하게 추측하는 것이다. 이 목적을 위해 학습을 진행하면, (그 부산물로) 단어의 의미가 인코딩된 '단어의 분산 표현'을 얻을 수 있다.

그럼 CBOW 모델의 본래 목적인 '맥락으로부터 타깃을 추측하는 것'은 어디에 이용할 수 있을까? 여기서 언어 모델이 등장한다.

### 5.1.2 언어 모델

**언어 모델** 은 단어 나열에 확률을 부여한다. 특정한 단어의 시퀀스에 대해서, 그 시퀀스가 일어날 가능성이 어느 정도인지(얼마나 자연스러운 단어 순서인지)를 확률로 평가하는 것이다. 예컨대 "you say goodbye"라는 단어 시퀀스에는 높은 확률을 출력하고, "you say good die"에는 낮은 확률ㅇ르 출력하는 것이 일종의 언어 모델이다.

이 언어 모델은 다양하게 응용할 수 있다. 가령 음성 인식 시스템의 경우, 사람의 음성으로부터 몇 개의 문장을 후보로 생성하고, 언어 모델을 사용하여 후보 문장이 '문장으로써 자연스러운지'를 기준으로 순서를 매길 수 있다.

또한 언어 모델은 새로운 문장을 생성하는 생성하는 용도가 될 수도 있다. 언어 모델은 단어의 자연스러움을 확률적으로 평가할 수 있으므로, 확률분포에 따라 다음으로 적당한 단어를 '자아낼'(샘플링) 수 있기 때문이다.

언어 모델을 수식으로 설명하면 다음과 같다.

![e 5-4](/assets/img/dlfs2/deep_learning_2_images/e 5-4.png)

위의 식에서 알 수 있듯이, 목적으로 하는 동시 확률 $$P(w_{1}, ..., w_{m}) $$ 은 사후 확률의 총 곱인 $$ \prod P(w_{t}| w_{1}, w_{2}, ... w_{t-1})$$ 으로 대표될 수 있다. 여기서 주목할 것은 이 사후 확률은 타깃 단어보다 왼쪽에 있는 모든 단어를 맥락(조건)으로 했을 때의 확률이라는 것이다.

![fig 5-3](/assets/img/dlfs2/deep_learning_2_images/fig 5-3.png)

즉,  $$P(w_{1}, ..., w_{m}) $$ 이라는 확률을 얻는 것이 목표이고, 이 확률을 계싼할 수 있따면 언어 모델의 동시 확률 $$P(w_{1}, w_{2}, w_{3}, ... w_{m})$$ 을 구할 수 있다.

> $$P(w_{t}| w_{1}, w_{2}, ... w_{t-1})$$ 을 나타내는 모델은 **조건부 언어 모델**(Conditional Language Model)이라고 한다. 한편, 이를 '언어 모델'이라고 하는 경우도 많다.

### 5.1.3 CBOW 모델은 언어 모델로?

그렇다면 word2vec의 CBOW 모델을 언어 모델로 적용하려면 어떻게 해야 할까? 이는 맥락의 크기를 특정 값으로 한정하여 근사적으로 나타낼 수 있다. 수식으로는 다음과 같다.

![e 5-8](/assets/img/dlfs2/deep_learning_2_images/e 5-8.png)

여기서는 맥락을 왼쪽 2개의 단어로 한정한다.

> 머신러닝이나 통계학에서는 **마르코프 연쇄**(Markov Chain) 또는 **마르코프 모델**(Markov Model)이라는 말을 자주 사용한다. 마르코프 연쇄란 미래의 상태가 현재 상태에만 의존해 결정되는 것을 뜻한다. 또한 이 사상의 확률이 '그 직전' N개의 사건에만 의존할 때, 이를 'N층 마르코프 연쇄' 라고 한다. 이번 예는 직전 2개의 단어에만 의존해 다음 단어가 정해지는 모델이므로 '2층 마르코프 연쇄' 라고 부를 수 있다.

이 예에서는 맥락으로 2개의 단어를 이용했지만, 이 맥락의 크기는 임의 길이로 설정할 수 있다. 그러나 임의 길이로 설정할 수 있다고 해도, 결국 특정 길이로 '고정' 된다. 가령 왼쪽 10개의 단어를 맥락으로 CBOW 모델을 만든다면, 그 맥락보다 더 왼쪽에 있는 단어의 정보는 무시된다.

![fig 5-4](/assets/img/dlfs2/deep_learning_2_images/fig 5-4.png)

이 문제에서 정답을 구하려면 "?"로부터 18번째나 앞에 있는 "Tom"을 기억해야 한다. 만약 CBOW 모델의 맥락이 10개까지였다면 이 문제에 재대로 답할 수 없을 것이다.

CBOW 모델의 맥락 크기를 키운다고 해도, CBOW모델에는 맥락 안의 단어 순서가 무시된다는 한계가 있다.

> CBOW란 Continuous bag-of-words의 약어이다. bag-of-words란 '가방 안의 단어'를 뜻하는데, 여기에서는 가방 속의 단어 '순서'는 무시된다는 뜻도 내포한다.

맥락의 단어 순서가 무시되는 문제의 구체적인 예를 보자. 예컨대 맥락으로 2개의 단어를 다루는 경우, CBOW 모델에서는 이 2개의 단어 벡터의 '합'이 은닉층에 온다.

![fig 5-5](/assets/img/dlfs2/deep_learning_2_images/fig 5-5.png)

즉 왼쪽 그림과 같이 은닉층에서는 단어 벡터들이 더해지므로 맥락 단어의 순서는 무시된다.

이상적으로는 맥락의 단어 순서도 고려한 모델이 바람직할 것이다. 이를 위해 오른쪽처럼 맥락의 단어 벡털르 은닉층에서 **연결** 하는 방식을 생각할 수 있따. 실제, 신경 확률론적 언어 모델에서 제안한 모델은 이 방식을 취한다. 그러나 연결하는 방식을 취하면 맥락의 크기에 비례해 가중치 매개변수도 늘어나게 된다. 물론, 매개변수가 증가한다는 것은 환영할만한 일이 아니다.

여기서 RNN이 등장한다. RNN은 맥락이 아무리 길더라도 그 맥락의 정보를 기억하는 메커니즘을 갖추고 있다. 그래서 RNN을 사용하면 아무리 긴 시계열 데이터에라도 대응할 수 있다.

## 5.2 RNN이란

> Recurrent Neural Network는 우리말로 '순환 신경망'으로 번역한다. 한편 Recursive N/eural Network 라는 신경망도 있다. 이는 주로 트리 구조의 데이터를 처리하기 위한 신경망으로, 순환 신경망과는 다르다.

### 5.2.1 순환하는 신경망

"순환한다"에는 어떤 의미가 있을까? 물론 "반복해서 되돌아감"을 의미한다. 원래 한 지점에서 시작한 것이, 시간을 지나 다시 원래 장소로 돌아오는 것, 그리고 이 과정을 반복하는 것이 바로 "순환" 이다.

RNN의 특징은, 순환하는 경로(닫힌 경로)가 있다는 것이다. 이 순환 경로를 따라 데이터는 끊임없이 순환할 수 있다. 그리고 데이터가 순환되기 때문에 과거의 정보를 기억하는 동시에 최신 데이터로 갱신될 수 있는 것이다.

RNN에 이용되는 계층을 'RNN 계층'이라 한다. RNN 계층은 아래처럼 그릴 수 있다.

![fig 5-6](/assets/img/dlfs2/deep_learning_2_images/fig 5-6.png)

위 그림에서 RNN 계층은 순환하는 경로를 포함한다. 이 순환 경로를 따라 데이터를 계층 안에서 순환시킬 수 있다. 또한 $$x_{t}$$ 를 입력받는데, t는 시각을 뜻한다. 이는 시계열 데이터 ($$x_{0}, x_{1}, .., x_{t}, ...$$)가 RNN 계층에 입력됨을 표현한 것이다. 그리고 이 입력에 대응하여 ($$h_{0}, h_{1}, .., h_{t}, ...$$) 가 출력된다.

또한, 각 시각에 입력되는 $$x_{t}$$ 는 벡터라고 가정한다. 문장(단어 순서)을 다루는 경우를 예를 들자면, 각 단어의 분산 표현(단어 벡터)이 $$x_{t}$$가 되며, 이 분산 표현이 순서대로 하나씩 RNN 계층에 입력되는 것이다.

또한, 각 시각에 입력되는 $$x_{t}$$는 벡터라고 가정하자. 문장(단어 순서)을 다루는 경우를 예로 든다면, 각 단어의 분산 표현(단어 벡터)이 $$x_{t}$$가 되며, 이 분산 표현이 순서대로 하나씩 RNN 계층에 입력되는 것이다.

> 위 그림을 보면 출력이 2개로 분기하고 있음을 알 수 있다. 여기서 말하는 '분기'란 같은 것이 복제되어 분기함을 의미한다. 그리고 이렇게 분기도니 출력 중 하나가 자기 자신에 입력된다. (즉 순환한다.)

![fig 5-7](/assets/img/dlfs2/deep_learning_2_images/fig 5-7.png)

### 5.2.2 순환 구조 펼치기

RNN 계층의 순환 구조에 대해 살펴보자. 

![fig 5-8](/assets/img/dlfs2/deep_learning_2_images/fig 5-8.png)

위 그림에서 보듯, RNN 계층의 순호나 구조를 펼침으로써 오른쪽으로 성장하는 긴 신경망으로 변신시킬 수 있다. 지금까지 본 피드포워드 신경망과 같은 구조이다. 지금까지 본 피드포워드 신경망과 같은 구조이다. (피드포워드에서는 데이터가 한 방향으로만 흐른다). 다만, [그림 5-8]에 등장하는 다수의 RNN 계층 모두가 실제로는 '같은 계층'인 것이 지금까지와의 신경망과의 차이다.

> 시계열 데이터는 시간 방향으로 데이터가 나열된다. 따라서 시계열 데이터의 인덱스를 가리킬 때는 "시각"이라는 용어를 사용한다. (가령 시각 t의 입력 데이터 $$x_{t}$$ 등) 자연어의 경우에도 't번째 단어'나 't번째 RNN 계층' 이라는 표현도 사용하지만, '시각 t의 단어'나 '시각 t의 RNN 계층'처럼 표현하기도 한다.

각 시각의 RNN 계층은 그 계층으로부터 입력과 1개 전의 RNN 계층으로부터의 출력을 받는다. 그리고 이 두 정보를 바탕으로 현 시각의 출력을 계산한다. 이때 수행하는 계산의 수식은 다음과 같다. 

![e 5-9](/assets/img/dlfs2/deep_learning_2_images/e 5-9.png)

우선 위 식에 쓰인 기호들을 설명해 보자면, RNN에는 가중치가 2개 있다. 하나는 입력 **x** 를 출력 **h** 로 변환하기 위한 가중치 $$W_{x}$$ 이고, 다른 하나는 1개의 RNN 출력을 다음 시각의 출력으로 변환하기 위한 가중치 $$W_{h}$$ 이다. 또한 편향 b도 있다. 참고로 $$h_{t-1}$$ 과 $$x_{t}$$는 행 벡터이다.

현재의 출력($$h_{t}$$)는 한 시각 이전 출력 $$h_{t-1}$$ 에 기초해 계산됨을 알 수 있다. 다른 관점으로 보면, RNN은 h라는 '상태'를 가지고 있으며, 위 식의 형태로 갱신된다고 해석할 수 있다. 그래서 RNN 계층을 '상태를 가지는 계층' 혹은 '메모리(기억력)'가 있는 계층이라 한다.

> RNN의 h는 '상태'를 기억해 시각이 1 스텝(1단위) 진행될 때마다 위의 식의 형태로 갱신된다. 많은 문헌에서 RNN의 출력 $$h_{t}$$ 를 은닉 상태(hidden state) 혹은 은닉 상태 벡터(hidden state vector)라고 부른다. 이 책에서도 RNN의 출력 $$h_{t}$$를 '은닉 상태' 또는 '은닉 상태 벡터'로 부르겠다.

![fig 5-9](/assets/img/dlfs2/deep_learning_2_images/fig 5-9.png)

이 책에서는 오른쪽 그림과 같이 하나의 출력이 분기하는 것임을 명시한다.

### 5.2.3 BPTT

RNN 계층은 가로로 펼친 신경망으로 간주할 수 있다. 따라서 학습도 보통의 신경망과 같은 순서로 진행할 수 있다. 아래 그림과 같이 된다.

![fig 5-10](/assets/img/dlfs2/deep_learning_2_images/fig 5-10.png)

여기서도 일반적인 오차역전파법을 적용할 수 있다. 먼저 순전파를 수행하고, 이어서 역전파를 수행하여 원하는 기울기를 구할 수 있다. 여기서의 오차역전파법은 '시간 방향으로 펼친 신경망의 오차 역전파법' 이란 뜻으로, **BPTT**(Backpropagation Through Time) 이라고 한다.

하지만 긴 시계열 데이터를 학습할 때 문제가 있다. 시계열 데이터의 시간 크기가 커지는 것에 비례하여 BPTT가 소비하는 컴퓨팅 자원도 증가하기 때문이다. 시간 크기가 커지면 역전파 시의 기울기가 불안정해지는 것도 문제이다.

> BPTT를 이용해 기울기를 구하려면, 매 시각 RNN 계층의 중간 데이터를 메모리에 유지하지 않으면 안 된다. 따라서 시계열 데이터가 길어짐에 따라 (계산량 뿐 아니라) 메모리 사용량도 증가하게 된다.

### 5.2.4 Truncated BPTT

큰 시계열 데이터를 취급할 때에는 흔히 신경망 연결을 적당한 길이로 끊는다. 시간축 방향으로 너무 길어진 신경망을 적당한 지점에서 잘라내어 작은 신경망 여러 개로 만든다는 아이디어다. 그리고 이 잘라낸 작은 신경망에서 오차역전파법을 수행한다. 이것이 바로 **Truncated BPTT** 라는 기법이다.

> Truncated BPTT는 적당한 길이로 '잘라낸' 오차역전파법 이라는 뜻이다.

제대로 구현하려면 '역전파'의 연결만 끊으면 된다. 순전파의 연결은 반드시 그대로 유지해야 한다. 즉, 순전파의 흐름은 끊어지지 않고 전파된다. 다만 역전파의 연결은 적당한 길이로 잘라내, 그 잘라낸 신경망 단위로 학습을 수행한다.

가령 Truncated BPTT를 구체적인 예를 통해 살펴보자. 길이가 1000인 시계열 데이터를 가정해 보자. 자연어 문제에서라면 단어 1000개에 해당하는 말뭉치에 해당한다. 여러 문장을 연결한 것을 하나의 큰 시계열 데이터로 취급하자. 

그런데 길이가 1000인 시계열 데이터를 다루면서 RNN 계층을 펼치면 계층이 가로로 1000개나 늘어난 신경망이 된다. 물론 계층이 아무리 늘어나도 오차역전파법으로 기울기를 계산할 수는 있다. 다만 너무 길면 계산량과 메모리 사용량 등이 문제가 된다. 또한 계층이 길어짐에 따라 신경망을 하나 통괗ㄹ 때마다 기울기 값이 조금씩 작아져서, 이전 시각 t까지 역전파되기 전에 0이 되어 소멸할 수 있다. 이런 이유로 아래 그림처럼 길게 뻗은 신경망의 역전파에서는 연결을 적당한 길이로 끊는다.

![fig 5-11](/assets/img/dlfs2/deep_learning_2_images/fig 5-11.png)

위 그림에서는 RNN 계층을 길이 10개 단위로 학습할 수 있도록 역전파의 연결을 끊었다. 이처럼 역전파의 연결을 잘라버리면, 그보다 미래의 데이터에 대해서는 생각할 필요가 없어진다. 따라서 각각의 블록 단위로, 미래의 블록과는 독립적으로 오차역전파법을 완결시킬 수 있다.

여기서 중요한 점은 역전파의 연결은 끊어지지만, 순전파의 연결은 끊기지 않는다는 것이다. 그러므로 RNN을 학습시킬 때는 순전파가 연결된다는 것을 고려해서, 데이터를 '순서대로' 입력해야 한다. 

> 지금까지 본 신경망에서는 미니배치 학습을 수행할 때 데이터를 무작위로 선택해 입력했다. 하지만 RNN에서 Truncated BPTT를 수행할 때는 데이터를 '순서대로' 입력해야 한다.

이제 Truncated BPTT 방식으로 RNN을 학습시켜 보자. 가장 먼저 할 일은 첫번째 블록 데이터($$x_{0}, x_{1}, ... x_{9}$$)를 RNN 계층에 제공하는 것이다. 아래 그림과 같다.

![fig 5-12](/assets/img/dlfs2/deep_learning_2_images/fig 5-12.png)

먼저 순전파를 수행하고, 그 다음 역전파를 수행한다. 이렇게 하여 원하는 기울기를 구할 수 있다. 이어서 다음 블록의 입력 데이터($$x_{10}$$ 에서 $$x_{19}$$)를 입력해 오차역전파법을 수행한다.

![fig 5-13](/assets/img/dlfs2/deep_learning_2_images/fig 5-13.png)

첫 번째 블록과 마찬가지로 순전파를 수행한 다음, 역전파를 수행한다. 그리고 순전파 계산에는 앞 블록의 마지막 은닉 승태인 $$h_{9}$$ 가 필요하다.

같은 요령으로, 3번째 블록을 대상으로 학습을 수행한다. 이때도 두 번째 블록의 마지막 은닉 상태($$h_{19}$$)를 이용할 수 있다. 이처럼 RNN 학습에서는 데이터를 순서대로 입력하며, 은닉 승태를 계승하면서 학습을 수행한다. 지금까지의 설명으로 RNN 학습의 흐름은 아래 그림처럼 되는 것을 확인할 수 있다.

![fig 5-14](/assets/img/dlfs2/deep_learning_2_images/fig 5-14.png)

Truncated BPTT에서는 데이터를 순서대로 입력해 학습한다. 이런 식으로 순전파의 연결을 유지하면서 블록 단위로 오차역전파법을 적용할 수 있다.

### 5.2.5 Truncated BPTT의 미니배치 학습

지금까지는 Truncated BPTT 이야기에서는 미니배치 학습 시 가각ㄱ의 미니배치가 어떤 식으로 이뤄지는지는 생각하지 않았다. 즉, 지금까지의 이야기는 미니배치 수가 1일때 적당하다.

길이가 1000인 시계열 데이터에 대해서, 시각의 길이를 10개 단위로 잘라 Truncated BPTT로 학습하는 경우를 예로 설명해 보자. 그러면 이때 미니배치의 수를 2개로 지정해서 학습하려면 어떻게 해야 할까?

![fig 5-15](/assets/img/dlfs2/deep_learning_2_images/fig 5-15.png)

이처럼 미니배치 학습을 수행할 때는 각 미티배치의 시작 위치를 오프셋으로 옮겨준 후 순서대로 제공하면 된다. 또한 데이터를 순서대로 입력하다가 끝에 도달하면 다시 처음부터 입력하면 된다.

### 5.3 RNN 구현

![fig 5-16](/assets/img/dlfs2/deep_learning_2_images/fig 5-16.png)

길이가 T인 시계열 데이터를 받늗나. 그리고 각 시각은 은닉 상태를 T개 출력한다. 그리고 모듈화를 생각해, 옆으로 성장한 [그림 5-16]의 산경망을 '하나의 계층'으로 구현한다. 그림으로 보면 [그림 5-17]처럼 된다. 

![fig 5-17 2](/assets/img/dlfs2/deep_learning_2_images/fig 5-17 2.png)

입력과 출력을 각각 하나로 묶으면 옆으로 늘어선 일련의 계층을 하나의 계층으로 간주할 수 있다. 즉 ($$x_{0}, x_{1}, x_{2}, ... x_{T-1}$$) 을 묶은 **xs** 을 입력하면 ($$h_{0}, h_{1}, h_{2}, ... h_{T-1}$$) 을 묶은 **hs** 를 출력하는 단일 계층으로 볼 수 있다. 이때 Time RNN 계층 내에서 한 단계의 작업을 수행하는 계층을 "RNN 계층"이라고 하고, T개 단계분의 작업을 한꺼번에 처리하는 계층을 "Time RNN 계층" 이라 한다. 

> Time RNN 같이 시계열 데이터를 한꺼번에 처리하는 계층에는 앞에 'Time'을 붙인다. 이는 이 책 독자적인 명명규칙이다. 가령 Time Affine 계층과 Time Embedding 계층의 경우 시계열 데이터를 한꺼번에 처리한다.

먼저 RNN의 한 단계를 처리하는 클래스를 RNN이란 이름으로 구현하고, 이 RNN 클래스를 구현해 T개 단계의 처리를 한꺼번에 수행하는 계층을 TimeRNN 이라는 이름의 클래스로 완성시키자.

### 5.3.1 RNN 계층 구현

RNN 처리를 한 단계만 수행하는 RNN 클래스부터 구현해보자. RNN의 순전파는 아래 식과 같다.

![e 5-10](/assets/img/dlfs2/deep_learning_2_images/e 5-10.png)

여기에서 우리는 데이터를 미니배치로 모아 처리한다. 따라서 $$x_{t}$$ 와 $$h_{t}$$ 에는 각 샘플 데이터를 행 방향에 저장한다. 한편, 행렬을 계산할 때에는 행렬의 '형상 확인'이 중요하다. 미니배치 크기가 N, 입력 벡터의 차원 수가 D, 은닉 상태의 차원 수가 H라면, 지금 계산에서의 형상 확인은 아래와 같다.

![fig 5-18](/assets/img/dlfs2/deep_learning_2_images/fig 5-18.png)

그림에서 보듯, 행렬의 형상 확인을 수행함으로써, 올바로 구현되었는지를 확인할 수 있다. 그럼 이상을 바탕으로 RNN 클래스의 초기화와 순전파 메서드를 구현해 보자.

```python
class RNN:
    def __init__(self, Wx, Wh, b):
        self.params = [Wx, Wh, b]
        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]
        self.cache = None

    def forward(self, x, h_prev):
        Wx, Wh, b = self.params
        t = np.dot(h_prev, Wh) + np.dot(x, Wx) + b
        h_next = np.tanh(t)

        self.cache = (x, h_prev, h_next)
        return h_next
```

RNN의 초기화 메서드는 가중치 2개와 편향 1개를 인수로 받는다. 여기에서는 인수로 받은 매개변수를 인스턴스 변수 params에 리스트로 저장한다. 그리고 각 매개변수에 대응하는 형태로 기울기를 초기화한 후 grads에 저장한다. 마지막으로 역전파 계산시 사용하는 중간 값을 담을 cache를 None으로 초기화한다.

순전파 forward(x, h\_prev)에서는 인수 2개인 아래로부터 입력 x와 왼쪽으로부터 입력 h\_prev를 받는다. 그 다음은 식을 그대로 코드로 옮긴다.

하나 앞의 RNN 계층으로부터 받는 입력이 h\_prev이고, 현 시각 RNN 계층으로부터의 출력은 h\_next 이다. 

순전파를 계산 그래프로 나타내면 아래와 같다.

![fig 5-19](/assets/img/dlfs2/deep_learning_2_images/fig 5-19.png)

역전파의 경우 아래와 같다.

![fig 5-20](/assets/img/dlfs2/deep_learning_2_images/fig 5-20.png)

이제 RNN 계층의 backward() 코드를 살펴보자. 다음과 같이 구현될 수 있다.

```python
class RNN:
    # ...
		# ...
		# ...
    def backward(self, dh_next):
        Wx, Wh, b = self.params
        x, h_prev, h_next = self.cache

        dt = dh_next * (1 - h_next ** 2)
        db = np.sum(dt, axis=0)
        dWh = np.dot(h_prev.T, dt)
        dh_prev = np.dot(dt, Wh.T)
        dWx = np.dot(x.T, dt)
        dx = np.dot(dt, Wx.T)

        self.grads[0][...] = dWx
        self.grads[1][...] = dWh
        self.grads[2][...] = db

        return dx, dh_prev
```

### 5.3.2 Time RNN 계층 구현

Time RNN 계층은  T개의 RNN 계층으로 구성된다. (T는 임의의 수로 설정할 수 있다.) Time RNN 계층은 아래 그림처럼 생겼다.

![fig 5-21](/assets/img/dlfs2/deep_learning_2_images/fig 5-21.png)

위 그림에서 보듯, Time RNN 계층은 RNN 계층 T개를 연결한 신경망이다. 이 신경망을 우리는 TimeRNN 클래스로 구현할 것이다. 그리고 여기에서는 RNN 계층의 은닉 상태 h를 인스턴스 변수로 유지한다. 이 변수를 아래 그림처럼 은닉 상태를 '인계'받는 용도로 이용한다.

![fig 5-22](/assets/img/dlfs2/deep_learning_2_images/fig 5-22.png)



위 그림처럼 RNN 계층의 은닉 상태를 Time RNN 계층에서 관리하기로 한다. 이렇게 하면 Time RNN 사용자는 RNN 계층 사이에서 은닉 상태를 '인계하는 작업'을 생각하지 않아도 된다는 장점이 생긴다. 그리고 이 책에서는 이 기능(은닉 상태를 인계받을지)을 stateful 이라는 인수로 조정할 수 있도록 했다.

먼저 초기화 메서드와 또 다른 메서드 2개를 살펴보자.



```python
class TimeRNN:
    def __init__(self, Wx, Wh, b, stateful=False):
        self.params = [Wx, Wh, b]
        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]
        self.layers = None

        self.h, self.dh = None, None
        self.stateful = stateful
        
    def set_state(self, h):
        self.h = h

    def reset_state(self):
        self.h = None

```

초기화 메서드는 가중치와 편향, 그리고 stateful 이라는 불리언 값을 인수로 받는다. 인스턴스 변수 중 layers는 다수의 RNN 계층을 리스트로 저장하는 용도이다. 인스턴수 변수 h는 foward() 메서드를 호출했을 때 마지막 RNN 계층의 은닉 상태를 저장하고, dh는 backward()를 불렀을 때 하나 앞 블록의 은닉 상태의 기울기를 저장한다.

> Time RNN 계층의 은닉 상태를 설정하는 메서드를 set\_state(h)로, 은닉 상태를 초기화하는 메서드를 reset\_state()로 구현했다.

앞의 인수 중 stateful은 '상태가 있는'이라는 뜻이다. 이 책의 구현에서는 stateful이 True 일때, Time RNN 계층은 '상태가 있다' 라는 뜻이다. 이는 Time RNN 계층이 은닉 상태를 유지한다는 뜻이다. 즉 아무리 긴 시계열이라도 Time RNN 계층의 순전파를 끊지 않고 전파한다는 의미다. 

반면 stateful이 False 일때 Time RNN 계층은 은닉 상태를 '영행렬'로 초기화한다. 이것이 상태가 없는 모드이며, '무상태' 라고 한다.

```python
    def forward(self, xs):
        Wx, Wh, b = self.params
        N, T, D = xs.shape
        D, H = Wx.shape

        self.layers = []
        hs = np.empty((N, T, H), dtype='f')

        if not self.stateful or self.h is None:
            self.h = np.zeros((N, H), dtype='f')

        for t in range(T):
            layer = RNN(*self.params)
            self.h = layer.forward(xs[:, t, :], self.h)
            hs[:, t, :] = self.h
            self.layers.append(layer)

        return hs
```

순전파 메서드인 forward(fx)는 아래로부터 입력 xs를 받는다. xx는 T개 분량의 시계열 데이터를 하나로 모은 것이다. 따라서 미니배치 크기를 N, 입력 벡터의 차원 수를 D라고 하면, xs의 형상은 (N, T, D)가 된다.

RNN 계층의 은닉 상태 h는 처음 호출 시 (self.h가 None일 때)에는 원소가 모두 0인 영행렬로 초기화된다. 그리고 인스턴스 변수 stateful이 False일 때도, 항상 영행렬로 초기화한다.

기본 구현에서는 처음 hs = np.empty((N, T, H), dtype='f') 문장에서 출력값을 담을 그릇(hs)를 준비한다. 이어서 총 T회 반복되는 for 문 안에서 RNN 계층을 생성하여 인스턴스 변수 layers에 추가한다. 그 사이에 RNN 계층이 각 시각 t의 은닉 상태 h를 계산하고, 이를 hs에 해당 인덱스 (시각)의 값으로 설정한다.

> Time RNN 계층의 forward() 메서드가 불리면, 인스턴스 변수 h에는 마지막 RNN 계층의 은닉 상태가 저장된다. 그래서 다음번 forward() 메서드 호출 시 stateful이 True면 먼저 저장된 h 값이 그대로 이용되고, stateful이 False면 다시 영행렬로 초기화된다.

이어서 Time RNN 계층의 역전파 구현이다. 이 역전파의 계산 그래프는 아래와 같다.

![fig 5-23](/assets/img/dlfs2/deep_learning_2_images/fig 5-23.png)위 그림과 같이, 여기에서는 상류(출력 쪽 층)에서 전해지는 기울기를 **dhs**로 쓰고, 하류로 내보내는 기울기를 **dxs** 로 사용한다. 여기서 우리는 Truncated BPTT를 수행하기 때문에, 이 블록의 이전 시각 역전파는 필요하지 않다. 단, 이전 시각의 으닌ㄱ 상태 기울기는 인스턴스 변수 dh에 저장해 둔다.

![fig 5-24](/assets/img/dlfs2/deep_learning_2_images/fig 5-24.png)

t번째 RNN 계층에서는 위로부터 기울기 $$dh_{t}$$ 와 '한 시각 뒤(미래) 계층' 으로부터의 기울기 $$dh_{next}$$가 전해진다. 여기서의 주의점은 RNN 계층의 순전파에서는 출력이 2개로 분기된다는 점이다. 순전파 시 분기했을 경우, 그 역전파에서는 각 기울기가 합산되어 전해진다. 따라서 역전파 시 RNN 계층에서는 합산된 기울기 ($$dh_{t} + dh_{next}$$)가 입력된다. 이상을 주의하여 역전파를 구현한다면 다음과 같다.

```python
    def backward(self, dhs):
        Wx, Wh, b = self.params
        N, T, H = dhs.shape
        D, H = Wx.shape

        dxs = np.empty((N, T, D), dtype='f')
        dh = 0
        grads = [0, 0, 0]
        for t in reversed(range(T)):
            layer = self.layers[t]
            dx, dh = layer.backward(dhs[:, t, :] + dh)
            dxs[:, t, :] = dx

            for i, grad in enumerate(layer.grads):
                grads[i] += grad

        for i, grad in enumerate(grads):
            self.grads[i][...] = grad
        self.dh = dh

        return dxs
```

가장 먼저 하류로 흘려보낼 기울기를 담을 그릇인 dxs를 만든다. 그리고 순전파와는 반대 순서로 RNN 계층의 backward() 메서드를 호출하여 각 시각의 기울기  dx를 구해 xs의 해당 인덱스에 저장한다. 그리고 가중치 매개변수에 대해서도 각  RNN 계층의 가중치 기울기를 합산하여 최종 결과를 멤버 변수 self.grads에 덮어쓴다.

> Time RNN 계층 안에는 RNN 계층이 여러 개 있다. 그리고 그 RNN 계층들에서 똑같은 가중치를 사용하고 있다. 따라서  Time RNN 계층의 최종 가중치의 기울기는 각  RNN 계층의 가중치 기울기를 모두 더한 것이 된다.



## 5.4 시계열 데이터 처리 계층 구현

이번 장의 목표는 RNN을 사용하여 '언어 모델'을 구현하는 것이다. 지금까지 RNN 계층과 시계열 데이터를 한꺼번에 처리하는  Time RNN 계층을 구현했는데, 이번 절에서는 시계열 데이터를 처리하는 계층을 몇 개 더 만들어 보자. 또한, RNN을 사용한 언어 모델은 영어로 RNN Language Model 이므로 앞으로 RNNLM이라 칭한다.

### 5.4.1 RNNLM의 전체 그림

![fig 5-25](/assets/img/dlfs2/deep_learning_2_images/fig 5-25.png)

![fig 5-26](/assets/img/dlfs2/deep_learning_2_images/fig 5-26.png)

입력 데이터는 단어 ID의 배열이다. 우선 첫 번째 시각에 주목해보자. 첫번째 단어로 단어 ID가 0인 "you"가 입력되고, 확률분포를 보면 "say"가 가장 높게 나온 것을 알 수 있다. 즉 "you" 다음에 출현하는 단어가 "say"라는 것을 올바르게 예측했다.

RNNLM은 지금까지 입력된 단어를 '기억'하고 그것을 바탕으로 다음에 출현할 단어를 예측한다. 이 일을 가능하게 하는 비결이 바로 RNN 계층의 존재이다. RNN 계층이 과거에서 현재로 데이터를 계속 흘려보내줌으로써 과거의 정보를 인코딩해 저장(기억)할 수 있는 것이다.



### 5.4.2 Time 계층 구현

시계열 데이터를 한꺼번에 처리하는 계층을 Time Embedding, Time Affine이라는 형태로 구현하자. 이 Time XX 계층들을 다 만들면 아래와 같다.

![fig 5-27](/assets/img/dlfs2/deep_learning_2_images/fig 5-27.png)

> 이처럼 시계열 데이터를 한꺼번에 처리하는 계층을 'Time XX' 계층이라고 부른다. 이러한 계층들이 구현되어 있다면, 그 계층들을 레고 블록처럼 조립하는 것만으로 시계열 데이터를 다루는 신경망을 완성할 수 있다.

![fig 5-28](/assets/img/dlfs2/deep_learning_2_images/fig 5-28.png)

Time Embedding 계층 역시 순전파 시 T개의 Embedding 계층을 준비하고 각  Embedding 계층이 각 시각의 데이터를 처리한다.

![fig 5-29](/assets/img/dlfs2/deep_learning_2_images/fig 5-29.png)

$$x_{0}$$ 이나 $$x_{1}$$ 등의 데이터는 아래층에서부터 전해지는 '점수'를 나타낸다. 또한 $$t_{0}$$ 나 $$t_{1}$$ 등의 데이터는 정답 레이블을 나타낸다. 그림에서 보듯,  T개의 Softmax with Loss 계층 각각이 손실을 산출한다. 그리고 그 손실들을 합산해 평균한 값이 최종 손실이 된다. 이때 수행하는 계산의 수식은 다음과 같다.

![e 5-11](/assets/img/dlfs2/deep_learning_2_images/e 5-11.png)

미니배치 처럼 Time Softmax with Loss 계층도 시계열에 대한 평균을 구하는 것으로, 데이터 1개당 평균 손실을 구해 최종 출력으로 내보낸다.

## 5.5 RNNLM학습과 평가

RNNLM 구현에 필요한 계층은 모두 설명했다. 그러면 RNNLM을 구현하고 실제로 학습을 시켜보자.

### 5.5.1 RNNLM 구현

RNNLM에서 사용하는 신경망을 SimpleRnnlm 이라는 이름의 클래스로 구현한다. SimpleRnnlm의 계층 구성은 아래 그림과 같다.

![fig 5-30](/assets/img/dlfs2/deep_learning_2_images/fig 5-30.png)

SimpleRnnlm 클래스는 4개의 Time 계층을 쌓은 신경망이다. 초기화 코드부터 살펴보자.

```python
class SimpleRnnlm:
    def __init__(self, vocab_size, wordvec_size, hidden_size):
        V, D, H = vocab_size, wordvec_size, hidden_size
        rn = np.random.randn

        # 가중치 초기화
        embed_W = (rn(V, D) / 100).astype('f')
        rnn_Wx = (rn(D, H) / np.sqrt(D)).astype('f')
        rnn_Wh = (rn(H, H) / np.sqrt(H)).astype('f')
        rnn_b = np.zeros(H).astype('f')
        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')
        affine_b = np.zeros(V).astype('f')

        # 계층 생성
        self.layers = [
            TimeEmbedding(embed_W),
            TimeRNN(rnn_Wx, rnn_Wh, rnn_b, stateful=True),
            TimeAffine(affine_W, affine_b)
        ]
        self.loss_layer = TimeSoftmaxWithLoss()
        self.rnn_layer = self.layers[1]

        # 모든 가중치와 기울기를 리스트에 모은다.
        self.params, self.grads = [], []
        for layer in self.layers:
            self.params += layer.params
            self.grads += layer.grads

```

이 초기화 메서드는 각 계층에서 사용하는 매개변수를 초기화하고 필요한 계층을 생성한다. 또, Truncated BPTT로 학습한다고 가정하여 Time RNN 계층의  stateful을 True로 설정한다. 그 결과 Time RNN 계층은 이전 시각의 은닉 상태를 계승할 수 있게 된다.

또한 초기화 코드는 RNN 계층과 Affine 계층에서 'Xavier 초깃값'을 이용했다. Xavier 초깃값에서는 이전 계층의 노드가 n개라면 표준편차가  $$\frac{1}{\sqrt(n)}$$ 인 분포로 값들을 초기화한다. 참고로 표준편차는 데이터의 차이를 직관적으로 나타내는 척도로 해석할 수 있다.

![fig 5-31](/assets/img/dlfs2/deep_learning_2_images/fig 5-31.png)



계속해서 forward(), backward(), reset\_state() 메서드의 구현을 살펴보자.

```python
    def forward(self, xs, ts):
        for layer in self.layers:
            xs = layer.forward(xs)
        loss = self.loss_layer.forward(xs, ts)
        return loss

    def backward(self, dout=1):
        dout = self.loss_layer.backward(dout)
        for layer in reversed(self.layers):
            dout = layer.backward(dout)
        return dout

    def reset_state(self):
        self.rnn_layer.reset_state()

```

각각의 계층에서 순전파와 역전파를 적당히 구현해 뒀으므로, 여기에서는 해당 계층의 forward() 메서드를 적절한 순서로 호출해 줬을 뿐이다. 마지막 reset\_state() 는 신경망의 상태를 초기화하는 편의 메서드이다. 

### 5.5.2 언어 모델의 평가

언어 모델은 주어진 과거 단어(정보)로부터 다음에 출현한 단어의 확률분포를 출력한다. 이때 언어 모델의 예측 성능을 평가하는 척도로 **퍼플렉서티**를 자주 이용한다.

퍼플렉시티는 확률의 역수이다. 

![fig 5-32](/assets/img/dlfs2/deep_learning_2_images/fig 5-32.png)

가령 '모델 1'의 경우 확률은 0.8이어서 퍼플렉서티는 그 역수인 1.25가 된다. '모델 2'는 확률이 0.2이어서 퍼플렉서티가 5가 된다. 이 값은 직관적으로 '분기의 수(number of branches)'로 해석할 수 있다. 분기 수란 다음에 취할 수 있는 선택사항의 수(구체적으로 말하면, 다음에 출현할 수 있는 단어의 후보 수)를 말한다. 앞의 예에서 좋은 모델이 예측한 '분기 수'가 1.25라는 것은 다음에 출현할 수 있는 단어의 후보를 1개 정도로 좁혔다는 뜻이 된다. 반면, 나쁜 모델에서는 후보가 아직 5개가 된다는 의미이다.

> 앞의 예처럼 퍼플렉서티로 모델의 예측 성능을 평가할 수 있다. 좋은 모델은 정답 단어를 높은 확률로 예측할 수 있다. 따라서 퍼플렉서티 값이 작다.(최솟값은 1.0). 한편, 나쁜 모델은 정답 언어를 낮은 확률로 예측하므로 퍼플렉서티 값이 크다.

입력 데이터가 여러 개가 된다면 어떻게 될까?

![e 5-12](/assets/img/dlfs2/deep_learning_2_images/e 5-12.png)

![e 5-13 2](/assets/img/dlfs2/deep_learning_2_images/e 5-13 2.png)

N은 데이터의 총 개수이고,  $$t_{n}$$은 원핫 벡터로 나타낸 정답 레이블이다. $$t_{nk}$$는 n개째 데이터의  k번째 값을 의미한다. 그리고  $$y_{nk}$$ 는 확률분포를 나타낸다. (신경망에서는  Softmax의 출력)

참고로 L은 신경망의 손실을 뜻하고, 사실 교차 엔트로피 오차를 뜻하는 식 1.8과 완전히 같은 식이다.  이 L을 이용해 $$e^{L}$$ 을 계산한 값이 곧 퍼플렉서티이다.

데이터가 하나일 때 설명한 '확률의 역수', '분기 수', '선택사항의 수' 같은 개념이 그대로 적용된다. 즉, 퍼플렉서티가 작아질수록 분기 수가 줄어 좋은 모델이 된다.

### 5.5.3 RNNLM의 학습 코드

PTB 데이터셋을 이용해 RNNLM 학습을 수행해보자. 단, 이번에 구현할  RNNLM은  PTB 데이터셋(훈련 데이터)전부를 대상으로 학습하면 전혀 좋은 결과를 낼 수 없기 때문에, 처음 1000개의 단어만 이용하겠다. 다음은 학습을 수행하는 코드다.

```python
# coding: utf-8
import sys
sys.path.append('..')
import matplotlib.pyplot as plt
import numpy as np
from common.optimizer import SGD
from dataset import ptb
from simple_rnnlm import SimpleRnnlm


# 하이퍼파라미터 설정
batch_size = 10
wordvec_size = 100
hidden_size = 100 # RNN의 은닉 상태 벡터의 원소 수
time_size = 5     # Truncated BPTT가 한 번에 펼치는 시간 크기
lr = 0.1
max_epoch = 100

# 학습 데이터 읽기(전체 중 1000개만)
corpus, word_to_id, id_to_word = ptb.load_data('train')
corpus_size = 1000
corpus = corpus[:corpus_size]
vocab_size = int(max(corpus) + 1)

xs = corpus[:-1]  # 입력
ts = corpus[1:]   # 출력(정답 레이블)
data_size = len(xs)
print('말뭉치 크기: %d, 어휘 수: %d' % (corpus_size, vocab_size))

# 학습 시 사용하는 변수
max_iters = data_size // (batch_size * time_size)
time_idx = 0
total_loss = 0
loss_count = 0
ppl_list = []

# 모델 생성
model = SimpleRnnlm(vocab_size, wordvec_size, hidden_size)
optimizer = SGD(lr)

# 1. 미니배치의 각 샘플의 읽기 시작 위치를 계산
jump = (corpus_size - 1) // batch_size
offsets = [i * jump for i in range(batch_size)]

for epoch in range(max_epoch):
    for iter in range(max_iters):
        # 2. 미니배치 취득
        batch_x = np.empty((batch_size, time_size), dtype='i')
        batch_t = np.empty((batch_size, time_size), dtype='i')
        for t in range(time_size):
            for i, offset in enumerate(offsets):
                batch_x[i, t] = xs[(offset + time_idx) % data_size]
                batch_t[i, t] = ts[(offset + time_idx) % data_size]
            time_idx += 1

        # 기울기를 구하여 매개변수 갱신
        loss = model.forward(batch_x, batch_t)
        model.backward()
        optimizer.update(model.params, model.grads)
        total_loss += loss
        loss_count += 1

    # 3. 에폭마다 퍼플렉서티 평가
    ppl = np.exp(total_loss / loss_count)
    print('| 에폭 %d | 퍼플렉서티 %.2f'
          % (epoch+1, ppl))
    ppl_list.append(float(ppl))
    total_loss, loss_count = 0, 0
```

이것이 학습을 수행하는 코드다. 기본적으로는 지금까지 본 신경망 학습과 거의 같지만, '데이터 제공 방법'과 '퍼플렉서티 계산'부분이 다르다.

우리는 Truncated BPTT 방식으로 학습을 수행한다. 따라서 데이터는 순차적으로 주고 각각의 미니배치에서 데이터를 읽는 시작 위치를 조정해야 한다. 소스 코드의 1에서는 미니배치가 데이터를 읽기 시작하는 위치를 계산해 offsets에 저장한다. 다시 말해 이  offsets의 각 원소에 데이터를 읽는 시작 위치 (오프셋)이 담기게 된다.

이서 코드 2에서는 데이터를 순차적으로 읽는다. 먼저 '그릇'인 batch\_x와 batch\_t를 준비한다. 그런 다음 변수 time\_idx를 1씩 (순차적으로) 늘리면서, 말뭉치에서 time\_idx 위치의 데이터를 얻는다. 여기서 1에서 계산한 offsets를 이용해서 각 미니배치에서 오프셋을 추가한다. 여기서 1에서 계산한 offsets을 이용하여 각 미니배치에서 오프셋을 추가한다. 또한, 말뭉치를 읽는 위치가 말뭉치 크기를 넘어설 경우 말뭉치의 처음으로 돌아와야 하는데, 이를 위해 말뭉치의 크기로 나눈 나머지를 인덱스로 사용한다.

마지막으로 퍼플렉시티를 코드 3부분에 의해 계산한다. 

```python
# 그래프 그리기
x = np.arange(len(ppl_list))
plt.plot(x, ppl_list, label='train')
plt.xlabel('epochs')
plt.ylabel('perplexity')
plt.show()
```

![fig 5-33](/assets/img/dlfs2/deep_learning_2_images/fig 5-33.png)

작은 말뭉치로 실험한 것이고, 현재의 모델로서는 큰 말뭉치에는 전혀 대응을 할 수가 없다. 현재  RNNLM의 이런 문제는 다음 장에서 계선하도록 한다.

### 5.5.4 RNNLM의 Trainer 클래스

```python
# coding: utf-8

# 모델 생성
model = SimpleRnnlm(vocab_size, wordvec_size, hidden_size)
optimizer = SGD(lr)
trainer = RnnlmTrainer(model, optimizer)

trainer.fit(xs, ts, max_epoch, batch_size, time_size)
trainer.plot()

```

먼저 RnnlmTrainer 클래스에 model과 optimizer를 주어 초기화한다. 그런 다음  fit() 메서드를 호출해 학습을 수행한다. 이때 그 내부에서는 앞 절에서 수행한 일련의 작업이 진행된다.

1. 미니배치를 '순차적'으로 만들어
2. 모델의 역전파와 순전파를 호출하고
3. 옵티마이저로 가중치를 갱신하고
4. 퍼플렉시티를 구한다.

## 5.6 정리

RNN은 데이터를 순환시킴으로써 과거에서 현재, 그리고 미래로 데이터를 계속해서 흘려보낸다. 이를 위해 RNN 계층 내부에는 '은닉 상태'를 기억하는 능력이 추가되었다. 이번 장에서는 RNN 계층의 구조를 시간을 들여 설명하고, 실제로  RNN 계층(그리고 Time RNN 계층)을 구현해 보았다.

RNN 을 이용해 언어 모델은 만들었다. 언어 모델은 단어 시퀀스에 확률을 부여하며, 특히 조건부 언어 모델은 지금까지의 단어 시퀀스로부터 다음에 출현할 단어의 확률을 계산해 준다. 여기서 RNN을 이용한 신경망 구성이 등장하며, 이론적으로는 아무리 긴 시계열 데이터라도 중요 정보를  RNN의 은닉 상태에 기억해 둘 수 있다. 그러나 실제 문제에서는 잘 학습하지 못하는 경우가 많다.

- RNN은 순환하는 경로가 있고, 이를 통해 내부에 '은닉 상태'를 기억할 수 있다.
- RNN의 순환 경로를 펼침으로써, 다수의 RNN 계층이 연결된 신경망으로 해석할 수 있으며, 보통의 오차역전파법으로 학습할 수 있다.(BPTT)
- 긴 시계열 데이터를 학습할 때는 데이터를 적당한 길이씩 모으고(이를 '블록' 이라 한다.), 블록 단위로 BPTT에 의한 학습을 수행한다.
- Truncated BPTT에서는 역전파의 연결만 끊는다.
- Truncated BPTT에서는 순전파의 연결을 유지하기 위해 데이터를 '순차적으로' 입력해야 한다.
- 언어 모델은 단어 시퀀스를 확률로 해석한다.
- RNN 계층을 이용한 조건부 언어 모델은 (이론적으로는) 그때까지 등장한 모든 단어의 정보를 기억할 수 있다.