---
layout: post
title: Chapter 01. 신경망 복습
comments: true
categories: [Deep Learning from Scratch 2]
tags: [Deep Learning, Machine Learning]
author: lsjhome
---


# Chapter 01 신경망 복습

## 1.1 수학과 파이썬 복습

### 1.1.1 벡터와 행렬

![fig 1-1](/assets/img/dlfs2/deep_learning_2_images/fig 1-1.png)

'벡터'는 크기와 방향을 가진 양이다. 벡터는 숫자가 일렬로 늘어선 집합으로 표현할 수 있으며, 파이썬에서는 1차원 배열로 취급할 수 있다. 그에 반해 '행렬'은 숫자가 2차원 형태(사각형 형상)로 늘어선 것이다.

행렬의 가로줄을 행(row), 세로줄을 열(column)이라 한다. 벡터와 행렬은 **x** 와 **W** 처럼 굵게 표기하여 스칼라 값과 구별한다.

### 1.1.2 행렬의 원소별 연산

```python
W = np.array([[1, 2, 3], [4, 5, 6]])
X = np.array([[0, 1, 2], [3, 4, 5]])

>>> W + X
array([[ 1,  3,  5],
       [ 7,  9, 11]])

>>> W * X
array([[ 0,  2,  6],
       [12, 20, 30]])
```

### 1.1.3 브로드캐스트

```python
A = np.array([[1, 2], [3, 4]])
>>> A * 10
array([[10, 20],
       [30, 40]])
```

```python
A = np.array([[1, 2], [3, 4]])
b = np.array([10, 20])
>>> A * b
array([[10, 40],
       [30, 80]])
```

![fig 1-4](/assets/img/dlfs2/deep_learning_2_images/fig 1-4.png)

### 1.1.4 벡터의 내적과 행렬의 곱

벡터의 내적(inner product)

![e 1-1](/assets/img/dlfs2/deep_learning_2_images/e 1-1.png)

> 벡터의 내적은 직관적으로는 '두 벡터가 얼마나 같은 방향을 향하고 있는가' 를 나타낸다. 베겉의 길이가 1인 경우로 한정하면, 완전히 같은 방향을 향하는 두 벡터의 내적은 1이 된다. 반대로, 반대 방향을 향하는 두 벡터의 내적은 -1 이다.

![fig 1-5](/assets/img/dlfs2/deep_learning_2_images/fig 1-5.png)

벡터의 내적과 행렬의 곱 파이썬 구현

```python
# 벡터의 내적
a = np.array([1, 2, 3])
b = np.array([4, 5, 6])
>>> np.dot(a, b)
32

# 행렬의 곱

A = np.array([[1, 2], [3, 4]])
B = np.array([[5, 6], [7, 8]])
>>> np.matmul(A, B)
array([[19, 22],
       [43, 50]])

```

사실 벡터의 내적과 행렬의 곱 모두 np.dot()을 사용할 수 있지만, 둘을 구분하는 것이 보기에 좋다.

### 1.1.5 행렬 형상 확인

![fig 1-6](/assets/img/dlfs2/deep_learning_2_images/fig 1-6.png)

## 1.2 신경망의 추론

### 1.2.1 신경망 추론 전체 그림

신경망은 간단히 말하면 단순한 '함수' 이다. 아래는 2차원 데이터를 입력하여 3차원 데이터를 출력하는 함수이다. **입력층**에는 뉴런 2개를, **출력층**에는 3개를 각각 준비한다. 은닉층에는 뉴런 4개를 둔다.

![fig 1-7](/assets/img/dlfs2/deep_learning_2_images/fig 1-7.png)

뉴런을 O로, 그 사이의 연결을 화살표로 나타낸다. 화살표에는 **가중치**가 존재하며, 그 가중치와 뉴런의 값을 각각 곱해서 그 합이 다음 뉴런의 입력으로 쓰인다. (정확히는 그 합에 활성화 함수 activation function를 적용한 값이 다음 뉴런의 입력이 된다.) 또, 이때 각 층에서는 이전 뉴런의 값에 영향 받지 않는 '정수'도 더해진다. 이 정수는 **편향**(bias)이라고 한다. 덧붙여, 위의 신경망은 인접하는 층의 모든 뉴런과 연결되어 있다는 뜻에서 **완전연결계층**(fully connected layer)라고 한다.

> 위에서 가중치를 가지는 층은 사실 2개 뿐이다. 그래서 이를 2층 신경망이라고 부르겠지만, 문헌에 따라서 3층 신경망이라 부르기도 한다.

첫 번째 뉴런은 다음과 같에 계산 된다.

![e 1-2](/assets/img/dlfs2/deep_learning_2_images/e 1-2.png)

![e 1-3](/assets/img/dlfs2/deep_learning_2_images/e 1-3.png)

여기에서 은닉층의 뉴런들은 (h1, h2, h3, h4)로 정리되며, 1 x 4 행렬로 간주할 수 있다. (혹은 행벡터) 위 식은 다음처럼 간소화 될 수 있다.

![e 1-4](/assets/img/dlfs2/deep_learning_2_images/e 1-4.png)

**x** 는 입력, **h** 는 은닉층의 뉴런, **W** 는 가중치, **b** 는 편향

![fig 1-8](/assets/img/dlfs2/deep_learning_2_images/fig 1-8.png)

신경망의 추론이나 학습에서는 다수의 샘플 데이터(미니배치)를 한꺼번에 처리한다. 이렇게 하려면 행렬 **x** 의 행 각각에 샘플 데이터를 하나씩 저장해야 한다. 예컨대 N개의 샘플 데이터를 미니배치로 한꺼번에 처리한다면 [그림 1-9]처럼 된다.

![fig 1-9](/assets/img/dlfs2/deep_learning_2_images/fig 1-9.png)

완전연결층에 의한 변환의 미니배치를 파이썬으로 구현해 보자

```python
W1 = np.random.randn(2, 4)
b1 = np.random.randn(4)
x = np.random.randn(10, 2)
h = np.matmul(x, W1) + b1
```

x의 첫 번째 차원이 각 샘플 데이터에 해당한다. 예컨대 x[0]은 0번째 입력 데이터, x[1]은 첫 번째 입력 데이터가 되는 식이다. 마찬가지로 h[0]은 0번째 데이터의 은닉층 뉴런, h[1]은 1번째 데이터의 은닉층 뉴런이 저장된다.

완전연결계층에 의한 변환은 '선형' 변환이다. 여기에 '비선형' 효과를 부여하는 것이 활성화 함수이다. 더 정확하게 말하면, 비선형 활성화 함수를 이용함으로써, 신경망의 표현력을 높일 수 있다. 활성화 함수는 아주 다양하지만, 여기에서는 **시그모이드 함수** 를 사용하자.

![e 1-5](/assets/img/dlfs2/deep_learning_2_images/e 1-5.png)

시그모이드 함수는 아래 그림과 같은 알파벳 'S'자 모양의 곡선 함수이다.

![fig 1-10](/assets/img/dlfs2/deep_learning_2_images/fig 1-10.png)

```python
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

a = sigmoid(h)
```

시그모이드 함수에 의해 비선형 변환이 가능하다. 계속해서 이 활성화 함수의 출력인 a(이를 **활성화** 라고 한다) 를 또 다른 완전연결계층에 통과시켜 변환한다. 

```python
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

a = sigmoid(h)

x = np.random.randn(10, 2)
W1 = np.random.randn(2, 4)
b1 = np.random.randn(4)
W2 = np.random.randn(4, 3)
b2 = np.random.randn(3)

h = np.matmul(x, W1) + b1
a = sigmoid(h)
s = np.matmul(a, W2) + b2
```

x의 형상은 (10, 2) 이다. 2차원 데이터 10개가 미니배치로 처리된다는 뜻이다. 그리고 최종 출력인 s의 형상은 (10, 3)이 된다. 즉, 이것은 10개의 데이터가 한꺼번에 처리되었고, 각 데이터는 3차원 데이터로 변환되었다는 뜻이다.

이 신경망은 3차원 데이터를 출력한다. 따라서 각 차원의 값을 이용하여 3클래스 분류를 할 수 있다. 이 경우 출력된 3차원의 벡터의 각 차원은 각 클래스에 대응하는 '점수score'가 된다.

### 1.2.2 계층으로 클래스화 및 순전파 구현

신경망에서 하는 처리를 계층(layer)로 구현해 보자. 완전연결계층에 읳나 변환을 Affine 계층으로, 시그모이드 함수에 의한 변환을 Sigmoid 계층으로 구현할 것이다. 완전연결계층에 의한 변환은 기하학에서 아핀(affine)변환에 해당한다. 각 계층은 파이썬 클래스로 구현하며, 기본 변환을 수행하는 메서드의 이름은 forward()로 한다.

모든 계층은 다음과 같은 구현 규칙을 가진다.

- 모든 계층은 forward()와 backward() 메서드를 가진다.
- 모든 계층은 인스턴스 변수인 params와 grads를 가진다.

params은 가중치와 편향 같은 매개변수를 담는 리스트이다. grads는 params에 저장된 각 매개변수에 대응하여, 해당 매개변수의 기울기를 보관하는 리스트이다.

```python
class Sigmoid:
    def __init__(self):
        self.params = []
        
    def forward(self, x):
        return 1 / (1 + np.exp(-1))
        
class Affine:
    def __init__(self, W, b):
        self.params = [W, b]
        
    def forward(self, x):
        W, b = self.params
        out = np.matmul(x, W) + b
        return out
```

Affine 계층은 초기화될 때 가중치와 편향을 받는다. 즉 가중치와 편향은 Affine 계층의 매개변수이며, 리스트인 params 인스턴스 변수에 보관한다. 다음으로 forward(x)는 순전파 처리를 구현한다.

> 이 책에서는 모든 매개변수가 반드시 인스턴스 변수인 params에 존재하게 된다. 이 덕분에 신경망의 모든 매개변수를 간단하게 정리할 수 있고, 자연스럽게 매개변수 갱신 작업이나 매개변수를 파일로 저장하는 일이 쉬워진다.

앞의 계층을 사용해 신경망의 추론 처리를 구현해 보자.

![fig 1-11](/assets/img/dlfs2/deep_learning_2_images/fig 1-11.png)

입력 **x** 가 Affine 계층, Sigmoid 계층, Affine 계층을 차례로 거쳐 점수인 **s** 를 출력하게 된다.

```python
class TwoLayerNet:
    def __init__(self, input_size, hidden_size, output_size):
        I, H, O = input_size, hidden_size, output_size
        
        # 가중치와 편향 초기화
        W1 = np.random.randn(I, H)
        b1 = np.random.randn(H)
        W2 = np.random.randn(H, O)
        b2 = np.random.randn(O)
        
        # 계층 생성
        self.layers = [
            Affine(W1, b1),
            Sigmoid(),
            Affine(W2, b2)
        ]
        
        # 모든 가중치를 리스트에 모은다.
        self.params = []
        for layer in self.layers:
            self.params += layer.params
            
    def predict(self, x):
        for layer in self.layers:
            x = layer.forward(x)
        return x
```

```python
x = np.random.randn(10, 2)
model = TwoLayerNet(2, 4, 3)
s = model.predict(x)

>>> model.params
# [W1, B1, W2, B2]
[array([[-0.14086675,  0.50386149,  1.75241359, -0.03977884],
        [-1.04109588,  0.3512808 ,  1.22560817, -1.66394488]]),
 array([-3.01589946,  0.07725342, -2.45144965, -1.43196132]),
 array([[-1.02587472, -0.66724738, -0.33460752],
        [-0.47035178, -0.95073322, -0.87942741],
        [-0.66897059,  0.95275625, -1.5144184 ],
        [-1.74817495,  1.05990488, -0.76076031]]),
 array([ 1.69070709,  0.81757579, -0.29542278])]

```

## 1.3 신경망의 학습

학습되지 않은 신경망은 '좋은 추론'을 해낼 수 없다. 그래서 학습을 먼저 수행하고, 그 학습된 매개변수를 이용해 추론을 수행하는 흐름이 일반적이다. 신경망의 학습에 대해 알아보자.

### 1.3.1 손실 함수

**손실** 은 학습 데이터와 신경망이 예측한 결과를 비교하여 예측이 얼마나 나쁜가를 산출한 단일 값(스칼라) 이다. 

**손실 함수** 를 이용해 구한다. 다중 클래스 분류 신경망에서는 손실 함수로 흔히 **교차 엔트로피 오차** 를 이용한다. 교차 엔트로피 오차는 신경망이 출력하는 각 클래스의 '확률'과 '정답 레이블'을 이용해 구할 수 있다. 

앞 절의 신경망에 Softmax 계층과 Cross Entropy Error 계층을 새로 추가하자.

![fig 1-22](/assets/img/dlfs2/deep_learning_2_images/fig 1-12.png)

**x** 는 입력 데이터, **t** 는 정답 레이블, **L** 은 손실을 나타낸다. 

아래는 소프트맥스 함수이다.

![e 1-6](/assets/img/dlfs2/deep_learning_2_images/e 1-6.png)

소프트맥스 함ㅁ수의 출력의 각 원소는 0.0이상 1.0 이하의 실수이다. 그리고 그 원소들을 모두 더하면 1.0이 된다. 이것이 소프트맥스의 출력ㅇ르 '확률'로 해석할 수 있는 이유다. 소프트맥스의 출력인 이 '확률'이 다음 차례인 교차 엔트로피 오차에 입력된다. 이때 교차 엔트로피 오차의 수식은 다음과 같다.

![e 1-7](/assets/img/dlfs2/deep_learning_2_images/e 1-7.png)

$$t_k$$ 는 k번째 클래스에 해당하는 정답 레이블이다. log는 네이피어 상수(혹은 오일러의 수) $e$ 를 밑으로 하는 로그이다. ($log_e$) 정답 에티블은 t = [0, 0, 1]와 같이 원핫 벡터로 표기한다.

미니배치를 고려하면 교차 엔트로피 오차의 식은 다음과 같이 된다. 데이터는 N개이며, $t_{nk}$ 는 n번째 데이터의 k차원째의 값을 의미한다. 그리고 $y_{nk}$는 신경망의 출력이고, $t_{nk}$ 는 정답 레이블이다.

![e 1-8](/assets/img/dlfs2/deep_learning_2_images/e%201-8.png)

데이터가 N개이므로, N으로 나눠서 1개당의 '평균 손실 함수'를 구한다. 

교차 엔트로피 오차를 계산하는 계층을 Softmax with Loss 계층 하나로 구현한다.

![fig 1-13](/assets/img/dlfs2/deep_learning_2_images/fig 1-13.png)

### 1.3.2 미분과 기울기

$L$은 스칼라, $x$ 는 벡터인 함수 $L = f(x)$ 가 있다. 이때 $x$의 $i$ 번째 원소인 $x_i$ 에 대한 $L$ 의 미분은 $\frac{\partial L}{\partial x_{i}}$ 그리고 벡터 x의 다른 원소의 미분도 구할 수 있고, 이는 다음과 같이 정리할 수 있다.

![e 1-9](/assets/img/dlfs2/deep_learning_2_images/e 1-9.png)

벡터의 각 원소에 대한 미분을 정리한 것이 **기울기** 이다.

벡터와 마찬가지로 행렬에서도 기울기를 생각할 수 있다. 예컨데 **W**가 행렬이라면, $L$ = $g$(**W**)함수의 기울기는 다음과 같이 쓸 수 있다.

![e 1-10](/assets/img/dlfs2/deep_learning_2_images/e 1-10.png)

위와 같이 $L$의 **W**에 대한 기울기를 행렬로 정리할 수 있다. 여기서 중요한 점은 **W** 와 $\frac{\partial l}{\partial W}$ 의 형상이 같다는 것이다. 그리고 '행렬과 그 기울기의 형상이 같다' 라는 성질을 이용하면 매개변수 갱신과 연쇄 법칙을 쉽게 구현할 수 있다.

### 1.3.3 연쇄 법칙

학습 시 신경망은 학습 데이터를 주면 손실을 출력한다. 여기서 우리가 얻고 싶은 것은 각 매개변수에 대한 손실의 기울기이다. 그 기울기를 얻을 수 있다면, 그것을 사용해 매개변수를 갱신할 수 있기 때문이다. 신경망의 기울기를 구하기 위해서, **오차역전파** 가 등장한다.

**연쇄 법칙** 이란 합성함수에 대한 미분의 법칙이다. (합성함수란 여러 함수로 구성된 함수이다). 연쇄 법칙이 중요한 이유는, 아무리 많은 함수를 연결하더라도, 그 미분은 개별 함수의 미분들을 이용해 구할 수 있기 때문이다. 달리 말하면, 각 함수의 국소적인 미분을 계산할 수 있다면 그 값들을 곱해서 전체의 미분을 구할 수 있다. 

![e 1-11](/assets/img/dlfs2/deep_learning_2_images/e 1-11.png)

### 1.3.4 계산 그래프

![fig 1-15](/assets/img/dlfs2/deep_learning_2_images/fig 1-15.png)

계산 그래프에서는 연산을 노드로 나타내고, 그 처리 결과가 순서대로 흐른다. 이것이 계산 그래프의 '순전파' 이다. 중요한 점은 기울기가 순전파와 반대 방향으로 전파된다는 사실인데, 이 방향의 전파가 '역전파' 이다. 최종적으로 스칼라 값인 $L$가 출력된다고 가정해 보자.

우리 목표는 $L$ 의 미분을 각 변수에 대해 구하는 것이다.

![fig 1-17](/assets/img/dlfs2/deep_learning_2_images/fig 1-17.png)

역전파는 두꺼운(붉은) 화살표로 그리고, 화살표 아래에 '전파되는 값'을 쓴다. 이때 '전파되는 값'은 최종 출력 *L*의 각 변수에 대한 미분이다. '전파되는 값'은 최종 출력 $L$ 의 각 변수에 대한 미분이다. 

앞서 복습한 연쇄 법칙에 따르면, 역전파로 흐르는 미분 값은 상류로부터 흘러온 미분과 각 연산 노드의 국소적인 미분을 곱해 계산할 수 있다. 그러므로 이 예에서는 $\frac{\partial L}{\partial x} = \frac{\partial L}{\partial z} \frac{\partial z}{\partial x}$ 이고, $\frac{\partial L}{\partial y} = \frac{\partial L}{\partial z} \frac{\partial z}{\partial y}$ 가 된다.

#### 곱셈 노드

![fig 1-19](/assets/img/dlfs2/deep_learning_2_images/fig 1-19.png)

벡터나 행렬 또는 텐서 같은 다변수를 흘려도 상관없다. 

#### 분기 노드

![fig 1-20](/assets/img/dlfs2/deep_learning_2_images/fig 1-20.png)

분기 노드는 따로 그리지 않고, 단순히 선이 두 개로 나뉘도록 그리는데, 이때 같은 값이 복제되어 분기한다. 따라서 분기 노드를 '복제 노드'라고 할 수도 있다. 그리고 그 역전파는 상류에서 온 기울기들의 '합'이 된다.

#### Repeat 노드

2개로 분기하는 분기 노드를 일반화하면 N개로의 분기(복제)노드가 된다. 이를 Repeat 노드라고 한다.

![fig 1-21](/assets/img/dlfs2/deep_learning_2_images/fig 1-21.png)

[그림 1-21]은 길이가 D인 배열을 N개로 복제하는 예이다. 이 Repeat 노드는 N개의 분기 노드로 볼 수 있으므로, 그 역전파는 N개의 기울기를 모두 더해 구할 수 있다. 코드로는 다음과 같이 구현 가능하다.

```python
D, N = 8, 7
x = np.random.randn(1, D) # 입력
y = np.repeat(x, N, axis = 0) # 순전파
dy = np.random.randn(N, D) # 무작위 기울기
dx = np.sum(dy, axis = 0, keepdims=True) # 역전파
```

np.repeat() 메서드가 원소 복제를 수행한다. 이 코드에서는 배열 x를 N번 복제하는데, 이때 axis를 지정하여 어느 축 방향으로 복제할지를 조정할 수 있다. 또, 역전파에서는 총합을 구해야 하므로 np.sum() 메서드를 이용한다. 이때도 axis인수를 설정하여 어느축 방향으로 합을 구할 지 지정한다.

또한 인수로 keepdims=True를 설정하여 2차원 배열의 차원 수를 유지한다. keepdims가 True면 np.sum()의 결과의 형상은 (1, D)이 되며, False면 (D, )이 된다.

#### Sum 노드

Sum노드는 범용 덧셈 노드이다. 예컨데 N x D 배열에 대해 그 총합을 0축에 대해 구하는 계산을 생각해보자. 이때 Sum 노드의 순전파와 역전파는 아래와 같다.

![fig 1-22](/assets/img/dlfs2/deep_learning_2_images/fig 1-22.png)

Sum 노드의 역전파는 상류에서의 기울기를 모든 화살표에 분배한다. 덧셈 노드의 역전파를 자연스럽게 확장한 모양이다.

```python
D, N = 8, 7
x = np.random.randn(N, D) # 입력
y = np.sum(x, axis=0, keepdims=True) # 순전파

dy = np.random.randn(1, D) # 무작위 기울기
dx = np.repeat(dy, N, axis=0) # 역전파
```

Sum 노드와 Repeat 노드는 서로 반대 관계이다. Sum 노드의 순전파가 Repeat 노드의 역전파이고, Sum 노드의 역전파가 Repeat 노드의 순전파가 된다.

#### MatMul 노드

**y** = **xW** 라는 계산을 예로 들자. **x, W, y**의 형상은 각각 $1 \times D$, $D \times H$, $1 \times H$ 이다.

![fig 1-23](/assets/img/dlfs2/deep_learning_2_images/fig 1-23.png)

**x**의 *i* 번째 원소에 대한 미분 $\frac{\partial L}{\partial x_{i}}$ 은 다음과 같이 구한다.

![e 1-12](/assets/img/dlfs2/deep_learning_2_images/e 1-12.png)

$\frac{\partial L}{\partial x_{i}}$ 은 $x_{i}$ 를 (조금) 변화시켰을때, L이 얼마나 변할 것인가라는 '변화의 정도'를 나타낸다. 여기서 $x_{i}$ 를 변화시키면 벡터 **y** 의 모든 원소가 변하고, 그로 인해 최종적으로 $L$이 변하게 된다. 

$\frac{\partial y_{j}}{\partial x_{i}} = W_{y}$ 가 성립하므로, 이를 위 식에 대입하면 아래와 같다.

![e 1-13](/assets/img/dlfs2/deep_learning_2_images/e 1-13.png)

$\frac{\partial L}{\partial x_{i}}$ 은 벡터 $\frac{\partial L}{\partial y}$ 과 **W** 의 i행 벡터의 내적으로 구해짐을 알 수 있다. 

![e 1-14](/assets/img/dlfs2/deep_learning_2_images/e 1-14.png)

$\frac{\partial L}{\partial x}$ 은 행렬의 곱을 사용해 단번에 구할 수 있다. 

![fig 1-24](/assets/img/dlfs2/deep_learning_2_images/fig 1-24.png)

미니배치를 고려해 **x** 에 N개의 데이터가 담겨 있다고 가정하자. 그러면 **x**, **W**, **y** 의 형상은 각각 $N \times D$, $D \times H$, $N \times H$가 된다. 역전파의 계산 그래프는 다음과 같다.

![fig 1-25](/assets/img/dlfs2/deep_learning_2_images/fig 1-25.png)

곱셈의 역전파에서는 '순전파 시의 입력을 서로 바꾼 값'을 사용했다. 이와 마찬가지로 행렬의 역전파에서도 '순전파 시의 입력을 서로 바꾼 행렬'을 사용하는게 열쇠이다. 

![fig 1-26 2](/assets/img/dlfs2/deep_learning_2_images/fig 1-26 2.png)

하나의 계층으로 구현해 보자

```python
class MatMul:
    def __init__(self, W):
        self.params = [W]
        self.grads = [np.zeros_like(W)]
        self.x = None
        
    def forward(self, x):
        W, = self.params
        out = np.matmul(x, W)
        self.x = x
        return out
    
    def backward(self, dout):
        W, = self.params
        dx = np.matmul(dout, W.T)
        dW = np.matmul(self.x.T, dout)
        self.grads[0][...] = dw
        return dx
```

MatMul 계층은 학습하는 매개변수를 params에 보관한다. 그리고 거기에 대응하는 형태로, 기울기는 grads에 보관한다. 역전파에서는 dx와 dW를 구해 가중치의 기울기를 인스턴스 변수인 grads에 저장한다. 

> grads[0] = dW처럼 '할당'해도 되지만, '생략 기호'는 넘파이 배열의 '덮어쓰기'를 수행한다. 결국 얕은 복사나 깊은 복사냐의 차이다. 전자의 경우 '얕은 복사'가 이루어 지고 grads [0] […] = dW처럼 붙여쓰면 '깊읜 복사'가 이루어진다.

![fig 1-27](/assets/img/dlfs2/deep_learning_2_images/fig 1-27.png)

이렇게 메모리 주소를 고정함으로써 인스턴스 변수 grads를 다루기가 더 쉬워진다.

### 1.3.5 기울기 도출과 역전파 구현

#### Sigmoid 계층

시그모이드 함수를 수식으로 쓰면 $y = \frac{1}{1 + \exp(-x)}$ 이다. 그리고 그 미분은 다음과 같다.

![e 1-15](/assets/img/dlfs2/deep_learning_2_images/e 1-15.png)

![fig 1-28](/assets/img/dlfs2/deep_learning_2_images/fig 1-28.png)

Sigmoid 계층을 파이썬으로 구현해 보자. [그림 1-28]을 토대로 다음과 같이 구현할 수 있다.

```python
class Sigmoid:
    def __init__(self):
        self.params, self.grads = [], []
        self.out = None
        
    def forward(self, x):
        out = 1 / (1 + np.exp(-x))
        self.out = out
        return out
    
    def backward(self, dout):
        dx = dout * (1.0 - self.out) * self.out
        return dx
```

순전파 때는 출력을 인스턴스 변수 out에 저장하고, 역전파를 계산할 때 이 out 변수를 사용하는 모습을 볼 수 있다.

#### Affine 계층

Affine 계층의 순전파는 y = np.matmul(x, W) + b로 구현할 수 있다. 여기서 편향을 더할 때는 넘파이의 브로드캐스트가 사용된다.

 ![fig 1-29](/assets/img/dlfs2/deep_learning_2_images/fig 1-29.png)

```python
class Affine:
    def __init__(self, W, b):
        self.params = [W, b]
        self.grads = [np.zeros_like(W), np.zeros_like(b)]
        self.x = None
        
    def forward(self, x):
        W, b = self.params
        out = np.matmul(x, W) + b
        self.x = x
        return out
    
    def backward(self, dout):
        W, b = self.params
        dx = np.matmul(dout, W.T)
        dW = np.matmul(self.x.T, dout)
        db = np.sum(dout, axis=0)
        
        self.grads[0][...] = dW
        self.grads[1][...] = db
        return dx
```

인스턴스 변수 params에는 매개변수를, grads에는 기울기를 저장한다. Affine의 역전파는 MatMul 노드와 Repeat 노드의 역전파를 수행하면 구할 수 있다. Repeat 노드의 역전파는 np.sum() 메서드로 계산할 수 있는데, 이때 행렬의 형상을 잘 살펴보고 어느 축(axis)으로 합을 구할지 명시해야 한다. 마지막으로, 가중치 매개변수의 기울기를 인스턴스 변수 grads에 저장한다.

> Affine 계층은 미리 구현한 MatMul 계층을 이용하면 더 쉽게 구현할 수 있다.

#### Softmax with Loss 계층

![fig 1-30](/assets/img/dlfs2/deep_learning_2_images/fig 1-30.png)

이 계산 그래프에서는 소프트맥스 함수는 softmax 계층으로, 교차 엔트로피 오차는 Cross Entropy Error 계층으로 표기했다. 그리고 3-클래스 분류를 가젖ㅇ하여 이전 계층 (입력층에 가까운 계층)으로부터 3개의 입력을 받도록 했다. 

softmax 계층은 입력 ($a_{1}$. $a_{2}$. $a_{3}$)를 정규화하여 ($y_{1}$, $y_{2}$, $y_{3}$)를 출력한다. 그리고 Cross Entropy Error 계층은 Softmax의 출력 ($y_{1}$. $y_{2}$. $y_{3}$)과 정답 레이블 ($t_{1}$. $t_{2}$. $t_{3}$)를 받고, 이 데이터로부터 손실 L을 구해 출력한다.

> 위 그림에서 주목할 것은 역전파의 결과이다. Softmax 계층의 역전파는 ($y_{1} - t_{1}$, $y_{2} - t_{2}$, $y_{3} - t_{3}$)로 깔끔하게 떨어진다. 즉 Softmax 계층의 역전파는 자신의 출력과 정답 레이블의 차이라는 것이다. 이처럼 신경망의 역전파는 이 차이(오차)를 앞 계층에 전해주는 것으로, 신경망 학습에서 아주 중요한 성질이다.

### 1.3.6 가중치 갱신

- **1단계: 미니배치**
  훈련 데이터 중에서 무작위로 다수의 데이터를 골라낸다.
- **2단계: 기울기 계산**
  오차역전파법으로 각 가중치 매개변수에 대한 손실 함수의 기울기를 구한다.
- **3단계: 매개변수 갱신**
  기울기를 사용하여 가중치 매개변수를 갱신한다.
- **4단계: 반복**
  1~3단계를 필요한 만큼 반복한다.

이러한 단계를 거쳐 신경망 학습이 이루어진다. 기울기는 현재의 가중치 매개변수에서 손실을 가장 크게 하는 방향을 가리킨다. 따라서 매개변수를 그 기울기와 반대방향으로 갱신하면 손실을 줄일 수 있다. 이를 **경사하강법** 이라고 한다.

3단계에서 수행하는 가중치 갱신 방법은 매우 다양한데, 여기서는 그중 가장 단순한 **확률적 경사하강법(SGD)** 을 구현한다. '확률적(Stochastic)'은 무작위로 선택된 데이터(미니배치)에 대한 기울기를 이용한다는 것이다.

SGD는 단순한 방법이다. SGD는 (현재의) 가중치를 기울기 방향으로 일정한 거리만큼 갱신한다. 수식으로는 다음과 같다.

![e 1-16](/assets/img/dlfs2/deep_learning_2_images/e 1-16.png)

가중치 매개변수가 **W**이고, **W**에 대한 손실 함수의 기울기가 $\frac{\partial L}{\partial W}$ 이다. $\eta^{에타}$ 는 $학습률^{learning rate}$ 을 나타내며, 실제로는 0.01이나 0.001 같은 값을 미리 정해 사용한다.

매개변수를 갱신하는 클래스는 update (params, grads)라는 공통 메서드를 갖도록 구현한다. 이 메서드의 인수 params은 신경망의 가중치가, 그리고 grads에는 기울기가 각각 리스트로 저장되어 있어야 한다. 

```python
class SGD:
    def __init__(self, lr=0.01):
        self.lr = lr
        
    def update(self, params, grads):
        for i in range(len(params)):
            params[i] -= self.lr * grads[i]
```

## 1.4 신경망으로 문제를 풀다

### 1.4.1 스파이럴 데이터셋

```python
import sys
sys.path.append('..')

from dataset import spiral
import matplotlib.pyplot as plt

x, t = spiral.load_data()
print ('x', x.shape) # (300, 2)
print ('t', t.shape) # (300, 3)

# 데이터점 플롯
N = 100
CLS_NUM = 3
markers = ['o', 'x', '^']
for i in range(CLS_NUM):
    plt.scatter(x[i*N:(i+1)*N, 0], x[i*N:(i+1)*N, 1], s=40, marker=markers[i])
plt.show()
```

![fig 1-31](/assets/img/dlfs2/deep_learning_2_images/fig 1-31.png)

입력은 2차원 데이터이고, 분류할 클래스는 3개이다. 직선만으로는 클래스를 분리할 수 없다. 따라서 비선형 분리를 학습해야 한다. (비선형인 시그모이드 함수를 활성화 함수로 사용하는 은닉층이 있는)

### 1.4.2 신경망 구현

```python
class TwoLayerNet:
    def __init__(self, input_size, hidden_size, output_size):
        I, H, O = input_size, hidden_size, output_size
        
        # 가중치와 편향 초기화
        W1 = 0.01 * np.random.randn(I, H)
        b1 = np.zeros(H)
        W2 = 0.01 * np.random.randn(H, O)
        b2 = np.zeros(O)
        
        # 계층 생성
        self.layers = [
            Affine(W1, b1),
            Sigmoid(),
            Affine(W2, b2)
        ]
        self.loss_layer = SoftmaxWithLoss()
        
        # 모든 가중치와 기울기를 리스트에 모은다.
        self.params, self.grads = [], []
        for layer in self.layers:
            self.params += layer.params
            self.grads += layer.grads
            
    def predict(self, x):
        for layer in self.layers:
            x = layer.forward(x)
        return x
    
    def forward(self, x, t):
        score = self.predict(x)
        loss = self.loss_layer.forward(score, t)
        return loss
    
    def backward(self, dout=1):
        dout = self.loss_layer.backward(dout)
        for layer in reversed(self.layers):
            dout = layer.backward(dout)
        return dout
```

### 1.4.3 학습용 코드

```python
import sys
sys.path.append('..')
import numpy as np
from common.optimizer import SGD
from dataset import spiral
import matplotlib.pyplot as plt
from two_layer_net import TwoLayerNet

# 1. 하이퍼파라미터 설정
max_epoch = 300
batch_size = 30
hidden_size = 10
learning_rate = 1.0

# 2. 데이터 읽기, 모델과 옵티마이저 생성
x, t = spiral.load_data()
model = TwoLayerNet(input_size=2, hidden_size=hidden_size, output_size=3)
optimizer = SGD(lr=learning_rate)

# 학습에 사용하는 변수
data_size = len(x)
max_iters = data_size // batch_size
total_loss = 0
loss_count = 0
loss_list = []

for epoch in range(max_epoch):
    
    # 3 데이터 뒤섞기
    idx = np.random.permutation(data_size)
    x = x[idx]
    t = t[idx]
    
    for iters in range(max_iters):
        batch_x = x[iters * batch_size:(iters+1)*batch_size]
        batch_t = t[iters * batch_size:(iters+1)*batch_size]
        
        # 기울기를 구해 매개변수 갱신
        loss = model.forward(batch_x, batch_t)
        model.backward()
        optimizer.update(model.params, model.grads)
        
        total_loss += loss
        loss_count += 1
        
        # 정기적으로 학습 결과 출력
        if (iters+1) % 10 == 0:
            avg_loss = total_loss / loss_count
            print ('| 에폭 %d | 반복 %d / %d | 손실 %.2f' 
                   % (epoch + 1, iters + 1, max_iters, avg_loss))
            loss_list.append(avg_loss)
            total_loss, loss_count = 0, 0
```

1. 우선 하이퍼파라미터를 설정한다. 구체적으로는 학습하는 에폭 수, 미니배치 크기, 은닉층의 뉴런 수, 학습률을 설정한다. 
2. 계속해서 데이터를 읽어 들이고, 신경망(모델)과 옵티마이저를 생성한다. 

> 에폭(epoch)은 학습 단위이다. 1에폭은 학습 데이터를 모두 '살펴본' 시점을 뜻한다.

3. 에폭 단위로 데이터를 뒤섞고, 뒤섞은 데이터 중 앞에서부터 순서대로 뽑아내는 방식을 사용했다.
4. 계속해서 기울기를 구해 매개변수를 갱신한다.
5. 정기적으로 학습 결과를 출력하낟. 10번째 반복마다 손실의 평균을 구해 loss\_list 변수에 추가했다.

![fig 1-32](/assets/img/dlfs2/deep_learning_2_images/fig 1-32.png)

![fig 1-33](/assets/img/dlfs2/deep_learning_2_images/fig 1-33.png)

### 1.4.4 Trainer 클래스

Trainer 클래스는 신경망과 옵티마이저를 인수로 받는다.

![table 1-1](/assets/img/dlfs2/deep_learning_2_images/table 1-1.png)

```python
import sys
sys.path.append('..')  # 부모 디렉터리의 파일을 가져올 수 있도록 설정
from common.optimizer import SGD
from common.trainer import Trainer
from dataset import spiral
from two_layer_net import TwoLayerNet


# 하이퍼파라미터 설정
max_epoch = 300
batch_size = 30
hidden_size = 10
learning_rate = 1.0

x, t = spiral.load_data()
model = TwoLayerNet(input_size=2, hidden_size=hidden_size, output_size=3)
optimizer = SGD(lr=learning_rate)

trainer = Trainer(model, optimizer)
trainer.fit(x, t, max_epoch, batch_size, eval_interval=10)
trainer.plot()
```

## 1.5 계산 고속화

신경망의 학습과 추론에 드는 연산량은 상당하다. 그래서 신경망에서는 얼마나 빠르게 계산하느냐가 매우 중요한 주제이다. 

### 1.5.1 비트 정밀도

기본적으로 64비트 데이터 타입을 사용한다.

```python
import numpy as np
a = np.random.randn(3)
>>> a.dtype
dtype('float64')
```

넘파이의 부동소수점 수는 기본적으로 64비트 데이터 타입을 사용한다. float64는 64비트 부동소수점 수라는 뜻이다. 신경망의 추론과 학습은 32비트 부동소수점 수로도 문제없이 수행할 수 있다. 

32비트는 64비트의 절반이므로, 메모리 관점에서는 32비트가 항상 더 좋다고 할 수 있따. 또한 신경망 계산시 '버스 대역폭'(bus bandwith)이 병목이 되는 경우가 있다. 이런 경우에도 데이터 타입이 작은 게 유리하다. 마지막으로 계산 속도 측면에서도 32비트 부동소수점 수가 일반적으로 더 빠르다.

이런 이유로 이 책에서는 주로 32비트 부동소수점 수를 우선으로 사용한다. 넘파이에서 32비트 부동소수점 수를 사용하려면 다음과 같이 데이터 타입을 np.float32나 'f'로 지정한다.

```python
b = np.random.randn(3).astype(np.float32)
>>> b.dtype
dtype('float32')

c = np.random.randn(3).astype('f')
>>> c.dtype
dtype('float32')
```

신경망 추론으로 한정하면 16비트 부동소수점 수를 사용해도 인식률이 거의 떨어지지 않는다. 그리고 넘파이에도 16비트 부동소수점 수가 준비되어 있다. 하지만 CPU와 GPU는 연산 자체를 32비트로 수행한다. 따라서 16비트 부동소수점으로 변환하여도 계산 자체는 32비트로 이뤄져서 처리 속도 측면에서는 혜택이 없을 수도 있다.

하지만 학습된 가중치를 (파일에) 저장할 때는 16비트 부동소수점 수가 여전히 유효하다. 저장시 절반의 용량만 사용하기 때문이다.

> 최근 GPU들은 '저장'과 '연산' 모두에 16비트 반정밀도 부동소수점 수를 지원하도록 진화했다. 구글에서 개발한 TPU칩은 8비트 계산도 지원한다.

### 1.5.2 GPU(쿠파이)

쿠파이는 GPU를 이용해 병렬 계산을 수행해주는 라이브러리인데, 아쉽게도 엔비디아의 GPU에서만 동작한다. 또한 CUDA라는 GPU 전용 병렬 컴퓨팅 플랫폼을 설치해야 한다.

쿠파이를 사용하면 엔비디아 GPU를 사용해 간단하게 병렬 계산을 수행할 수 있다. 또한 넘파이와 호환되는 API를 제공한다. 기본적으로 넘파이와 사용법이 같다.

## 1.6 정리

신경망의 기본을 복습했다. 계층을 구현할 때는 모든 클래스가 forward()와 backward() 메서드를 제공하고, params와 grads라는 인스턴스 변수를 갖는다는 '구현 규칙'을 따랐다. 이 규칙 덕분에 앞으로의 신경망 구현이 훨씬 쉬워진다.

- 신경망은 입력층, 은닉층(중간층), 출력층을 지닌다.
- 완전연결계층에 의해 선형 변환이 이뤄지고, 활성화 함수에 의해 비선형 변환이 이뤄진다.
- 완전연결계층이나 미니배치 처리는 행렬로 모아 한꺼번에 계산할 수 있다.
- 오차역전파법을 사용하여 신경망의 손실에 관한 기울기를 효율적으로 구할 수 있다.
- 신경망이 수행하는 처리는 계산 그래프로 시각화할 수 있으며, 순전파와 역전파를 이해하는데 도움이 된다.
- 신경망의 구성요소들을 '계층'으로 모듈화해두면, 이를 조립시켜 신경망을 쉽게 구성할 수 있다.
- 신경망 고속화에는 GPU를 이용한 병렬 계산과 데이터의 비트 정밀도가 중요하다.

