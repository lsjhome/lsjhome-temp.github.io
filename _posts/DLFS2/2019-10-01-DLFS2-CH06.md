---
layout: post
title: Chapter 06. 게이트가 추가된 RNN
comments: true
categories: [Deep Learning from Scratch 2]
tags: [Deep Learning, Machine Learning, NLP, RNN, LSTM]
author: lsjhome
---

# Chapter 6 게이트가 추가된 RNN

5장에서 본 RNN은 시계열 데이터에서 시간적으로 멀리 떨어진, 장기(long term) 의존 관계를 잘 학습할 수 없다.

요즘은 앞 장의 단순한 RNN 대신 LSTM이나 GRU 계층이 주로 쓰인다. 

LSTM이나 GRU 에는 '게이트'라는 구조가 있어, 시계열 데이터의 장기 의존 관계를 학습할 수 있다. 

## 6.1 RNN의 문제점

BPTT에서는 기울기 소실 혹은 기울기 폭발이 일어난다. 

### 6.1.1 RNN 복습

![fig 6-1](/assets/img/dlfs2/deep_learning_2_images/fig 6-1.png)



RNN 계층은 시계열 데이터인 $$x_{t}$$ 를 입력하면, $$h_{t}$$ 를 출력한다. 이 $$h_{t}$$ 는 RNN 계층의 **은닉 상태**라고 하여, 과거 정보를 저장한다.

RNN의 특징은 바로 이전 시각의 은닉 상태를 이용한다는 점이다. 이렇게 해서 과거 정보를 계승할 수 있게 된다. 이때 RNN 계층이 수행하는 처리를 계산 그래프로 나타내면 아래와 같다.

![fig 6-2](/assets/img/dlfs2/deep_learning_2_images/fig 6-2.png)

RNN 계층의 순전파에서 수행하는 계산은 행렬의 곱과 합, 그리고 활성화 함수인 tanh 함수에 의한 변환으로 구성된다. 이어서 이 RNN 계층이 안고 있는 문제, 즉 장기 기억에 취약하다는 문제를 살펴보자.

### 6.1.2 기울기 소실 또는 기울기 폭발

아래 문제를 다시 한번 생각해보자.

![fig 6-3](/assets/img/dlfs2/deep_learning_2_images/fig 6-3.png)

이 문제에 대해 올바르게 답하려면, 현재 맥락에서 "Tom이 방에서 TV를 보고 있음" 과 "그 방에 Marry가 들어옴" 이란 정보를 기억해둬야 한다.

이 예를 RNNLM 학습의 관점에서 생각해보자. 여기에서는 정답 레이블로 "Tom"이라는 단어가 주어졌을 때, RNNLM에서 기울기가 어떻게 전파되는지 살펴보자. 물론, 학습은 BPTT로 수행한다. 따라서 정답 레이블이 "Tom"이라고 주어진 시점으로부터 과거 방향으로 기울기를 전달하게 된다. 

![fig 6-4](/assets/img/dlfs2/deep_learning_2_images/fig 6-4.png)

RNN 계층이 과거 방향으로 '의미 있는 기울기'를 전달함으로써 시간 방향의 의존 관계를 학습할 수 있다. 하지만 만약 이 기울기가 중간에 사그라들면(거의 아무런 정보도 남지 않게 되면), 가중치 매개변수는 전혀 갱신되지 않게 된다. 즉, 단순한 RNN 계층에서는 시간을 거슬러 올라갈수록 기울기가 작아지거나(기울기 소실) 혹은 커질 수 있으며(기울기 폭발), 대부분 둘 중 하나의 운명을 걷게 된다.

### 6.1.3 기울기 소실과 기울기 폭발의 원인

![fig 6-5](/assets/img/dlfs2/deep_learning_2_images/fig 6-5.png)

역전파로 전해지는 기울기는 차례로 'tanh', '+', 'MatMul(행렬 곱)' 연산을 통과한다는 것을 알 수 있다.

'+'의 역전파는 상류에서 전해지는 기울기를 그대로 하류로 흘려보낼 뿐이다. 그래서 기울기는 변하지 않는다. 'tanh'와 'Matmul'의 경우는 어떨까?

y = tanh(x)의 미분은 $$\frac{\partial{y}}{\partial x} = 1 - y^{2}$$ 이다. 이때 y = tanh(x) 값과 미분 값을 각각 그래프로 그리면 그림처럼 된다. 

![fig 6-6](/assets/img/dlfs2/deep_learning_2_images/fig 6-6.png)

점선인 미분의 경우 그 값이 1 이하이고, x가 0에서 멀어질수록 작아진다. 즉, 역전파에서는 기울기가 tanh 노드를 지날 때마다 그 값은 계속 작아진다는 뜻이다. 따라서 tanh 함수를 T번 통과하면 기울기도 T번 반복해서 작아지게 된다.

> RNN 계층의 활성화 함수로 주로 tanh를 사용하는데, 이를 ReLU로 바꾸면 기울기 소실을 줄 일 수 있다. 그 이유는 ReLU는 입력 x가 0 이상이면, 역전파 시 상류의 기울기를 그대로 하류에 흘려보내기 때문이다. (ReLU에 x를 입력하면, max(0, x))

다음은 MatMul (행렬 곱)노드에 관해 알아보자.

![fig 6-7](/assets/img/dlfs2/deep_learning_2_images/fig 6-7.png)

상류로부터 **dh** 라는 기울기가 흘러들어온다. 이때 MatMul 노드에서의 역전파는 $$dhW_{t}^{t}$$ 라는 행렬 곱으로 기울기를 계산한다. 그리고 같은 계산을 시계열 데이터의 시간 크기만큼 반복한다. 여기서 주목할 점은 이 행렬 곱셈에서는 매번 똑같은 가중치인 $$W_{h}$$ 가 사용된다.

역전파 시 기울기는 MatMul 노드를 지날 때마다 변하게 된다. 

```python
import numpy as np
import matplotlib.pyplot as plt


N = 2   # 미니배치 크기
H = 3   # 은닉 상태 벡터의 차원 수
T = 20  # 시계열 데이터의 길이

dh = np.ones((N, H))

np.random.seed(3) # 재현할 수 있도록 난수의 시드 고정

Wh = np.random.randn(H, H)
#Wh = np.random.randn(H, H) * 0.5

norm_list = []
for t in range(T):
    dh = np.dot(dh, Wh.T)
    norm = np.sqrt(np.sum(dh**2)) / N
    norm_list.append(norm)
```

dh를 np.ones() 로 초기화하고 역전파의 MatMul 노드 수 만큼 dh를 갱신한다. 각 단계에서 dh 크기(노름)를 norm\_list에 추가한다. 또한 미니배치 N개의 평균 'L2 노름'을 구해 dh 크기로 사용하고 있다. 참고로 L2 노름으란, 각각의 원소를 제곱해 더하고 제곱근을 취한 값이다.

![fig 6-8](/assets/img/dlfs2/deep_learning_2_images/fig 6-8.png)

기울기의 크기는 시간에 비례해 지수적으로 증가하는 것을 알 수 있다. 이것이 바로 **기울기 폭발**(exploding gradients) 이다. 이러한 기울기 폭발이 일어나면 결국 오버플로를 일으켜 NaN 과 같은 값을 발생시킨다. 따라서 신경망 학습을 제대로 수행할 수 없게 된다.

Wh의 초깃값을 다음과 같이 변경한 후 두 번째 실험을 해 보자.

```python
# Wh = np.random.randn(H, H)     # 변경 전
Wh = np.random.randn(H, H) * 0.5 # 변경 후
```

![fig 6-9](/assets/img/dlfs2/deep_learning_2_images/fig 6-9.png)

기울기가 지수적으로 감소한다. 이것이 **기울기 소실** 이다. 기울기 소실이 일어나면 기울기가 매우 빠르게 작아진다. 그리고 기울기가 일정 수준 이하로 작아지면 가중치 매개변수가 더 이상 갱신되지 않으므로, 장기 의존 관계를 학습할 수 없게 된다.

이러한 지수적인 변화가 일어날 이유로는, 행렬 Wh를 T번 반복해서 곱했기 때문이다. 만약 Wh가 스칼라라면 이야기는 단순하지만, wh가 1보다 크면 지수적으로 증가하고, 1보다 작으면 지수적으로 감소한다.

만약 Wh가 스칼라가 아니라 행렬이라면, 행렬의 '특이값'이 척도가 된다. 행렬의 특이값이란, 데이터가 얼마나 퍼져 있는지를 나타낸다. 이 특잇값의 값(여러 특잇값 중 최댓값)이 1보다 큰지 여부를 보면 기울기 크기가 어떻게 변할지 예측할 수 있다.

> 특잇값의 최대값이 1보다 크면 지수적으로 증가하고, 1보다 작으면 지수적으로 감소할 가능성이 높다. 하지만 반드시 특잇값이 1보다 크다고 해서 기울기가 폭발하는 것은 아니다. 즉, 필요조건일 뿐 충분조건은 아니다.

### 6.1.4 기울기 폭발 대책

기울기 폭발의 대책으로는, **기울기 클리핑**(gradients clipping)이라는 기법이 있다. 의사 코드로 나타내면 다음과 같다.

![e 6-0](/assets/img/dlfs2/deep_learning_2_images/e 6-0.png)

신경망에서 처리되는 사용되는 모든 매개변수에 대한 기울기를 하나로 처리한다고 가정하고, 이를 기호 **$$\hat{g}$$** 로 표기했다. 그리고 *threshold* 를 문턱값으로 설정한다. 이때 기울기의 L2 노름이 문턱값을 초과하면, 두 번째 줄의 수식과 같이 기울기를 수정한다. 이것이 기울기 클리핑이다. 

> **$$\hat{g}$$** 는 신경망에서 사용되는 모든 매개변수의 기울기를 하나로 모은 것이다. 예컨대 두 개의 가중치 W1과 W2 매개변수를 사용하는 모델이 있다면, 이 매개변수에 대한 기울기 dW1과 dW2를 결합한 것을 **$$\hat{g}$$** 라 한다.

```python
dW1 = np.random.rand(3, 3) * 10
dW2 = np.random.rand(3, 3) * 10
grads = [dW1, dW2]
max_norm = 5.0

def clip_grads(grads, max_norm):
    total_norm = 0
    for grad in grads:
        total_norm += np.sum(grad ** 2)
      total_norm = np.sqrt(total_norm)

    rate = max_norm / (total_norm + 1e-6)
    if rate < 1:
        for grad in grads:
            grad *= rate
```

## 6.2 기울기 소실과 LSTM

RNN학습에서는 기울기 소실도 큰 문제이다. 이를 근본적으로 고치기 위해서 '게이트가 추가된 RNN' 이 등장한다. LSTM과 GRU 등인데, LSTM의 경우 기울기 소실이 일어나지 않는다.

### 6.2.1 LSTM의 인터페이스

계산 그래프를 단순화하는 도법을 하나 도입하자. 

![fig 6-10](/assets/img/dlfs2/deep_learning_2_images/fig 6-10.png)

여기에서는 tanh(**$$h_{t-1}W_{h} + x_{t}W_{x} + b$$**) 계산을 tanh라는 직사각형 노드 하나로 그렸다. $$h_{t-1}$$과 $$x_{t}$$는 행벡터이다. 이 직사각형 노드 안에 행렬 곱의 편향의 합, 그리고 tanh 함수에 의한 변환이 모두 포함된 것이다.

LSTM의 입출력을 RNN과 비교해보자.

![fig 6-11](/assets/img/dlfs2/deep_learning_2_images/fig 6-11.png)

LSTM 계층의 인터페이스에는 **c**라는 경로가 있다는 차이가 있다. 이 **c**를 **기억 셀**(혹은 단순히 '셀') 이라 하며, LSTM 전용의 기억 메커니즘이라고 한다.

기억 셀의 특징은 데이터를 자기 자신으로만(LSTM 계층 내에서만) 주고받는다는 것이다. 즉, LSTM 계층 내에서만 완결되고, 다른 계층으로는 출력하지 않는다. 반면, LSTM의 은닉 상태 **h**는 RNN 계층과 마찬가지로 다른 계층으로 (위쪽으로) 출력된다.

> LSTM 출력을 받는 쪽에서 보면 LSTM의 출력은 은닉 상태 벡터 **h** 뿐이다. 기억 셀 **c** 는 외부에서는 보이지 않는다. 그래서 그 존재 자체를 생각할 필요가 없다.

### 6.2.2 LSTM 계층 조립하기

LSTM에는 기억 셀 $$c_{t}$$가 존재한다. 이 $$c_{t}$$ 에는 시각 t에서의 LSTM 기억이 저장되어 있는데, 과거로부터 시각 t까지에 필요한 모든 정보가 저장되어 있다고 가정하자. (혹은 그렇게 되도록 학습을 수행한다.) 그리고 필요한 정보를 모두 간직한 이 기억을 바탕으로, 외부 계층에(그리고 다음 시각의 LSTM) 은닉 상태 **$$h_{t}$$** 를 출력한다. 이때 출력하는 **$$h_{t}$$** 는 기억 셀의 값을 tanh 함수로 변환한 값이다.

![fig 6-12](/assets/img/dlfs2/deep_learning_2_images/fig 6-12.png)

위 그림처럼 현재의 기억 셀 $$c_{t}$$는 3개의 입력 (**$$c_{t-1}, h_{t-1}, x_{t}$$**) 으로부터 '어떤 계산'을 수행하여 구할 수 있다. 여기서 핵심은 갱신된 $$c_{t}$$ 를 사용해 은닉 상태 $$h_{t}$$를 계산한다는 것이다. 또한 이 계산은 $$h_{t} = tanh(c_{t})$$ 인데, 이는 $$c_{t}$$의 각 요소에 tanh 함수를 적용한다는 뜻이다.

> 기억 셀 $$c_{t}$$ 와 은닉 상태 $$h_{t}$$ 의 관계는 $$c_{t}$$ 의 각 원소에 tanh 함수를 적용한 것일 뿐이다. 이 말이 뜻하는 바는 기억 셀 $$c_{t}$$ 와 은닉 상태 $$h_{t}$$ 의 원소 수는 같다는 것이다. 예컨대 기억 셀 $$c_{t}$$의 원소가 100개이면 은닉 상태 $$h_{t}$$ 의 원소 수도 100개가 된다.

게이트란 우리말로는 '문'을 의미하는 단어다. 문은 열거나 닫을 수 있듯이, 게이트는 데이터의 흐름을 제어한다. 마치 [그림 6-13]처럼 물의 흐름을 멈추거나 배출하는 것이 게이트의 역할이다.

![fig 6-13](/assets/img/dlfs2/deep_learning_2_images/fig 6-13.png)

LSTM에서 사용하는 게이트는 '열기/닫기' 뿐 아니라, 어느 정도 열지를 조절할 수 있다. 다시 말해 다음 단게로 흘려 보낼 물의 양을 제어한다. '어느 정도'를 '열림 상태'라 부르며, 아래 그림처럼 0.7(70%)이나 0.2(20%)처럼 제어할 수 있다.

![fig 6-14](/assets/img/dlfs2/deep_learning_2_images/fig 6-14.png)

게이트의 열림 상태는 0.0~1.0 사이의 실수로 나타난다. (1.0은 완전 개방) 여기서 중요한 점은, '게이트를 얼마나 열까' 라는 것도 데이터로부터 (자동으로) 학습한다는 점이다.

>  게이트는 게이트의 열림 상태를 제어하기 위해서 전용 가중치 매개변수를 이용하며, 이 가중치 매개변수는 학습 데이터로부터 갱신된다. 참고로, 게이트의 열림 상태를 구할 때는 시그모이드 함수를 사용하는데, 시그모이드 함수의 출력이 마침 0.0 ~ 1.0 사이의 실수이기 때문이다.

### 6.2.3 output 게이트

다시 LSTM으로 돌아오자. 바로 앞에서의 은닉 상태 $$h_{t}$$ 는 기억 셀 $$c_{t}$$ 에 단순히 tanh 함수를 다시 적용했을 뿐이라고 설명했다. 이번 절에서는 tanh($$c_{t}$$) 에 게이트를 적용하는 걸 생각해보자. 즉, $$tanh(c_{t})$$ 의 각 원소에 대해 '그것이 다음 시각의 은닉 상태에 얼마나 중요한가'를 조정한다. 한편, 이 게이트는 다음 은닉 상태 $$h_{t}$$ 의 출력을 담당하는 게이트이므로 **output 게이트** (출력 게이트) 라고 한다.

output 게이트의 열림 상태 (다음 몇 %만 흘려보낼까)는 입력 $$x_{t}$$와 이전 상태 $$h_{t-1}$$ 로부터 구한다. 이때의 계산은 아래 식과 같다. 참고로, 여기서 사용하는 가중치 매개변수와 편향에는 output의 첫 글자인 **o**를 첨자로 추가한다. 이후에도 마찬가지로 첨자를 붙여 게이트임을 표시한다. 한편, 시그모이드 함수는 $$\sigma$$로 표기한다. (식 6.1)

![e 6-1](/assets/img/dlfs2/deep_learning_2_images/e 6-1.png)

입력 $$x_{t}$$ 에는 가중치 $$W_{x}^{o}$$ 가, 그 이전의 은닉 상태 $$h_{t-1}$$ 에는 가중치 $$W_h^{o}$$ 가 붙어있다. ($$x_{t}$$ 와 $$h_{t-1}$$ 는 행벡터). 그리고 이 행렬들의 곱과 편향 $$b^{o}$$ 를 모두 더한 다음 다음 시그모이드 함수를 거쳐 출력 게이트의 출력 $$o$$ 를 구한다. 마지막으로 이 $$o$$와  $$tanh(c_{t})$$ 의 원소별 곱을 $$h_{t}$$ 로 출력한다. 이상의 계산을 계산 그래프로는 아래처럼 그릴 수 있다.

![fig 6-15](/assets/img/dlfs2/deep_learning_2_images/fig 6-15.png)

위 그림과 같은 output 게이트에서 수행하는 [식 6.1]의 계산을 ''$$\sigma$$"로 표기하겠다. 그리고 $$\sigma$$ 의 출력을 $$o$$라고 하면, $$h_{t}$$ 는 $$o$$와 $$tanh(c_{t})$$ 의 곱으로 계산된다. 여기서 말하는 '곱'이란 원소별 곱이며, 이것을 **아다마르 곱**(Hadamard product) 이라고도 한다. 아다마르 곱을 기호로는 $$\bigodot$$ 으로 나타내며, 아래와 같은 계산을 수행한다.

![e 6-2](/assets/img/dlfs2/deep_learning_2_images/e 6-2.png)

이상이 LSTM의 output 게이트이다. 이제  LSTM의 출력 부분은 완성되었다. 이어서 기억 셀 갱신 부분을 살펴보자. 

> tanh의 출력은 -1.0 ~ 1.0 의 실수이다. 이 -1.0 ~ 1.0의 수치를 그 안에 인코딩된 '정보'의 강약 (정도)을 표시한다고 해석할 수 있다. 한편 시그모이드 함수의 출력은 0.0 ~ 1.0의 실수이며, 데이터를 얼마만큼 통과시킬지를 정하는 비율을 뜻한다. 따라서 (주로) 게이트에서는 시그모이드 함수가, 실질적인 '정보'를 지니느 데이터에서는 tanh 함수가 활성화 함수로 사용된다. 

### 6.2.4 forget 게이트

망각은 중요하다. 우리가 할 일은 기억 셀에 '무엇을 잊을까'를 명확하게 지시하는 것이다. 이런 일도 물론 게이트를 사용해 해결할 수 있다.

$$c_{t-1}$$ 의 기억 중에서 불필요한 기억을 잊게 해 주는 게이트를 추가해보자. 이를  **forget 게이트 **(망각 게이트) 라 부르기로 하자. forget 게이틀르  LSTM 계층에 추가하면 계산 그래프가 [그림 6-16] 처럼 된다.

![fig 6-16](/assets/img/dlfs2/deep_learning_2_images/fig 6-16.png)

forgot 게이트가 수행하는 일련의 계산을 $$\sigma$$ 노드로 표기했다. 이 $$\sigma$$ 안에는 forget 게이트 전용의 가중치 매개변수가 있으며, 다음 식의 계산을 수행한다. [식 6.3]

![e 6-3](/assets/img/dlfs2/deep_learning_2_images/e 6-3.png)

위  식을 실행하면 forget 게이트의 출력 f가 구해진다. 그리고 이 f와 이전 기억 셀인 $$c_{t-1}$$과의 원소별 곱, 즉 $$c_{t} = f$$ $$\bigodot$$ $$ c_{t-1}$$ 을 계산하여 $$c_{t}$$ 를 구한다.

### 6.2.5 새로운 기억 셀

forget 게이트를 거치면서 이전 시각의 기억 셀로부터 잊어야 할 기억이 삭제되었다. 이제 새롭게 기억해야 할 정보를 기억 셀에 추가해야 한다. 그렇지 않으면 기억 셀이 잊는 일 밖에 하지 못할 것이다. 아래와 같이 tanh 노드를 추가하자.

![fig 6-17](/assets/img/dlfs2/deep_learning_2_images/fig 6-17.png)

tanh 노드가 계산한 결과가 이전 시각의 기억 셀 $$c_{t-1}$$ 에 더해진다. 기억 셀에 새로운 '정보'가 추가된 것이다. 이 tanh 노드는 '게이트'가 아니며, 새로운 '정보'를 기억 셀에 추가하는 것이 목적이다. 따라서 활성화 함수로 시그모이드 함수가 아닌 tanh 함수가 사용된다. tanh 노드에서 수행하는 계산은 다음과 같다.

![e 6-4](/assets/img/dlfs2/deep_learning_2_images/e 6-4.png)

여기에서 기억 셀에 추가하는 새로운 기억을 **g** 로 표기했다. 이 식 **g** 가 이전 시각의 기억 셀인 $$c_{t-1}$$ 에 더해지면서 새로운 기억이 생겨난다.

### 6.2.6 input 게이트

![fig 6-18](/assets/img/dlfs2/deep_learning_2_images/fig 6-18.png)

input 게이트는 g의 각 원소가 새로 추가되는 정보로써의 가치가 얼마나 큰지를 판단한다. 새 정보를 무비판적으로 수용하는 것이 아니라, 적절히 취사선택하는 것이 이 게이트의 역할이다. 다른 관점에서 보면, input 게이트에 의해 가중된 정보가 새롭게 추가되는 셈이다.

위 그림에서는 input 게이트를 $$\sigma$$ 로, 그 출력을 i로 표기했다. 이때 수행하는 계산은 다음과 같다.

![e 6-5](/assets/img/dlfs2/deep_learning_2_images/e 6-5.png)

그런 다음 **i** 와 **g** 의 원소별 곱 결과를 기억 셀에 추가한다. 이상이 LSTM 안에서 이뤄지는 처리이다.

### 6.2.7 LSTM의 기울기 흐름

LSTM의 구조는 설명했지만, 이것이 어떤 원리로 기울기 소실을 없애 주는 것일까? 기억 셀 **c**의 역전파에 주목해 보자.

![fig 6-19](/assets/img/dlfs2/deep_learning_2_images/fig 6-19.png)

위 그림은 기억 셀에만 집중하며, 그 역전파의 흐름을 그린 것이다. 이때 기억 셀의 역전파에서는 '+'와 'x' 노드만을 지나게 된다. '+'노드는 상류에서 전해지는 기울기를 그대로 흘릴 뿐이다. 따라서 기울기 변화(감소)는 일어나지 않는다.

남는 것은 'x'노드인데, 이 노드는 '행렬 곱' 이 아닌 '원소별 곱(아마다르 곱)'을 계산한다. 앞의 RNN의 역전파에서는 똑같은 가중치 행렬을 사용하여 '행렬 곱'을 반복했고, 그래서 기울기 소실(또는 기울기 폭발)이 일어났다. 반면 LSTM의 역전파에서는 '행렬 곱'이 아닌 '원소별 곱'이 이뤄지고, 매 시각 다른 게이트 값을 이용해 원소별 곱을 계산한다. 이렇게 매번 새로운 게이트 값을 이용하므로 곱셈의 효과가 누적되지 않아 기울기 소실이 일어나지 않는 (혹은 일어나기 힘든) 것이다.

'x' 노드의 계산은 forget 게이트가 제어한다. 그리고 forget 게이트가 '잊어야 한다'고 판단한 기억 셀의 원소에 대해서는 그 기울기가 작아진다. 한편, forget 게이트가 '잊어서는 안 된다' 고 판단한 원소에 대해서는 그 기울기가 약화되지 않은 채로 과거 방향으로 전해진다. 따라서 기억 셀의 기울기가(오래 기억해야 할 정보일 경우) 소실 없이 전파되리라 기대할 수 있다.

따라서 LSTM의 기억 셀에서는 기울기 소실이 잘 일어나지 않는다. 그렇기 때문에 기억 셀이 장기 의존 관계를 유지(학습) 하리라 기대할 수 있다.

> LSTM은 Long Short-Term Memory인데, 이 뜻은 **단기 기억**(short-term memory)을 **긴**(long) 시간 동안 지속할 수 있음을 의미한다.

## 6.3 LSTM 구현

우선 최초의 한 단계만 처리하는 LSTM 클래스를 구현한 다음, 이어서 T개의 단계를 한꺼번에 처리하는 Time LSTM 클래스를 구현한다. 다음은 LSTM에서 수행하는 계산이다. 

![e 6-6](/assets/img/dlfs2/deep_learning_2_images/e 6-6.png)

![e 6-7](/assets/img/dlfs2/deep_learning_2_images/e 6-7.png)

![e 6-8](/assets/img/dlfs2/deep_learning_2_images/e 6-8.png)

여기서 주목할 점은, 식의 네 수식에 포함된 아핀 변환(affine transformation)이다. 여기서 '아핀 변환' 이란 행렬 변환과 평행 이동(편향)을 결합항 형태, 즉 $$xW_{x} + hW_{h}+b$$ 형태의 식을 가리킨다. 위의 수식에서는 아핀 변환을 개별적으로 수행하지만, 이를 하나의 식으로 정리해 계산할 수 있다. 그 방법을 시각적으로 설명하면 아래 그림과 같다.

![fig 6-20](/assets/img/dlfs2/deep_learning_2_images/fig 6-20.png)

위 그림에서 보듯, 4개의 가중치(또는 편향)를 하나로 모을 수 있고, 그렇게 하면 원래 개별적으로 총 4번을 수행하던 아핀 변환을 단 1회의 계산으로 끝마칠 수 있다. 계산 속도가 빨라진다. 왜냐하면 일반적으로 행렬 라이브러리는 '큰 행렬'을 한꺼번에 계산할 때가 각각을 따로 계산할 때 보다 빠르기 때문이다. 한편, 가중치를 한 데로 모아 관리하게 되어 소스 코드도 간결해진다.

그러면 $$ W_{x}, W_{h}, b $$ 각각에 4개분의 가중치(혹은 편향)가 포함되어 있다고 가정하고, 이때의 LSTM 계산 그래플르 그려보면 아래 그림과 같이 된다.

![fig 6-21](/assets/img/dlfs2/deep_learning_2_images/fig 6-21.png)

처음 4개분의 아핀 변환을 한꺼번에 수행한다. 그리고 slice 노드를 통해 그 4개의 결과를 꺼낸다. slice는 아핀 변환의 결과(행렬)를 균등하게 네 조각으로 나눠서 꺼내주는 단순한 노드이다. slice 노드 다음에는 활성화 함수(시그모이드 함수 또는 tanh 함수)를 거쳐 앞 절에서 설명한 계산을 수행한다.

LSTM 클래스를 구현해 보자. 

```python
class LSTM:
    
    def __init__(self, Wx, Wh, b):
        self.params = [Wx, Wh, b]
        self.grads = [np.zeros_like(Wx), np.zeros_like(Wb), np.zeros_like(b)]
        self.cache = None
```

초기화 인수는 가중치 매개변수인 Wx, Wh 그리고 편향을 뜻하는 b이다. 앞에서 말한 것처럼, 이 가중치(와 편향)에는 4개분의 가중치가 담겨 있다. 이 인수들은 인스턴스 변수인 params에 할당하고, 이에 대응하는 형태로 기울기도 초기화한다. 한편, cache는 순전파때 중간 결과를 보관했다가 역전파 계산에 사용하려는 용도의 인스턴스 변수이다.

계속해서 순전파 구현을 살펴보자. 순전파는 forward(x, h_prev, c_prev) 메서드로 구현한다. 인수로는 현 시각의 입력 x, 이전 시각의 은닉 상태 h_prev, 이전 시각의 기억 셀 c_prev를 받는다.

```python
    def forward(self, x, h_prev, c_prev):
        Wx, Wh, b = self.params
        N, H = h_prev.shape

        A = np.dot(x, Wx) + np.dot(h_prev, Wh) + b

        f = A[:, :H]
        g = A[:, H:2*H]
        i = A[:, 2*H:3*H]
        o = A[:, 3*H:]

        f = sigmoid(f)
        g = np.tanh(g)
        i = sigmoid(i)
        o = sigmoid(o)

        c_next = f * c_prev + g * i
        h_next = o * np.tanh(c_next)

        self.cache = (x, h_prev, c_prev, i, f, g, o, c_next)
        return h_next, c_next
```

이 메서드에서 가장 먼저 아핀 변환을 진행한다. 이때 인스턴스 변수 Wx, Wh, b에는 각각 4개분의 매개변수가 저장되어 있으며, 이 변환을 행렬의 형상과 함께 그려보면 아래 그림처럼 된다.

![fig 6-22](/assets/img/dlfs2/deep_learning_2_images/fig 6-22.png)

위 그림에서는 미니배치 수를 N, 입력 데이터의 차원 수를 D, 기억 셀과 은닉 상태의 차원 수를 모두 H로 표시했다. 그리고 계산 결과인 A에는 네 개분의 아핀 변환 결과가 저장된다. 따라서 이 결과로부터 데이터를 꺼낼 때는 A[:, :H]나 A[:, H:2*H] 형태로 슬라이스 해서 꺼내고, 꺼낸 데이터를 다음 연산 노드에 분배한다.

slice 노드는 행렬을 네 조각으로 나눠서 분배했으니, 역전파에서는 반대로 4개의 기울기를 결합해야 한다. 아래와 같다.

![fig 6-23](/assets/img/dlfs2/deep_learning_2_images/fig 6-23.png)

그림에서는 4개의 기울기 **df, dg, di, do**를 연결해서 **dA**를 만들었다. 이를 넘파이로 수행하려면 np.hstack() 메서드를 사용하면 된다. np.hstac() 은 인수로 주어진 배열들을 가로로 연결한다. (세로로 연결하는 np.vstack()도 있다.) 따라서 이 처리를 다음의 한 줄로 끝낼 수 있다.

```python
da = np.hstack([df, dg, di, do])
```

### 6.3.1 Time LSTM 구현

계속해서 Time LSTM 구현으로 넘어가 보자. Time LSTM은 T개분의 시계열 데이터를 한꺼번에 처리하는 계층이다. 그 전체 그림은 아래 그림과 같이 T개의 LSTM 계층으로 구성된다.

![fig 6-24](/assets/img/dlfs2/deep_learning_2_images/fig 6-24.png)

순전파의 흐름은 그대로 유지한다. 은닉 상태와 기억 셀을 인스턴스 변수로 유지하도록 한다. 그렇게 하면 다음번에 forward()가 불렸을 때, 이전 시각의 은닉 상태(와 기억 셀)에서부터 시작할 수 있게 되는 것이다.

![fig 6-25](/assets/img/dlfs2/deep_learning_2_images/fig 6-25.png)

```python
class TimeLSTM:
    def __init__(self, Wx, Wh, b, stateful=False):
        self.params = [Wx, Wh, b]
        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]
        self.layers = None

        self.h, self.c = None, None
        self.dh = None
        self.stateful = stateful

    def forward(self, xs):
        Wx, Wh, b = self.params
        N, T, D = xs.shape
        H = Wh.shape[0]

        self.layers = []
        hs = np.empty((N, T, H), dtype='f')

        if not self.stateful or self.h is None:
            self.h = np.zeros((N, H), dtype='f')
        if not self.stateful or self.c is None:
            self.c = np.zeros((N, H), dtype='f')

        for t in range(T):
            layer = LSTM(*self.params)
            self.h, self.c = layer.forward(xs[:, t, :], self.h, self.c)
            hs[:, t, :] = self.h

            self.layers.append(layer)

        return hs

    def backward(self, dhs):
        Wx, Wh, b = self.params
        N, T, H = dhs.shape
        D = Wx.shape[0]

        dxs = np.empty((N, T, D), dtype='f')
        dh, dc = 0, 0

        grads = [0, 0, 0]
        for t in reversed(range(T)):
            layer = self.layers[t]
            dx, dh, dc = layer.backward(dhs[:, t, :] + dh, dc)
            dxs[:, t, :] = dx
            for i, grad in enumerate(layer.grads):
                grads[i] += grad

        for i, grad in enumerate(grads):
            self.grads[i][...] = grad
        self.dh = dh
        return dxs

    def set_state(self, h, c=None):
        self.h, self.c = h, c

    def reset_state(self):
        self.h, self.c = None, None
```

LSTM은 은닉 상태 h와 함께 기억 셀 c도 이용하지만, TimeLSTM 클래스의 구현은 TimeRNN 클래스와 흡사하다. 여기에서도 인수 stateful로 상태를 유지할지를 지정한다. 다음 순서로는 이 TimeLSTM을 이용해서 언어 모델을 만들어보자.

## 6.4 LSTM을 사용한 언어 모델

![fig 6-26](/assets/img/dlfs2/deep_learning_2_images/fig 6-26.png)

앞 장에서 구현한 언어 모델과의 차이는 LSTM을 사용한다는 점 뿐이다. 

```python
import sys
sys.path.append('..')
from common.time_layers import *
from common.base_model import BaseModel


class Rnnlm(BaseModel):
    def __init__(self, vocab_size=10000, wordvec_size=100, hidden_size=100):
        V, D, H = vocab_size, wordvec_size, hidden_size
        rn = np.random.randn

        # 가중치 초기화
        embed_W = (rn(V, D) / 100).astype('f')
        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')
        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')
        lstm_b = np.zeros(4 * H).astype('f')
        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')
        affine_b = np.zeros(V).astype('f')

        # 계층 생성
        self.layers = [
            TimeEmbedding(embed_W),
            TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True),
            TimeAffine(affine_W, affine_b)
        ]
        self.loss_layer = TimeSoftmaxWithLoss()
        self.lstm_layer = self.layers[1]

        # 모든 가중치와 기울기를 리스트에 모은다.
        self.params, self.grads = [], []
        for layer in self.layers:
            self.params += layer.params
            self.grads += layer.grads

    def predict(self, xs):
        for layer in self.layers:
            xs = layer.forward(xs)
        return xs

    def forward(self, xs, ts):
        score = self.predict(xs)
        loss = self.loss_layer.forward(score, ts)
        return loss

    def backward(self, dout=1):
        dout = self.loss_layer.backward(dout)
        for layer in reversed(self.layers):
            dout = layer.backward(dout)
        return dout

    def reset_state(self):
        self.lstm_layer.reset_state()

```

위 클래스에서는 Softmax 계층 직전까지를 처리하는 predict() 메서드가 추가되었다. 이 메서드는 7장에서 수행하는 문장 생성에 사용된다. 그리고 매개변수 읽기/쓰기를 처리하는 load\_params()와 save\_params() 메서드도 추가되었다. 나머지는 앞 장의 SimpleRnnlm 클래스와 같다.

이 신경망을 사용해 PTB 데이터셋을 학습해보자. 

```python
# coding: utf-8
import sys
sys.path.append('..')
from common.optimizer import SGD
from common.trainer import RnnlmTrainer
from common.util import eval_perplexity
from dataset import ptb
from rnnlm import Rnnlm


# 하이퍼파라미터 설정
batch_size = 20
wordvec_size = 100
hidden_size = 100  # RNN의 은닉 상태 벡터의 원소 수
time_size = 35     # RNN을 펼치는 크기
lr = 20.0
max_epoch = 4
max_grad = 0.25

# 학습 데이터 읽기
corpus, word_to_id, id_to_word = ptb.load_data('train')
corpus_test, _, _ = ptb.load_data('test')
vocab_size = len(word_to_id)
xs = corpus[:-1]
ts = corpus[1:]

# 모델 생성
model = Rnnlm(vocab_size, wordvec_size, hidden_size)
optimizer = SGD(lr)
trainer = RnnlmTrainer(model, optimizer)

# 1. 기울기 클리핑을 적용하여 학습
trainer.fit(xs, ts, max_epoch, batch_size, time_size, max_grad,
            eval_interval=20)
trainer.plot(ylim=(0, 500))

# 2. 테스트 데이터로 평가
model.reset_state()
ppl_test = eval_perplexity(model, corpus_test)
print('테스트 퍼플렉서티: ', ppl_test)

# 3. 매개변수 저장
model.save_params()

```

RnnlmTrainer 클래스의 fit() 메서드는 모델의 기울기를 구해 모델의 매개변수를 갱신한다. 이때 인수로 max\_grad를 지정해 기울기 클리핑을 적용한다.

1의 fit 메서드에서 인수 eval\_interval=20은 20번째 반복마다 퍼플렉시티를 평가하라는 뜻이다. 이번에는 데이터가 크기 때문에 모든 에폭에서 평가하지 않고, 20번 반복될 때마다 평가하도록 한다. 그리고 나중에 plot() 메서드를 호출해 그 결과를 그래프로 그린다.

2에서는 학습이 끝난 후 테스트 데이터를 사용해 퍼플렉시티를 평가한다. 이때 모델 상태(LSTM의 은닉 상태와 기억 셀)을 재설정하여 평가를 수행하는 점에 주의하자.

마지막으로 3에서 학습이 완료된 매개변수들을 파일로 저장하자. 다음 장에서 학습된 가중치 매개변수를 사용해 문장으로 생성할 때 이 파일을 사용한다. 

![fig 6-27](/assets/img/dlfs2/deep_learning_2_images/fig 6-27.png)

위 그림에서는 매 20번째 반복의 퍼플렉서티 값이 출력되었다. 첫 번째 퍼플렉서티의 값이 10000.84라는 뜻은, 다음에 나오 수 있는 후보의 단어 수를 10,000개 정도로 좁혔다는 뜻이 된다. 데이터셋의 어휘 수가 10,000개라는 것을 생각하면, 결국 아무것도 학습하지 않은 상태라는 것이다.

하지만 학습을 계속하면서 기대한 대로 퍼플렉서티가 좋아진다. 실제로 반복이 300회를 넘자 퍼플렉서티가 400을 밑돌기 시작한다.

![fig 6-28](/assets/img/dlfs2/deep_learning_2_images/fig 6-28.png)

최종적으로는 136 전후가 된다. 즉, 다음에 나올 단어의 후보를 136개 정도로 줄일 때가지 개선된 것이다.

2017년 기준 최첨단 연구에서는 PTB 데이터셋의 퍼플렉서티가 60을 밑돌고 있다. 개선 방법에 대해서 다음 절에서 알아보자.

## 6.5 RNNLM 추가 개선

RNNLM의 개선 포인트 3가지를 설명하자. 그리고 그 개선들을 구현하고, 마지막에는 실제로 얼마나 좋아졌는지를 평가해 보자.

### 6.5.1 LSTM 계층 다층화

![fig 6-29](/assets/img/dlfs2/deep_learning_2_images/fig 6-29.png)

지금까지 우리는 LSTM 계층을 1층만 사용했지만, 여러 겹 쌓으면 언어 모델의 정확도가 향상되리라 기대할 수 있다. 예컨대 LSTM을 2층으로 쌓아 RNNLM을 만든다고 하면 [그림 6-29]처럼 된다.

언어 모델에서 LSTM의 층 수는 2~4 정도일 때 좋은 결과를 얻는다.

> 구글 번역에서 사용하는 GNMT 모델은 LSTM을 8층이나 쌓은 신경망이라고 알려져 있다. 복잡하고 학습 데이터를 대량으로 준비할 수 있다면 LSTM 계층을 '깊게' 쌓는 것이 정확도 향상을 이끌 수도 있다.

### 6.5.2 드롭아웃에 의한 과적합 억제

LSTM 계층을 다층화하면 시계열 데이터의 복잡한 의존 관계를 학습할 수 있을 것이라 기대할 수 있다. 다르게 표현하자면, 층을 깊게 쌓음으로써 표현력이 풍부한 모델을 만들 수 있다. 그러나 이런 모델은 종종 **과적합**을 일으킨다. 불행히도 RNN은 일반적인 피드포워드 신경망보다 더 쉽게 과적합을 일으킨다.

과적합을 억제하는 전통적인 방법으로는, '훈련 데이터의 양'을 늘리는 것과 '모델의 복잡도'를 줄이는 방법이 있다. 그 외에는 모델의 복잡도에 페널티를 주는 **정규화**도 효과적이다. 예컨대 L2 정규화는 가중치가 너무 커지면 페널티를 부과한다.

또 드롭아웃(dropout) 처럼 훈련 시 계층 내의 뉴런 몇개(예컨대 50% 등)을 무작위로 무시하고 학습하는 방법도 일종의 정규화라고 할 수 있다. 이번 절에서는 이 드롭아웃에 관해 자세히 살펴보고 RNN에 적용해보자.

![fig 6-30](/assets/img/dlfs2/deep_learning_2_images/fig 6-30.png)

드롭아웃은 무작위로 뉴런을 선택하여 선택한 뉴런을 무시한다. 무시한다는 뜨승ㄴ, 그 앞 계층으로부터 신호 전달을 막는다는 뜻이다. 이 '무작위한 무시'가 제약이 되어 신경망의 일반화 성능을 개선한다는 것이다. 

![fig 6-31](/assets/img/dlfs2/deep_learning_2_images/fig 6-31.png)

RNN을 사용한 모델에서는 드롭아웃 계층을 어디에 삽입해야 할까? 첫 번재 후보는 시계열 방향으로 삽입하는 것이다.

![fig 6-32](/assets/img/dlfs2/deep_learning_2_images/fig 6-32.png)

RNN에서 시계열 방향으로 드롭아웃을 넣어버리면 (학습 시) 시간의 흐름에 따라 정보가 사라질 수 있다. 즉, 흐르는 시간에 비례해 드롭아웃에 의한 노이즈가 축적된다. 노이즈 축적을 고려하면, 시간축으로의 드롭아웃은 사용하지 않는 것이 낫다.

![fig 6-33](/assets/img/dlfs2/deep_learning_2_images/fig 6-33.png)

'일반적인 드롭아웃'은 시간 방향에는 적합하지 않다. 그러나 최근 연구에서는 RNN의 시간 방향 정규화를 목표로 하는 방법이 다양하게 제안되고 있다. **변형 드롭아웃**의 경우, 시간 방향으로 적용하는데 성공하였다. 

변형 드롭아웃은 깊이 방향은 물론 시간 방향에도 이용할 수 있어서, 언어 모델의 정확도를 한층 더 향상시킬 수 있다. 그 구조는 아래와 같은데, 같은 계층에 속한 드롭아웃은 같은 마스크(mask)를 공유한다. 여기서 말하는 '마스크'란 데이터의 '통과/차단'을 결정하는 이진(binary)형태의 무작위 패턴을 뜻한다.

![fig 6-34](/assets/img/dlfs2/deep_learning_2_images/fig 6-34.png)

같은 계층의 드롭아웃끼리 마스크를 공유함으로써 마스크가 '고정' 된다. 그 결과 정보를 잃게 되는 방법도 '고정'되므로, 일반적인 들보아웃 때와 달리 정보가 지수적으로 손실되는 사태를 피할 수 있다.

> 변형 드롭아웃은 일반 드롭아웃보다 결과가 좋다고 한다. 하지만 이번 장에서는 변형 드롭아웃을 이용하지 않고, 일반적인 드롭아웃을 사용한다.

### 6.5.3 가중치 공유

언어 모델을 개선하는 아주 간단한 트릭 중 **가중치 공유**가 있다. wieght tying을 직역하면 '가중치를 연결한다' 이지만, 실실적으로는 아래 그림에서 보듯, 가중치를 공유하는 효과를 준다.

![fig 6-35](/assets/img/dlfs2/deep_learning_2_images/fig 6-35.png)

위 그림처럼 Embedding 계층의 가중치와 Affine 계층의 가중치를 연결하는 (공유하는) 기법이 가중치 공유이다. 두 계층이 가중치를 공유함으로써 학습하는 매개변수 수가 크게 줄어드는 동시에 정확도도 향상된다. 

어휘 수를 V로, LSTM의 은낙 상태의 차원 수를 H라고 하면, Embedding 계층의 가중치는 형상이 V X H이며, Affine 계층의 가중치 형상은 H X V가 된다. 이때 가중치 공유를 적용하려면 Embedding 계층의 가중치를 전치하여 Affine 계층의 가중치로 설정하면 된다. 간단하지만 훌륭한 결과를 얻을 수 있다.

> 가중치 공유 덕분에 학습해야 할 매개변수 수를 줄여주고, 그 결과 학습하기가 더 쉬워진다. 게다가 매개변수 수가 줄어들기 때문에 과적합이 억제되는 혜택이 있다. 

### 6.5.4 개선된 RNNLM 구현

![fig 6-36](/assets/img/dlfs2/deep_learning_2_images/fig 6-36.png)

개선점은 다음과 같다.

- LSTM 계층의 다층화(여기서는 2층)
- 드롭아웃 사용(깊이 방향으로만 적용)
- 가중치 공유(Embedding 계층과 Affine 계층에서 가중치 공유)

이제 이 세 가지 개선점을 도입한 BetterRnnlm 클래스를 구현한다. 

```python
# coding: utf-8
import sys
sys.path.append('..')
from common.time_layers import *
from common.np import *  # import numpy as np
from common.base_model import BaseModel


class BetterRnnlm(BaseModel):
    '''
     LSTM 계층을 2개 사용하고 각 층에 드롭아웃을 적용한 모델이다.
     아래 [1]에서 제안한 모델을 기초로 하였고, [2]와 [3]의 가중치 공유(weight tying)를 적용했다.

     [1] Recurrent Neural Network Regularization (https://arxiv.org/abs/1409.2329)
     [2] Using the Output Embedding to Improve Language Models (https://arxiv.org/abs/1608.05859)
     [3] Tying Word Vectors and Word Classifiers (https://arxiv.org/pdf/1611.01462.pdf)
    '''
    def __init__(self, vocab_size=10000, wordvec_size=650,
                 hidden_size=650, dropout_ratio=0.5):
        V, D, H = vocab_size, wordvec_size, hidden_size
        rn = np.random.randn

        embed_W = (rn(V, D) / 100).astype('f')
        lstm_Wx1 = (rn(D, 4 * H) / np.sqrt(D)).astype('f')
        lstm_Wh1 = (rn(H, 4 * H) / np.sqrt(H)).astype('f')
        lstm_b1 = np.zeros(4 * H).astype('f')
        lstm_Wx2 = (rn(H, 4 * H) / np.sqrt(H)).astype('f')
        lstm_Wh2 = (rn(H, 4 * H) / np.sqrt(H)).astype('f')
        lstm_b2 = np.zeros(4 * H).astype('f')
        affine_b = np.zeros(V).astype('f')

        self.layers = [
            TimeEmbedding(embed_W),
            TimeDropout(dropout_ratio),
            TimeLSTM(lstm_Wx1, lstm_Wh1, lstm_b1, stateful=True),
            TimeDropout(dropout_ratio),
            TimeLSTM(lstm_Wx2, lstm_Wh2, lstm_b2, stateful=True),
            TimeDropout(dropout_ratio),
            TimeAffine(embed_W.T, affine_b)  # 가중치 공유!!
        ]
        self.loss_layer = TimeSoftmaxWithLoss()
        self.lstm_layers = [self.layers[2], self.layers[4]]
        self.drop_layers = [self.layers[1], self.layers[3], self.layers[5]]

        self.params, self.grads = [], []
        for layer in self.layers:
            self.params += layer.params
            self.grads += layer.grads

    def predict(self, xs, train_flg=False):
        for layer in self.drop_layers:
            layer.train_flg = train_flg

        for layer in self.layers:
            xs = layer.forward(xs)
        return xs

    def forward(self, xs, ts, train_flg=True):
        score = self.predict(xs, train_flg)
        loss = self.loss_layer.forward(score, ts)
        return loss

    def backward(self, dout=1):
        dout = self.loss_layer.backward(dout)
        for layer in reversed(self.layers):
            dout = layer.backward(dout)
        return dout

    def reset_state(self):
        for layer in self.lstm_layers:
            layer.reset_state()

```

TimeLSTM 계층을 2개 겹치고, 사이사이에 TimeDropout 계층을 사용한다. 그리고 TimEmbedding 계층과 TimeAffine 계층에서 가중치를 공유한다.

```python
# coding: utf-8
import sys
sys.path.append('..')
from common import config
# GPU에서 실행하려면 아래 주석을 해제하세요(CuPy 필요).
# ==============================================
# config.GPU = True
# ==============================================
from common.optimizer import SGD
from common.trainer import RnnlmTrainer
from common.util import eval_perplexity, to_gpu
from dataset import ptb
from better_rnnlm import BetterRnnlm


# 하이퍼파라미터 설정
batch_size = 20
wordvec_size = 650
hidden_size = 650
time_size = 35
lr = 20.0
max_epoch = 40
max_grad = 0.25
dropout = 0.5

# 학습 데이터 읽기
corpus, word_to_id, id_to_word = ptb.load_data('train')
corpus_val, _, _ = ptb.load_data('val')
corpus_test, _, _ = ptb.load_data('test')

if config.GPU:
    corpus = to_gpu(corpus)
    corpus_val = to_gpu(corpus_val)
    corpus_test = to_gpu(corpus_test)

vocab_size = len(word_to_id)
xs = corpus[:-1]
ts = corpus[1:]

model = BetterRnnlm(vocab_size, wordvec_size, hidden_size, dropout)
optimizer = SGD(lr)
trainer = RnnlmTrainer(model, optimizer)

best_ppl = float('inf')
for epoch in range(max_epoch):
    trainer.fit(xs, ts, max_epoch=1, batch_size=batch_size,
                time_size=time_size, max_grad=max_grad)

    model.reset_state()
    ppl = eval_perplexity(model, corpus_val)
    print('검증 퍼플렉서티: ', ppl)

    if best_ppl > ppl:
        best_ppl = ppl
        model.save_params()
    else:
        lr /= 4.0
        optimizer.lr = lr

    model.reset_state()
    print('-' * 50)


# 테스트 데이터로 평가
model.reset_state()
ppl_test = eval_perplexity(model, corpus_test)
print('테스트 퍼플렉서티: ', ppl_test)

```

학습을 진행하면서 매 에폭마다 검증 데이터로 퍼플렉서티를 평가하고, 그 값이 기존 퍼플렉서티보다 낮으면 학습률을 1/4로 줄인다. 이를 위해 이 코드에서는 RnnlmTrainer 클래스의 fit() 메서드를 이용해 1에폭분의 학습을 수행한 다음, 검증 데이터로 퍼플렉서티의 평가하는 처리를 for 문에서 반복한다.

퍼플렉서티는 75.76 정도가 된다. 개선 전 RNNLM의 퍼플렉서티가 약 136 이었음을 생각하면 상당히 개선되었다고 생각할 수 있다.

### 6.5.5 첨단 연구로

![fig 6-37](/assets/img/dlfs2/deep_learning_2_images/fig 6-37.png)

## 6.6 정리

이번 장에서는 게이트가 추가된 RNN을 살펴보았다. 앞 장의 단순한 RNN에서는 기울기 소실(또는 기울기 폭발)이 문제가 되었는데, 그것을 대신하는 계층으로써 게이트가 추가된 RNN(구체적으로는 LSTM과 GRU 등)이 효과가 있음을 설명했다. 이 계층들에는 게이트라는 구조가 사용되며, 게이트는 데이터와 기울기 흐름을 적절히 제어하는 메커니즘이다.

또한 LSTM 계층을 사용한 언어 모델을 만들어봤다. 그리고 PTB 데이터셋을 학습하여 퍼플렉서티를 구했다. 나아가 LSTM 다층화, 드롭아웃, 가중치 공유 등의 기법을 적용해 정확도를 큰 폭으로 향상시켰다. 이 기법들은 2017년 기준 최첨단 연구에서도 실제로 사용되고 있다.

- 단순한 RNN의 학습에서는 기울기 소실과 기울기 폭발이 문제가 된다.
- 기울기 폭발에서는 기울기 클리핑, 기울기 소실에는 게이트가 추가된 RNN(LSTM과 GRU등)이 효과적이다.
- LSTM에는 input 게이트, forget 게이트, output 게이트 등 3개의 게이트가 있다.
- 게이트에는 전용 가중치가 있으며, 시그모이드 함수를 사용하여 0.0~1.0 사이의 실수를 출력한다.
- 언어 모델 개선에는 LSTM 계층 다층화, 드롭아웃, 가중치 공유 등의 기법이 효과적이다.
- RNN의 정규화는 중요한 주제이며, 드롭아웃 기반의 다양한 기법이 제안되고 있다.



